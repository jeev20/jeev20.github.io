{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Using all GPUs when running Ollama and OpenWebUI in docker \n",
        "description: \"Bundled install of ollama and Open-Webui using docker\"\n",
        "author: \"Jeevith Hegde\"\n",
        "format: html\n",
        "toc: true\n",
        "lang: en\n",
        "jupyter: python3\n",
        "ipynb-shell-interactivity: all\n",
        "execute:\n",
        "  echo: false\n",
        "date: 2025-03-16 12:01 +0200\n",
        "categories: [\"llm\"]\n",
        "tags: [\"ollama\", \"openwebui\"]\n",
        "comments:\n",
        "  giscus:\n",
        "    repo: jeev20/jeev20.github.io\n",
        "---\n",
        "\n",
        "\n",
        "> Case: Run both Ollama and OpenWebUI in a single docker container and use all available GPUs in the host machine\n",
        "\n",
        "\n",
        "**TL;DR**\n",
        "```{.bash}\n",
        "# Installing nvidia-container-toolkit\n",
        "curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\\n",
        "&& curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\\n",
        "  sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\\n",
        "  sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\n",
        "sudo apt-get update\n",
        "sudo apt-get install -y nvidia-container-toolkit\n",
        "\n",
        "\n",
        "# Installing ollama and open-webui in docker\n",
        "sudo docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama\n",
        "sudo docker run --rm --volume /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once open-webui\n",
        " \n",
        "# Pulling some models from Ollama repository\n",
        "sudo docker exec open-webui ollama pull phi4\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "I have been using Ollama as a standalone installation in my homelab which ran as a systemd service. I used OpenWebUI as the user interface to the local Ollama installation. However, a recent version of Ollama failed to auto spawn on my LMDE 6 operating system after a restart. \n",
        "\n",
        "Naturally, I had to look for an alternative with minimal maintenance need. Thankfully, OpenWebUI offers an image which bundles both Ollama and the OpenWebUI and uses the Nvidia-Container-Toolkit to utilize all installed GPUs. \n",
        "\n",
        "This means that I only have to maintain one container for both Ollama and OpenWebUI installation. Further, to automate this update process, I use watchtower to install the latest updates of the image from the OpenWebUI team. \n",
        "\n",
        "## Docker run can fail\n",
        "I observed that running only `sudo docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama` did not work as it required [nvidia-container-toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) to be installed first. \n",
        "\n",
        "So I ran \n",
        "\n",
        "```{.bash}\n",
        "curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\\n",
        "  && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\\n",
        "    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\\n",
        "    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\n",
        "```\n",
        "\n",
        "Followed by\n",
        "\n",
        "```{.bash}\n",
        "sudo apt-get update\n",
        "sudo apt-get install -y nvidia-container-toolkit\n",
        "```\n",
        "\n",
        "And then ran the open-webui docker command\n",
        "\n",
        "```{.bash} \n",
        "sudo docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama\n",
        "sudo docker run --rm --volume /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once open-webui\n",
        "```\n",
        "\n",
        "Docker started the container and also setup the watchtower auto-update for the `open-webui` container. \n",
        "\n",
        "I can now use the `docker exec` command to use Ollama cli within the container. I downloaded some models by using\n",
        "\n",
        "```{.bash}\n",
        "sudo docker exec open-webui ollama pull phi4\n",
        "sudo docker exec open-webui ollama pull qwq\n",
        "```\n",
        "\n",
        "Since I reused the docker volume I previously used for open-webui, all my historical prompts were still available after this upgrade. Sweet! \n",
        "\n",
        "\n",
        "## Resources\n",
        "* [nvidia-container-toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)\n",
        "* [openwebui](https://github.com/open-webui/open-webui)\n",
        "* [watchtower](https://github.com/containrrr/watchtower)"
      ],
      "id": "c69392ac"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/home/wslap/Documents/jeev20.github.io/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}