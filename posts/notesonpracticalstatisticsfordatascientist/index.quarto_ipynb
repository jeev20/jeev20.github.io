{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Notes on Practical Statistics for Data Scientists ðŸ“—\n",
        "description: \"Notes of the book Practical Statistics for Data Scientists - 50+ Essential Concepts Using R and Python by Peter Bruce, Andrew Bruce, Peter Gedeck\"\n",
        "author: \"Jeevith Hegde\"\n",
        "format: html\n",
        "toc: true\n",
        "lang: en\n",
        "jupyter: python3\n",
        "ipynb-shell-interactivity: all\n",
        "execute:\n",
        "  echo: false\n",
        "date: 2025-04-15 \n",
        "categories: [\"datascience\", \"fundamentals\"]\n",
        "tags: [\"notes\"]\n",
        "comments:\n",
        "  giscus:\n",
        "    repo: jeev20/jeev20.github.io\n",
        "---\n",
        "\n",
        "\n",
        "This is my personal notes of the book [Practical Statistics for Data Scientists - 50+ Essential Concepts Using R and Python by Peter Bruce, Andrew Bruce, Peter Gedeck.](https://books.google.no/books?hl=no&lr=&id=k2XcDwAAQBAJ&oi=fnd&pg=PP1&dq=9781492072942&ots=dEKcnlVmx1&sig=A4W7tK5Um6BTbuzr3_gOvVinDRg&redir_esc=y#v=onepage&q=9781492072942&f=false) \n",
        "\n",
        "I will update this post, as I study and digest the contents of this book.\n",
        "\n",
        "\n",
        "## Chapter 1 - Exploratory Data Analysis \n",
        "\n",
        "John W. Tukey established the field of exploratory data analysis through his 1977 publication [exploratory data analysis](https://archive.org/details/exploratorydataa0000tuke_7616), in which he introduced methods to explore a dataset by using plots and summary statistics (mean, median etc.). Later in 2015, one of Tukey's former undergraduate student David Donoho published a summary article [50 years of Data Science](https://courses.csail.mit.edu/18.337/2015/docs/50YearsDataScience.pdf) showing the genesis and developments of data science as a field.\n",
        "\n",
        "\n",
        "* Before analyzing the data it is important to identify the type of data to be studied. \n",
        "* Type of data can influence the kind of data analysis methods which can be used to explore the data. \n",
        "\n",
        "The figure below provides the taxonomy of data types. \n",
        "```{mermaid}\n",
        "mindmap\n",
        "  root((Data types))\n",
        "    Numeric \n",
        "        Continuous (Continuous - Data that can contain any value in an interval)\n",
        "        Discrete (Discrete - Data that can contain only integer values)\n",
        "    Categorical\n",
        "        Binary (Binary - Data that contain just two categories )\n",
        "        Ordinal (Ordinal - Data that is explicitly ordered)\n",
        "\n",
        "```\n",
        "\n",
        "#### Ordinal data \n",
        "The interesting data type above is the ordinal data type where the order of the data is important. Here is an example of ordinal data type using sklearn. "
      ],
      "id": "eaf13461"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "enc = OrdinalEncoder()\n",
        "X = [['Male', 1], ['Female', 3], ['Female', 2]]\n",
        "enc.fit(X)\n",
        "enc.categories_"
      ],
      "id": "4f773eb5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "enc.transform([['Female', 3], ['Male', 1]])"
      ],
      "id": "6495db65",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Explanation of `enc.transform`\n",
        "First sample: `['Female', 3]`\n",
        "\n",
        "    Gender column: 'Female' is the first category in enc.categories_ â†’ encoded as 0\n",
        "    Number column: 3 is the third category in enc.categories_[1] â†’ encoded as 2\n",
        "    Result: [0., 2.]\n",
        "\n",
        "Second sample: `['Male', 1]`\n",
        "\n",
        "    Gender column: 'Male' is the second category â†’ encoded as 1\n",
        "    Number column: 1 is the first category â†’ encoded as 0\n",
        "    Result: [1., 0.]\n"
      ],
      "id": "2157c057"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "enc.inverse_transform([[1, 1], [0, 2]])"
      ],
      "id": "c6cffe0c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Rectangular data\n",
        "Typically analysis in data science focuses on rectangular data. It is a two-dimensional matrix containing records in form of rows and features/variables in form of columns. Rectangular data is usually the result of some preprocessing of unstructured data. Following are the key terms in rectangular data. \n",
        "\n",
        "\n",
        "```{mermaid}\n",
        "mindmap\n",
        "  root((Rectangular data))\n",
        "    Dataframe \n",
        "        A basic data structure used in statistical and machine learning models\n",
        "    Feature\n",
        "        Each column within a table is referred to as a feature\n",
        "    Outcome\n",
        "        The dependent variable. Output variable which is dependent on one or many features. Also called target, response, output. \n",
        "    Records\n",
        "        Each row in a table. Can be defined as singular case,  scenario, observation, pattern or sample\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### Non rectangular data structures\n",
        "\n",
        "* **Time Series data** records sequential measurements of a same variable with time. This kind of data is used to create statistical forecasting models. An example is a IOT sensor capturing temperature data every 2 minutes perpetually. Such data structures always need to include a time at which the record was captured. \n",
        "  \n",
        "* **Spatial data** can be used to create location based analytics. The object under observation can be for example a house or a point of interest in a map and its spatial coordinates. \n",
        "  \n",
        "* **Graph/ Network** are used to represent abstract relationships between the object under observation. An example can be a social network of a person showing how many contacts or friends that person has and how often he/she interacts with them. These types of data is useful in recommender systems and optimization problems. \n",
        "\n",
        "All these three can also be combined in a single use case. For example, Google maps can store spatial data in a time series manner for a person and include a graph/ network data on how the user interacts with other spatial objects (shops, landmarks) when they travel 60 kms away from their home. \n",
        "\n",
        "\n",
        "> **Difference in terminologies**\n",
        "> \n",
        "> Statisticians use **predicator variables** to predict a response or **dependent variable** and data scientist use **features** to predit a **target** \n",
        ">\n",
        "> The term **sample** to a computer scientist signifies a single row while a **sample** to a statistician means a collection of rows.\n",
        ">\n",
        "> Graph in statistics can mean plots or visualization and not just connections of entities as it is in computer science or information technology.\n",
        "\n",
        "\n",
        "### Estimates of location\n",
        "\n",
        "```{mermaid}\n",
        "mindmap\n",
        "  root((Estimates of location))\n",
        "    Mean \n",
        "        Sum of all observations divided by total number of observations. Also known as **average**\n",
        "    Trimmed mean\n",
        "        By removing the first n and the last n observations from a sorted set of observations, trimmed mean can then be calculated on the remaining observations divided by the total number of remaining observations. This helps us eliminate very high or very low values in the dataset. This can also result in a trimmed mean, which is closer to the median value.\n",
        "    Weighter mean\n",
        "        Sum of all observations multiplied by corresponding weights and divided by sum of the weights. Also known as **weighted average**\n",
        "    Median\n",
        "        The middle most observation in the ascendingly sorted dataset. Also known as 50th percentile and a robust estimate of location.\n",
        "    Weighted median\n",
        "        The value such that one-half of the sum of the weights lies below or above the sorted data\n",
        "    Percentile\n",
        "        The value which signifies the percentage of data which lies below or equal to a given data value. Can also be called as quantile.\n",
        "    Robust\n",
        "        Not sensitive to extreme observations\n",
        "    Outlier\n",
        "        Observation/s which are located in extreme ranges when compared with most of the observations\n",
        "```\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "  Mean = \\bar{x}= \\frac{\\sum_{i=1}^n x_i}{n} \n",
        "  \\label{eq:mean}\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "  TrimmedMean = \\bar{x}= \\frac{\\sum_{i=p+1}^{n-p} x_i}{n - 2p} \n",
        "  \\label{eq:trimmedmean}\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "  WeightedMean = \\bar{x}_w= \\frac{\\sum_{i=1}^{n} w_i*x_i}{\\sum_{i=1}^{n} w_i} \n",
        "  \\label{eq:weightedmean}\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "\n",
        "> **Difference in terminologies**\n",
        "> \n",
        "> Datascientist measure and statisticians estimates. Statisticians account for uncertainities in the analysis whereas concrete business objectives are the focus for datascientists.\n"
      ],
      "id": "aa2529e6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "import pandas as pd\n",
        "from scipy.stats import trim_mean\n",
        "import numpy as np\n",
        "import wquantiles\n",
        "\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/gedeck/practical-statistics-for-data-scientists/refs/heads/master/data/state.csv\")\n",
        "\n",
        "df_mean = df[\"Population\"].mean()\n",
        "df_median = df[\"Population\"].median()\n",
        "trimmed_mean = trim_mean(df[\"Population\"], 0.1)\n",
        "weighted_mean = np.average(df[\"Murder.Rate\"],weights=df[\"Population\"])\n",
        "weighted_median = wquantiles.median(df[\"Murder.Rate\"],weights=df[\"Population\"])"
      ],
      "id": "2fcb4d69",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Estimates of variability\n",
        "Variability (aka dispersion) is the second dimension of summarizing a feature. The first one is **Location**. Dispersion measures if the data values are tightly clustered or spread out. \n",
        "\n",
        "```{mermaid}\n",
        "mindmap\n",
        "  root((Estimates of variability))\n",
        "    Deviations \n",
        "        The difference between the observed values and the estimates of locations. Also known as residuals or errors. \n",
        "    Variance\n",
        "        The sum of squared deviations from the mean divided by n-1 where n is the number of observations. Also known as mean-squared-error\n",
        "    Standard deviation\n",
        "        The square root of variance\n",
        "    Mean absolute deviation\n",
        "        The absolute values of deviations from the mean. Also known as L1 norm or Manhattan norm\n",
        "    Median absolute deviation from the median\n",
        "        The median absolute values of the deviations from the median\n",
        "    Range\n",
        "        The difference between the largest and the smallest observations in the data set\n",
        "    Order statistics\n",
        "        Metrics based on sorted data from smallest ti biggest. Also known as ranks. \n",
        "    Interquartile range\n",
        "        The difference between the 75th percentile and 25th percentile\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## Related literature \n",
        "* Jon Tukey : [Exploratory Data Analysis](https://archive.org/details/exploratorydataa0000tuke_7616)  \n",
        "\n",
        "* David Donoho : [50 years of Data Science](https://courses.csail.mit.edu/18.337/2015/docs/50YearsDataScience.pdf) \n",
        "\n",
        "* Bruce, P., Bruce, A., & Gedeck, P. (2020). [Practical statistics for data scientists: 50+ essential concepts using R and Python](https://books.google.no/books?hl=no&lr=&id=k2XcDwAAQBAJ&oi=fnd&pg=PP1&dq=9781492072942&ots=dEKcnlVmx1&sig=A4W7tK5Um6BTbuzr3_gOvVinDRg&redir_esc=y#v=onepage&q=9781492072942&f=false). O'Reilly Media."
      ],
      "id": "512312a1"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/home/wslap/Documents/jeev20.github.io/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}