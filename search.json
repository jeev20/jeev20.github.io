[
  {
    "objectID": "assets/resume.html",
    "href": "assets/resume.html",
    "title": "Jeevith Hegde, Ph.D.",
    "section": "",
    "text": "linkedin.com/in/jeevhegde | github.com/jeev20\n\n\n2020-Present: Helse Midt-Norge IT Trondheim, Norway\nSenior Integration Developer (Robotic Process Automation)\n\nDelivered robots which saved over 50 man-years and enabled the team to save over 120 man-years in 5 years\n\nGrew the team from 2 developers to 8 developers by delivering 19 RPA automations, 5 test automation projects\n\nHeld technical interviews to recruit RPA developers\n\nDeveloped and maintained project templates for RPA, Test Automation and developed RPA candidate evaluation tool in Microsoft Excel and PowerApps.\n\nDesigned training material for new hires ensuring smooth technical onboarding (project ready within 3 weeks)\n\nImplemented routines for project management, technical solution design, development best practices, CI/CD pipelines, quality assurance of robots and operational maintenance of robots.\n\nImplemented routines to visualize gains from RPA/Test automation both in Splunk and PowerBI and ensured they were accessible to both upper management and internal customers.\n\n2019-2020: Ren Røros Intelligent Automation Trondheim, Norway\nSenior Automation Engineer\n\nAutomation of customer order processing using Rossum and analysis of electricity prices in BluePrism\n\nDevelopment of Power BI dashboards and Power BI reports for customers\n\nChatbot development for customers using Simplifai Studio and supervision of two master’s students in RPA\n\n2019-2020: Searo AS (acquired by IKM AS) Trondheim, Norway\nFounder\n\nStrategy development and responsible for applications for funding to Norwegian research institutions\n\n2014-2019: Norwegian University of Science and technology Trondheim, Norway\nPostdoctoral Fellow\n\nDevelopment of methods to support technical risk assessments using artificial intelligence and machine learning\n\nLecture assignments, supervision of master’s students and drafted proposals for funding\n\nPh.D. Candidate (2014-2018)\n\nDevelopment of underwater traffic rules for safe autonomous ROV IMR operations\n\nImplementation of a user interface promoting situational awareness between human and autonomous ROVs\n\n2012-2014: FMC Kongsberg Subsea AS Kongsberg, Norway\nGraduate Engineer - Reliability and Safety,\n\nWorked on concepts for the next generation Workover and HIPPS subsea safety systems\nPerformed FMEDA and system level SIL calculations in the Snorre B workover project\n\n2008-2009: Godrej & Boyce Manufacturing Company Limited Trondheim, Norway\nSales Engineer\n\nResponsible for market research, marketing and sale of Manitou and Godrej material handling equipment\n\n\n\n\n2014-2018: Norwegian University of Science and technology Trondheim, Norway\nPhD in Marine Engineering\n\nTools and methods to manage risk in autonomous subsea inspection maintenance and repair operations\n\n2010-2012: Norwegian University of Science and technology Trondheim, Norway\nMaster in Reliability Availability Maintainability & Safety - RAMS\n2004-2008: M. S. Ramaiah Institute of Technology Bangalore, India Bachelor in Industrial Engineering and Management\n\n\n\n\nLanguages: English (native), Kannada (native), Hindi (professional), Norwegian (professional)\n\nTechnical Skills: UiPath, BluePrism, Robocorp, PowerApps, Power Automate, Python (Data science, Machine Learning), PowerShell, Bash, SQL, Azure Devops, Git, PowerBI, Grafana, Splunk, Godot,, Office 365, BlockChain\n\nCertifications: UiRPA, UiARD, UiABA, AI-900, AZ-900, Robocorp, BluePrism Developer & Solution Designer, PowerBI, Splunk\n\nAwards: UiPath MVP 2022, 2023 and 2024, UiPath HyperHack 2022 1st prize winner\nResearch publications can be accessed here\n\n\nLast updated : August 2025"
  },
  {
    "objectID": "assets/resume.html#experience",
    "href": "assets/resume.html#experience",
    "title": "Jeevith Hegde, Ph.D.",
    "section": "",
    "text": "2020-Present: Helse Midt-Norge IT Trondheim, Norway\nSenior Integration Developer (Robotic Process Automation)\n\nDelivered robots which saved over 50 man-years and enabled the team to save over 120 man-years in 5 years\n\nGrew the team from 2 developers to 8 developers by delivering 19 RPA automations, 5 test automation projects\n\nHeld technical interviews to recruit RPA developers\n\nDeveloped and maintained project templates for RPA, Test Automation and developed RPA candidate evaluation tool in Microsoft Excel and PowerApps.\n\nDesigned training material for new hires ensuring smooth technical onboarding (project ready within 3 weeks)\n\nImplemented routines for project management, technical solution design, development best practices, CI/CD pipelines, quality assurance of robots and operational maintenance of robots.\n\nImplemented routines to visualize gains from RPA/Test automation both in Splunk and PowerBI and ensured they were accessible to both upper management and internal customers.\n\n2019-2020: Ren Røros Intelligent Automation Trondheim, Norway\nSenior Automation Engineer\n\nAutomation of customer order processing using Rossum and analysis of electricity prices in BluePrism\n\nDevelopment of Power BI dashboards and Power BI reports for customers\n\nChatbot development for customers using Simplifai Studio and supervision of two master’s students in RPA\n\n2019-2020: Searo AS (acquired by IKM AS) Trondheim, Norway\nFounder\n\nStrategy development and responsible for applications for funding to Norwegian research institutions\n\n2014-2019: Norwegian University of Science and technology Trondheim, Norway\nPostdoctoral Fellow\n\nDevelopment of methods to support technical risk assessments using artificial intelligence and machine learning\n\nLecture assignments, supervision of master’s students and drafted proposals for funding\n\nPh.D. Candidate (2014-2018)\n\nDevelopment of underwater traffic rules for safe autonomous ROV IMR operations\n\nImplementation of a user interface promoting situational awareness between human and autonomous ROVs\n\n2012-2014: FMC Kongsberg Subsea AS Kongsberg, Norway\nGraduate Engineer - Reliability and Safety,\n\nWorked on concepts for the next generation Workover and HIPPS subsea safety systems\nPerformed FMEDA and system level SIL calculations in the Snorre B workover project\n\n2008-2009: Godrej & Boyce Manufacturing Company Limited Trondheim, Norway\nSales Engineer\n\nResponsible for market research, marketing and sale of Manitou and Godrej material handling equipment"
  },
  {
    "objectID": "assets/resume.html#education",
    "href": "assets/resume.html#education",
    "title": "Jeevith Hegde, Ph.D.",
    "section": "",
    "text": "2014-2018: Norwegian University of Science and technology Trondheim, Norway\nPhD in Marine Engineering\n\nTools and methods to manage risk in autonomous subsea inspection maintenance and repair operations\n\n2010-2012: Norwegian University of Science and technology Trondheim, Norway\nMaster in Reliability Availability Maintainability & Safety - RAMS\n2004-2008: M. S. Ramaiah Institute of Technology Bangalore, India Bachelor in Industrial Engineering and Management"
  },
  {
    "objectID": "assets/resume.html#other",
    "href": "assets/resume.html#other",
    "title": "Jeevith Hegde, Ph.D.",
    "section": "",
    "text": "Languages: English (native), Kannada (native), Hindi (professional), Norwegian (professional)\n\nTechnical Skills: UiPath, BluePrism, Robocorp, PowerApps, Power Automate, Python (Data science, Machine Learning), PowerShell, Bash, SQL, Azure Devops, Git, PowerBI, Grafana, Splunk, Godot,, Office 365, BlockChain\n\nCertifications: UiRPA, UiARD, UiABA, AI-900, AZ-900, Robocorp, BluePrism Developer & Solution Designer, PowerBI, Splunk\n\nAwards: UiPath MVP 2022, 2023 and 2024, UiPath HyperHack 2022 1st prize winner\nResearch publications can be accessed here\n\n\nLast updated : August 2025"
  },
  {
    "objectID": "drafts/buildingapowerfulenoughlocalaiserver/index.html",
    "href": "drafts/buildingapowerfulenoughlocalaiserver/index.html",
    "title": "Building a local AI server",
    "section": "",
    "text": "I have been a avid tinkerer with home pc’s. I enjoy building pc. Now that LLM’s are a necessity for a developer, I wanted to build a great performant system for myself."
  },
  {
    "objectID": "drafts/buildingapowerfulenoughlocalaiserver/index.html#requirements",
    "href": "drafts/buildingapowerfulenoughlocalaiserver/index.html#requirements",
    "title": "Building a local AI server",
    "section": "Requirements",
    "text": "Requirements\n\nHardware\nMy requirements were quite basic:\n\nA minimum of 16GB VRAM (preferably Nvidia)\nA minimum of 16GB RAM\nA minimum of 6 cores / 12 threads Ryzen CPU\nA motherboard with 2 PCI GPU slots (does not need to be 16 lanes PCI)\nA minimum of 600 watt power supply\nA wi-fi smart-plug with scheduling capabilities\nA motherboard which support power-on after power restoration\n\n\n\nSoftware\n\nLinux OS with a long-term support and used by a lot of users\nLinux OS with Tailscale native support (systemd)\nLinux OS with easy installation of Docker, Nvidia Container Toolkit and Cuda Toolkit\nLinux OS with OpenSSH server to manage server remotely\nLinux OS with crontab to schedule running of scripts"
  },
  {
    "objectID": "drafts/buildingapowerfulenoughlocalaiserver/index.html#server",
    "href": "drafts/buildingapowerfulenoughlocalaiserver/index.html#server",
    "title": "Building a local AI server",
    "section": "Server",
    "text": "Server\nI ended up purchasing a second-hand PC with all the above requirements and then purchased two RTX 3060 GPUs with 12 GB Vram each. The RAM capacity is somewhat low for this use-case and I plan to upgrade it in the future."
  },
  {
    "objectID": "drafts/buildingapowerfulenoughlocalaiserver/index.html#scheduling",
    "href": "drafts/buildingapowerfulenoughlocalaiserver/index.html#scheduling",
    "title": "Building a local AI server",
    "section": "Scheduling",
    "text": "Scheduling"
  },
  {
    "objectID": "drafts/buildingapowerfulenoughlocalaiserver/index.html#updates",
    "href": "drafts/buildingapowerfulenoughlocalaiserver/index.html#updates",
    "title": "Building a local AI server",
    "section": "Updates",
    "text": "Updates\nAs everything runs on an Ubuntu LTS 24.04."
  },
  {
    "objectID": "drafts/buildingapowerfulenoughlocalaiserver/index.html#docker",
    "href": "drafts/buildingapowerfulenoughlocalaiserver/index.html#docker",
    "title": "Building a local AI server",
    "section": "Docker",
    "text": "Docker"
  },
  {
    "objectID": "drafts/buildingapowerfulenoughlocalaiserver/index.html#ollama-and-openwebui-bundle",
    "href": "drafts/buildingapowerfulenoughlocalaiserver/index.html#ollama-and-openwebui-bundle",
    "title": "Building a local AI server",
    "section": "Ollama and OpenWebUi Bundle",
    "text": "Ollama and OpenWebUi Bundle"
  },
  {
    "objectID": "drafts/buildingapowerfulenoughlocalaiserver/index.html#docker-exec",
    "href": "drafts/buildingapowerfulenoughlocalaiserver/index.html#docker-exec",
    "title": "Building a local AI server",
    "section": "Docker exec",
    "text": "Docker exec"
  },
  {
    "objectID": "drafts/buildingapowerfulenoughlocalaiserver/index.html#power-automation",
    "href": "drafts/buildingapowerfulenoughlocalaiserver/index.html#power-automation",
    "title": "Building a local AI server",
    "section": "Power automation",
    "text": "Power automation"
  },
  {
    "objectID": "posts/videodownloadscript/index.html",
    "href": "posts/videodownloadscript/index.html",
    "title": "Video download script",
    "section": "",
    "text": "Being a dad comes with opportunities to automate mundane chores. To curate a sensible kids video collection, I have to download select videos from YouTube. To organize this content, I have recently setup a container running the jellyfin media (offline) server. This exercise of curation and downloading videos manually requires a lot of unnecessary effort and should be have been automated.\nYes, it is time to automate!\n\nSolution architecture\nBelow is the solution architecture. The input or the video urls are saved on a csv with headers VideoURL & Type.\n\n\n\n\n\n---\ntitle: Video download automation\n---\nflowchart TD\nsubgraph 0[Video download]\n1[RequiredVideos.csv] -.-&gt; |VideoURL, &lt;br&gt; Type| 2[ytDownload.sh&lt;br&gt;yt-dlp]\n\n2 -.-&gt;|mkdir| 3[Folder_Category_1]\n\n2 -.-&gt;|mkdir| 4[Folder_Category_2]\n2  -.-&gt; |mkdir| 5[Folder_Category_N]\n\n3 -.-&gt;|Download video, description, &lt;br&gt; thumbnail, subtitles| 6[Individual_VideoFiles]\nend\n\n6 -.-&gt; |Copy|7\n\nsubgraph Jellyfin server\n        10[Jellyfin_Server]\n        7[Folder_Category_1]\n        8[Folder_Category_2]\n        9[Folder_Category_N]\n    end\n  \nsubgraph Tools used\n11[Bash&lt;br&gt;yt-dlp&lt;br&gt;github &lt;br&gt;Jellyfin]\nend\n\n\n\n\n\n\nThis .csv file then is an input file to a bash script. The bash script iterates over the rows in the csv file and uses the yt-dlp project/cli tool to download videos. In short, it will help me build a video collection (for personal use) with minimal resistance.\nThe next time when I or my wife want to add a video, we add a row in the .csv file with the video url and a corresponding type (category).\n\n\nytDownload.sh script\n#!/bin/bash\n\n# Reading the csv file and using Videos column\nVIDEO_URLS= readarray -t eCollection &lt; &lt;(cut -d, -f1,2 RequiredVideos.csv)\n\n# Set the directory where you want to save the downloaded files\nOUTPUT_DIR=\".\"\n\n# Create the output directory if it doesn't exist\nmkdir -p \"$OUTPUT_DIR\"\n\nfor row in ${eCollection[@]:1} \ndo\nurl=$(echo \"$row\" | cut -d, -f1)\ntypeVideo=$(echo \"$row\" | cut -d, -f2)\necho \"Starting to download : \"$url\" \"\n# Download the video and its subtitles no special characters do not overwrite video if already found(if available)\nyt-dlp -f 'bestvideo[ext=mp4]+bestaudio[ext=m4a]' --write-auto-sub --write-description --sub-lang en --restrict-filename --no-force-overwrites --output \"$typeVideo/%(title)s/%(title)s\" \"$url\"\necho \"Completed download : \"$url\" \"\ndone\n\nCommand explanation\n\nGet the best video resolution,\nBest audio quality,\nDownload subtitles if available in english,\nDownload description if available,\nRemove symbols and special characters from video filename,\nSkip download if resource already found in target folder,\nSave to respective type/videofilename folder\n\nyt-dlp -f 'bestvideo[ext=mp4]+bestaudio[ext=m4a]' --write-auto-sub --write-description \\\n --sub-lang en --restrict-filename --no-force-overwrites --output \"$typeVideo/%(title)s/%(title)s\" \"$url\"\n\n\n\nFurther improvements\nThe input .csv should be made accessible via a network-attached-storage device which supports a native mobile app. This way we always have access to the input file.\nA further step would be to run a cron job on a server to look for new rows added in the .csv file and run the script. This will ensure that my jellyfin collection is always up-to-date.\n\n\nCredits\nThank you to all the maintainers of yt-dlp!"
  },
  {
    "objectID": "posts/notesdatascience/FeatureEngineering/index.html",
    "href": "posts/notesdatascience/FeatureEngineering/index.html",
    "title": "Feature engineering",
    "section": "",
    "text": "This is my notes on feature engineering. The idea is to populate this article with all possible relevant knowledge I can get on my hands on and then use it as a reference in the future. Plan is also to use what I Study and note once, use many times.\n\n\n\n\n\nmindmap\n  root((Feature engineering))\n    Feature transformation\n        Handling missing values\n            Imputation\n                Remove observation\n                Mean replacement\n                Median replacement\n                Most frequest categorical\n        Handling categorical values\n            One-hot-encoding\n            Binning\n        Handling outliers\n            Outlier detection\n            Outlier removal\n        Feature scaling\n            Standardization\n            Normalization\n    Feature construction\n        Domain knowledge\n        Experience\n    Feature selection\n        Feature importance \n    Feature extraction\n\n\n\n\n\n\n\nimport polars as pl\ninput_data_path = f\"../data/iot/iot_telemetry_data.parquet\"\ndf_original =  pl.read_parquet(input_data_path)\ndf_original.head(1)\n\n\nshape: (1, 9)\n\n\n\nts\ndevice\nco\nhumidity\nlight\nlpg\nmotion\nsmoke\ntemp\n\n\nf64\nstr\nf64\nf64\nbool\nf64\nbool\nf64\nf64\n\n\n\n\n1.5945e9\n\"b8:27:eb:bf:9d:51\"\n0.004956\n51.0\nfalse\n0.007651\nfalse\n0.020411\n22.7\n\n\n\n\n\n\n\n\n\n\n\n\nimport duckdb\nimport polars as pl\nimport seaborn as sn\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\n\n\n# ts is in epoch time format so converting it to timestamp\n# rounding values for temperature and humidity\n# converting temperature from farhaneit to celsius\ndf_raw = duckdb.sql(\n    f\"SELECT ts, to_timestamp(ts) AS timestamp, device, temp,ROUND((temp - 32) * 5.0 / 9, 4) AS temp_c, ROUND(humidity, 4) AS humidity, lpg, smoke, light FROM '{input_data_path}'\"\n)\n\n\n\n\nThe seven questions to get insight into the data\n\n\n\n# Converting to polars to easy statistics and exploration\ndf_original = df_raw.pl()  \ndf_original.shape\n\n(405184, 9)\n\n\n\n\n\nThis dataset is quiet clean, there are no missing data in the input data in any feature. Docs Reference\n\ndf_original.null_count()\n\n\nshape: (1, 9)\n\n\n\nts\ntimestamp\ndevice\ntemp\ntemp_c\nhumidity\nlpg\nsmoke\nlight\n\n\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe ts column is in unix seconds, which we will convert to timestamp.\n\ndf_original = df_original.with_columns(pl.from_epoch(pl.col(\"ts\"), time_unit=\"s\").alias(\"timestamp\"))\ndf_original.head(2)\n\n\nshape: (2, 9)\n\n\n\nts\ntimestamp\ndevice\ntemp\ntemp_c\nhumidity\nlpg\nsmoke\nlight\n\n\nf64\ndatetime[μs]\nstr\nf64\nf64\nf64\nf64\nf64\nbool\n\n\n\n\n1.5945e9\n2020-07-12 00:01:34\n\"b8:27:eb:bf:9d:51\"\n22.7\n-5.1667\n51.0\n0.007651\n0.020411\nfalse\n\n\n1.5945e9\n2020-07-12 00:01:34\n\"00:0f:00:70:91:0a\"\n19.700001\n-6.8333\n76.0\n0.005114\n0.013275\nfalse\n\n\n\n\n\n\n\n\nThe time range of the dataset is\n\nprint(df_original.select(pl.col(['timestamp', 'ts'])).describe())\n\nshape: (9, 3)\n┌────────────┬────────────────────────────┬───────────────┐\n│ statistic  ┆ timestamp                  ┆ ts            │\n│ ---        ┆ ---                        ┆ ---           │\n│ str        ┆ str                        ┆ f64           │\n╞════════════╪════════════════════════════╪═══════════════╡\n│ count      ┆ 405184                     ┆ 405184.0      │\n│ null_count ┆ 0                          ┆ 0.0           │\n│ mean       ┆ 2020-07-16 00:06:56.798528 ┆ 1.5949e9      │\n│ std        ┆ null                       ┆ 199498.399262 │\n│ min        ┆ 2020-07-12 00:01:34        ┆ 1.5945e9      │\n│ 25%        ┆ 2020-07-14 00:20:00        ┆ 1.5947e9      │\n│ 50%        ┆ 2020-07-16 00:06:28        ┆ 1.5949e9      │\n│ 75%        ┆ 2020-07-18 00:02:56        ┆ 1.5950e9      │\n│ max        ┆ 2020-07-20 00:03:37        ┆ 1.5952e9      │\n└────────────┴────────────────────────────┴───────────────┘\n\n\n\n\n\n\ndf_original.select(pl.col(\"humidity\").mean())\n\n\nshape: (1, 1)\n\n\n\nhumidity\n\n\nf64\n\n\n\n\n60.511694\n\n\n\n\n\n\n\n\n\n\nWe are only interested in certain columns, the rest of the columns can be removed.\n\nirrelevant_columns = [\"co\", \"lpg\", \"motion\", \"smoke\", \"ts\", \"light\"]\nfor columnname in irrelevant_columns:\n    pass\n    #df = df_original.drop(columnname)\n#df_original.head(1)\n\n\n\n\n\ndf = df_original.select(['timestamp', 'device', 'temp', 'humidity'])\ndf = df.sort(by=\"timestamp\")\ndf.head(1)\n\n\nshape: (1, 4)\n\n\n\ntimestamp\ndevice\ntemp\nhumidity\n\n\ndatetime[μs]\nstr\nf64\nf64\n\n\n\n\n2020-07-12 00:01:34\n\"b8:27:eb:bf:9d:51\"\n22.7\n51.0\n\n\n\n\n\n\n\nimport seaborn as sn\nsn.lineplot(df.head(100), x=\"timestamp\", y=\"temp\")\n\n\n\n\n\n\n\n\n\nfrom prophet import Prophet\nimport pandas as pd\nimport dyplot\ndf_device_1 = df.filter(pl.col(\"device\")==\"b8:27:eb:bf:9d:51\")\ndf_device_2 = df.filter(pl.col(\"device\")==\"00:0f:00:70:91:0a\")\ndf_device_3 = df.filter(pl.col(\"device\")==\"1c:bf:ce:15:ec:4d\")\n\n\ndef get_forecast_data(df):\n    device_df = df.select(pl.col([\"timestamp\", \"temp\"])).rename({\"timestamp\":\"ds\", \"temp\":\"y\"})\n    device_df = device_df.to_pandas()\n    m = Prophet()\n    m.fit(device_df)\n\n    future = m.make_future_dataframe(periods=4)\n    forecast = m.predict(future)\n    forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n    fig = m.plot(forecast)\n    fig_comp = m.plot_components(forecast)\n    #fig_dyplot = dyplot.prophet(m, forecast)\n    #fig_dyplot.show()\n    return fig, fig_comp\n\nfig, fig_comp = get_forecast_data(df_device_1)\nfig, fig_comp = get_forecast_data(df_device_2)\nfig, fig_comp = get_forecast_data(df_device_3)\n\n21:45:04 - cmdstanpy - INFO - Chain [1] start processing\n21:46:24 - cmdstanpy - INFO - Chain [1] done processing\n21:46:45 - cmdstanpy - INFO - Chain [1] start processing\n21:47:28 - cmdstanpy - INFO - Chain [1] done processing\n21:47:41 - cmdstanpy - INFO - Chain [1] start processing\n21:48:40 - cmdstanpy - INFO - Chain [1] done processing"
  },
  {
    "objectID": "posts/notesdatascience/FeatureEngineering/index.html#importing-modules",
    "href": "posts/notesdatascience/FeatureEngineering/index.html#importing-modules",
    "title": "Feature engineering",
    "section": "",
    "text": "import duckdb\nimport polars as pl\nimport seaborn as sn\nimport pandas as pd\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/notesdatascience/FeatureEngineering/index.html#reading-data",
    "href": "posts/notesdatascience/FeatureEngineering/index.html#reading-data",
    "title": "Feature engineering",
    "section": "",
    "text": "# ts is in epoch time format so converting it to timestamp\n# rounding values for temperature and humidity\n# converting temperature from farhaneit to celsius\ndf_raw = duckdb.sql(\n    f\"SELECT ts, to_timestamp(ts) AS timestamp, device, temp,ROUND((temp - 32) * 5.0 / 9, 4) AS temp_c, ROUND(humidity, 4) AS humidity, lpg, smoke, light FROM '{input_data_path}'\"\n)"
  },
  {
    "objectID": "posts/notesdatascience/FeatureEngineering/index.html#exploring-the-data",
    "href": "posts/notesdatascience/FeatureEngineering/index.html#exploring-the-data",
    "title": "Feature engineering",
    "section": "",
    "text": "The seven questions to get insight into the data\n\n\n\n# Converting to polars to easy statistics and exploration\ndf_original = df_raw.pl()  \ndf_original.shape\n\n(405184, 9)\n\n\n\n\n\nThis dataset is quiet clean, there are no missing data in the input data in any feature. Docs Reference\n\ndf_original.null_count()\n\n\nshape: (1, 9)\n\n\n\nts\ntimestamp\ndevice\ntemp\ntemp_c\nhumidity\nlpg\nsmoke\nlight\n\n\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe ts column is in unix seconds, which we will convert to timestamp.\n\ndf_original = df_original.with_columns(pl.from_epoch(pl.col(\"ts\"), time_unit=\"s\").alias(\"timestamp\"))\ndf_original.head(2)\n\n\nshape: (2, 9)\n\n\n\nts\ntimestamp\ndevice\ntemp\ntemp_c\nhumidity\nlpg\nsmoke\nlight\n\n\nf64\ndatetime[μs]\nstr\nf64\nf64\nf64\nf64\nf64\nbool\n\n\n\n\n1.5945e9\n2020-07-12 00:01:34\n\"b8:27:eb:bf:9d:51\"\n22.7\n-5.1667\n51.0\n0.007651\n0.020411\nfalse\n\n\n1.5945e9\n2020-07-12 00:01:34\n\"00:0f:00:70:91:0a\"\n19.700001\n-6.8333\n76.0\n0.005114\n0.013275\nfalse\n\n\n\n\n\n\n\n\nThe time range of the dataset is\n\nprint(df_original.select(pl.col(['timestamp', 'ts'])).describe())\n\nshape: (9, 3)\n┌────────────┬────────────────────────────┬───────────────┐\n│ statistic  ┆ timestamp                  ┆ ts            │\n│ ---        ┆ ---                        ┆ ---           │\n│ str        ┆ str                        ┆ f64           │\n╞════════════╪════════════════════════════╪═══════════════╡\n│ count      ┆ 405184                     ┆ 405184.0      │\n│ null_count ┆ 0                          ┆ 0.0           │\n│ mean       ┆ 2020-07-16 00:06:56.798528 ┆ 1.5949e9      │\n│ std        ┆ null                       ┆ 199498.399262 │\n│ min        ┆ 2020-07-12 00:01:34        ┆ 1.5945e9      │\n│ 25%        ┆ 2020-07-14 00:20:00        ┆ 1.5947e9      │\n│ 50%        ┆ 2020-07-16 00:06:28        ┆ 1.5949e9      │\n│ 75%        ┆ 2020-07-18 00:02:56        ┆ 1.5950e9      │\n│ max        ┆ 2020-07-20 00:03:37        ┆ 1.5952e9      │\n└────────────┴────────────────────────────┴───────────────┘\n\n\n\n\n\n\ndf_original.select(pl.col(\"humidity\").mean())\n\n\nshape: (1, 1)\n\n\n\nhumidity\n\n\nf64\n\n\n\n\n60.511694\n\n\n\n\n\n\n\n\n\n\nWe are only interested in certain columns, the rest of the columns can be removed.\n\nirrelevant_columns = [\"co\", \"lpg\", \"motion\", \"smoke\", \"ts\", \"light\"]\nfor columnname in irrelevant_columns:\n    pass\n    #df = df_original.drop(columnname)\n#df_original.head(1)\n\n\n\n\n\ndf = df_original.select(['timestamp', 'device', 'temp', 'humidity'])\ndf = df.sort(by=\"timestamp\")\ndf.head(1)\n\n\nshape: (1, 4)\n\n\n\ntimestamp\ndevice\ntemp\nhumidity\n\n\ndatetime[μs]\nstr\nf64\nf64\n\n\n\n\n2020-07-12 00:01:34\n\"b8:27:eb:bf:9d:51\"\n22.7\n51.0\n\n\n\n\n\n\n\nimport seaborn as sn\nsn.lineplot(df.head(100), x=\"timestamp\", y=\"temp\")\n\n\n\n\n\n\n\n\n\nfrom prophet import Prophet\nimport pandas as pd\nimport dyplot\ndf_device_1 = df.filter(pl.col(\"device\")==\"b8:27:eb:bf:9d:51\")\ndf_device_2 = df.filter(pl.col(\"device\")==\"00:0f:00:70:91:0a\")\ndf_device_3 = df.filter(pl.col(\"device\")==\"1c:bf:ce:15:ec:4d\")\n\n\ndef get_forecast_data(df):\n    device_df = df.select(pl.col([\"timestamp\", \"temp\"])).rename({\"timestamp\":\"ds\", \"temp\":\"y\"})\n    device_df = device_df.to_pandas()\n    m = Prophet()\n    m.fit(device_df)\n\n    future = m.make_future_dataframe(periods=4)\n    forecast = m.predict(future)\n    forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n    fig = m.plot(forecast)\n    fig_comp = m.plot_components(forecast)\n    #fig_dyplot = dyplot.prophet(m, forecast)\n    #fig_dyplot.show()\n    return fig, fig_comp\n\nfig, fig_comp = get_forecast_data(df_device_1)\nfig, fig_comp = get_forecast_data(df_device_2)\nfig, fig_comp = get_forecast_data(df_device_3)\n\n21:45:04 - cmdstanpy - INFO - Chain [1] start processing\n21:46:24 - cmdstanpy - INFO - Chain [1] done processing\n21:46:45 - cmdstanpy - INFO - Chain [1] start processing\n21:47:28 - cmdstanpy - INFO - Chain [1] done processing\n21:47:41 - cmdstanpy - INFO - Chain [1] start processing\n21:48:40 - cmdstanpy - INFO - Chain [1] done processing"
  },
  {
    "objectID": "posts/nerdminersetup/index.html",
    "href": "posts/nerdminersetup/index.html",
    "title": "Nerd miner setup",
    "section": "",
    "text": "About two months ago, I found out about an open-source project called NerdMiner and the curiosity led me to try the project out myself.\nNerdMiner is a solo-miner software setup on very low specification hardware, such as ESP32 micro controllers. In this case it mines for bitcoin by pooling together large number of such boards.\nThe cool part about this setup is that it sips power in comparison to the state-of-the-art bitcoin miners. The ESP32’s I have probably use 160 mA per board, which is negligible in the grand scheme of things.\nThat said, the chances of finding a block on a hardware this weak is next to 0! Therefore, some call it a bitcoin lottery.\nThe hardware setup is quite simple as shown below. A cluster of 4 ESP32 Wroom boards connected to a powered USB hub.\nThe software is quite easy to flash onto the ESP32. The post and website goes over the flashing process.\n\n\n\n\n\ngraph TD\n\n    subgraph ESP32_Cluster\n    direction TB\n        Esp1[&lt;img src='https://no.mouser.com/images/espressifsystems/lrg/ESP32-DevKitC-32E_SPL.jpg' width='40' height='150'/&gt;] \n\n        Esp2[&lt;img src='https://no.mouser.com/images/espressifsystems/lrg/ESP32-DevKitC-32E_SPL.jpg' width='40' height='150'/&gt;]\n\n\n        Esp3[&lt;img src='https://no.mouser.com/images/espressifsystems/lrg/ESP32-DevKitC-32E_SPL.jpg' width='40' height='150'/&gt;] \n\n        Esp4[&lt;img src='https://no.mouser.com/images/espressifsystems/lrg/ESP32-DevKitC-32E_SPL.jpg' width='40' height='150'/&gt;] \n\n\n    end \n\n\n\n    Powered_USB_Hub\n\n \nESP32_Cluster--&gt;Powered_USB_Hub\n\n\n\n\n\n\n\n\nQuite happy with how it runs.\n\n\n\nOutputVSCodeExtension"
  },
  {
    "objectID": "posts/essentialsinstallerfordebiandistribution/index.html",
    "href": "posts/essentialsinstallerfordebiandistribution/index.html",
    "title": "Essentials installer for debian distribution",
    "section": "",
    "text": "I use the following script to quickly setup a new installation of a debian distribution with my commonly used packages and libraries.\nThis allows for quick headless deployment of all essential programs and decreases the overhead when hopping distributions!\n#!/bin/bash\n\n# Update and upgrade packages finally remove stale files\nsudo apt update -y && sudo apt upgrade -y && sudo apt autoremove -y\n\n# Install apt packages most used\nsudo apt-get remove docker docker-engine docker.io\nsudo apt install python-is-python3 -y \\\nneofetch -y \\\nhtop -y \\\nbpytop -y \\\nnpm -y \\\ntree -y \\\ndocker.io -y \\\ncurl -y \\\nssh -y \\\nopenssh-server -y\n\n# Install rustup to install cargo and python package management\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n\n# Install UV for python package management\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Install node packages\nsudo npm install -g tldr -y\n\n# Install cargo packages\ncargo install broot --force\nbroot -y"
  },
  {
    "objectID": "posts/switchingfrompiptouv/index.html",
    "href": "posts/switchingfrompiptouv/index.html",
    "title": "Switching from PIP to UV",
    "section": "",
    "text": "For the past 10 years I have constantly depended on python tools such as conda, pip, virtualenv, pyenv and poetry. These tools always made it difficult to master python tooling as I had to remember the CLI commands for each one of them separately.\nIn mid 2024, I came across a new tool called UV via an youtube suggestion. The developers built it on Rust, and back then I was very mesmerized by rust. But I did not feel comfortable switching my python workflows to UV right away.\nFast forward a year, the latest version of UV made it a complete no-brainer and I have decided to switch over.\nTo start a new project I let uv create the required boilerplate for me by using the init command.\nThe above command declares also the required python version by using the --python argument. To inspect everything is correct, I check the content of pyproject.toml\nI never liked how poetry or pyenv by default separated my repository and the virtual environments to different location. This is also possible in UV but not as a default. I like that all my project dependencies are in the project folder. I also no longer need to bother setting up a virtual environment. By adding the below command, uv can create a .venv environment automatically.\nFinally, to run a python script, I no longer need any alias or call python myself. uv runcommand does all that for me. It chooses the virtual environment and python version in the project folder and runs the script.\nAnother great feature is the ability to use ruff tool to lint check and format my scripts in seconds!\nDid I fail to mention all the above commands runs extremely fast when compared to the older python tools? It is fast alright! This gives developers more time to focus on their code base rather than fiddle around with multiple tools just to execute their python scripts.\nKudos to the entire team at Astral. I hope that this open-source project continues being open and does not follow the Redis way!\nThere are many more commands to explore in uv and uvx from UV Cli Commands."
  },
  {
    "objectID": "posts/switchingfrompiptouv/index.html#resources",
    "href": "posts/switchingfrompiptouv/index.html#resources",
    "title": "Switching from PIP to UV",
    "section": "Resources",
    "text": "Resources\nHere are some of the video resources which helped me learn the basic commands in UV and uvx, in no specific order.\n\nFeature review from Tim \nSoft introduction to UV by Arjan \nSoft introduction to UV by Ian"
  },
  {
    "objectID": "posts/installingollamaandopenwebuiusingdocker/index.html",
    "href": "posts/installingollamaandopenwebuiusingdocker/index.html",
    "title": "Using all GPUs when running Ollama and OpenWebUI in docker",
    "section": "",
    "text": "Case: Run both Ollama and OpenWebUI in a single docker container and use all available GPUs in the host machine\nTL;DR\nI have been using Ollama as a standalone installation in my homelab which ran as a systemd service. I used OpenWebUI as the user interface to the local Ollama installation. However, a recent version of Ollama failed to auto spawn on my LMDE 6 operating system after a restart.\nNaturally, I had to look for an alternative with minimal maintenance need. Thankfully, OpenWebUI offers an image which bundles both Ollama and the OpenWebUI and uses the Nvidia-Container-Toolkit to utilize all installed GPUs.\nThis means that I only have to maintain one container for both Ollama and OpenWebUI installation. Further, to automate this update process, I use watchtower to install the latest updates of the image from the OpenWebUI team."
  },
  {
    "objectID": "posts/installingollamaandopenwebuiusingdocker/index.html#docker-run-can-fail",
    "href": "posts/installingollamaandopenwebuiusingdocker/index.html#docker-run-can-fail",
    "title": "Using all GPUs when running Ollama and OpenWebUI in docker",
    "section": "Docker run can fail",
    "text": "Docker run can fail\nI observed that running only sudo docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama did not work as it required nvidia-container-toolkit to be installed first.\nSo I ran\ncurl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\\n  && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\\n    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\\n    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\nFollowed by\nsudo apt-get update\nsudo apt-get install -y nvidia-container-toolkit\nAnd then ran the open-webui docker command\nsudo docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama\nsudo docker run --rm --volume /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once open-webui\nDocker started the container and also setup the watchtower auto-update for the open-webui container.\nI can now use the docker exec command to use Ollama cli within the container. I downloaded some models by using\nsudo docker exec open-webui ollama pull phi4\nsudo docker exec open-webui ollama pull qwq\nSince I reused the docker volume I previously used for open-webui, all my historical prompts were still available after this upgrade. Sweet!"
  },
  {
    "objectID": "posts/installingollamaandopenwebuiusingdocker/index.html#resources",
    "href": "posts/installingollamaandopenwebuiusingdocker/index.html#resources",
    "title": "Using all GPUs when running Ollama and OpenWebUI in docker",
    "section": "Resources",
    "text": "Resources\n\nnvidia-container-toolkit\nopenwebui\nwatchtower"
  },
  {
    "objectID": "posts/updateollamainairgappedsystem/index.html",
    "href": "posts/updateollamainairgappedsystem/index.html",
    "title": "Updating Ollama in an airgapped linux server",
    "section": "",
    "text": "Case: Updating Ollama installation in a local server without internet access\nTL;DR\nAt work this week I had to update an Ollama instance in a airgapped linux server. The machine only had local ssh access. Thankfully ollama docs explained this scenario well."
  },
  {
    "objectID": "posts/updateollamainairgappedsystem/index.html#step-1---remove-previous-install",
    "href": "posts/updateollamainairgappedsystem/index.html#step-1---remove-previous-install",
    "title": "Updating Ollama in an airgapped linux server",
    "section": "Step 1 - Remove previous install",
    "text": "Step 1 - Remove previous install\nsudo rm -rf /usr/lib/ollama –&gt; removes the previous Ollama installation. This can also remove the downloaded models unless you do the following.\nPreviously, to make our downloaded models independent of the Ollama version, I set an environment variable OLLAMA_MODELS such that Ollama can get static models from a shared drive. This made sure that when we do have to update Ollama, we do not need to purge already downloaded models. By default the downloaded models are saved at\nmacOS: ~/.ollama/models\nLinux: /usr/share/ollama/.ollama/models\nWindows: C:\\Users\\%username%\\.ollama\\models\nThe following command would set OLLAMA_MODELS to the users home folder /home/$user/OllamaModels\nexport OLLAMA_MODELS=\"/home/$user/OllamaModels\"\nAnother way to set OLLAMA_MODELS path is to edit the /etc/systemd/system/ollama.service file as show in this walkthrough ## Step 2 - Download the specific version\ncurl -L https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgzcommand downloads the latest version of Ollama to the current folder. One can also download the same via the github releases page: Ollama Releases"
  },
  {
    "objectID": "posts/updateollamainairgappedsystem/index.html#step-3---extract-contents-and-copy",
    "href": "posts/updateollamainairgappedsystem/index.html#step-3---extract-contents-and-copy",
    "title": "Updating Ollama in an airgapped linux server",
    "section": "Step 3 - Extract contents and copy",
    "text": "Step 3 - Extract contents and copy\nsudo tar -C /usr -xzf ollama-linux-amd64.tgz command extracts the contents and copies it to /usr folder."
  },
  {
    "objectID": "posts/updateollamainairgappedsystem/index.html#step-4---check-the-installation",
    "href": "posts/updateollamainairgappedsystem/index.html#step-4---check-the-installation",
    "title": "Updating Ollama in an airgapped linux server",
    "section": "Step 4 - Check the installation",
    "text": "Step 4 - Check the installation\nollama serve starts the ollama service if it is not already running ollama -v outputs the version of the new version. For example : ollama version is 0.5.7 ollama list will then output all of your previously downloaded models."
  },
  {
    "objectID": "posts/updateollamainairgappedsystem/index.html#references",
    "href": "posts/updateollamainairgappedsystem/index.html#references",
    "title": "Updating Ollama in an airgapped linux server",
    "section": "References",
    "text": "References\n\nOllama linux installing manually\nOllama general FAQ"
  },
  {
    "objectID": "posts/repurposeoldmacsusingopencorelegacy/index.html",
    "href": "posts/repurposeoldmacsusingopencorelegacy/index.html",
    "title": "Reviving an old apple mac 💻 🖥️",
    "section": "",
    "text": "This easter break, I learnt about a brilliant open source project called the Open Core Legacy Patcher (OCLP), which aims to run newer versions of mac os on old apple hardware. It does this by providing patches and creating compatibility between old hardware and new apple operating system.\nI tried upgrading my old 2013 Macbok Pro and a early 2008 imac using OCLP. I was surprised to see how easy it was to upgrade to the newest version of operating system. However, since the 2008 imac I own has only 4 GB of memory and old AMD GPU, it did have issues running the newest os (sluggish, buggy graphics). Therefore, I skipped upgrading it to the newest version and rolled back to an older OS version called ElCaptain."
  },
  {
    "objectID": "posts/repurposeoldmacsusingopencorelegacy/index.html#my-expierence-with-oclp",
    "href": "posts/repurposeoldmacsusingopencorelegacy/index.html#my-expierence-with-oclp",
    "title": "Reviving an old apple mac 💻 🖥️",
    "section": "My expierence with OCLP",
    "text": "My expierence with OCLP\nI do believe, OCLP will revive many old apple computers, which is the way it should be. Old computers still work great and such projects are great for both the tinkers and people who do not want to spend a lot of money to buy a newer computer.\nMy plan is to keep a backup of the max supported Apple OS (BigSur) in a pendrive, just incase some bugs occur in the future versions of OCLP.\nAs of now Sequoia works very smoothly on this 2013 macbook pro. One thing to note is that, on startup the PC takes a while say around 2-4 minutes before it starts getting quick and responsive.\nI am planning to use Macbook Pro as my blogging and recording pc. This blog post was written on it using VSCode and devcontainer extension. Overall the installation works smooth and hassel free."
  },
  {
    "objectID": "posts/repurposeoldmacsusingopencorelegacy/index.html#resources",
    "href": "posts/repurposeoldmacsusingopencorelegacy/index.html#resources",
    "title": "Reviving an old apple mac 💻 🖥️",
    "section": "Resources",
    "text": "Resources\nHere are some of the video resources which helped me setup Open Core Legacy Patcher\n\nInstalling Sequoia from Anson Alexander \nOCLP basics video from Anson Alexander"
  },
  {
    "objectID": "posts/gettingfilecontentfromazuredevops/index.html",
    "href": "posts/gettingfilecontentfromazuredevops/index.html",
    "title": "Content of file from Azure DevOps repository",
    "section": "",
    "text": "At work this week I came across a hurdle which took me a lot of searching and asking perplexity (OpenAi) large language model for a solution. Sadly without any correct suggestions. In the end, I found out the solution in the AzureDevops API documenation"
  },
  {
    "objectID": "posts/gettingfilecontentfromazuredevops/index.html#api-parameters",
    "href": "posts/gettingfilecontentfromazuredevops/index.html#api-parameters",
    "title": "Content of file from Azure DevOps repository",
    "section": "API parameters",
    "text": "API parameters\nLets assume we have the following project details in Azure devops\nProject name (project) : FileContentProject\nRepository name (repositoryId) : TestRepo\nFile name (path): readme.json\nFile content (of readme.json): Raw value from https://dummyjson.com/comments\n\nAPI definition\nAccording to AzureDevops API documenation the API endpoint are as the following.\nhttps://dev.azure.com/{organization}/{project}/_apis/git/repositories/{repositoryId}/items?path={path}&scopePath={scopePath}&recursionLevel={recursionLevel}&includeContentMetadata={includeContentMetadata}&latestProcessedChange={latestProcessedChange}&download={download}&$format={$format}&versionDescriptor.version={versionDescriptor.version}&versionDescriptor.versionOptions={versionDescriptor.versionOptions}&versionDescriptor.versionType={versionDescriptor.versionType}&includeContent={includeContent}&resolveLfs={resolveLfs}&sanitize={sanitize}&api-version=7.1\nIn my use case these were the only parameters I need to have in place.\nhttps://dev.azure.com/{organization}/{project}/_apis/git/repositories/{repositoryId}/items?path={path}&$format={$format}&includeContent={includeContent}&api-version=7.1\nMy API request therefore was as mentioned below (remember to update it with your organization, project and repositoryId/repository name).\nhttps://dev.azure.com/{myorganization}/{myproject}/_apis/git/repositories/{myrepositoryName}/items?path=readme.json&$format=json&includeContent=true&api-version=7.1"
  },
  {
    "objectID": "posts/gettingfilecontentfromazuredevops/index.html#testing-the-api",
    "href": "posts/gettingfilecontentfromazuredevops/index.html#testing-the-api",
    "title": "Content of file from Azure DevOps repository",
    "section": "Testing the API",
    "text": "Testing the API\nTo send a request I use Bruno client which translates to\n\nIn powershell\n$headers=@{}\n$headers.Add(\"Authorization\", \"Basic **t7**X0=')\n$response = Invoke-RestMethod -Uri 'https://dev.azure.com/myORG/FileContentProject/_apis/git/repositories/TestRepo/items?path=readme.json&%24format=json&includeContent=true&api-version=7.1' -Method GET -Headers $headers\nor\n\n\nIn bash\ncurl --request GET \\\n  --url 'https://dev.azure.com/myORG/FileContentProject/_apis/git/repositories/TestRepo/items?path=readme.json&%24format=json&includeContent=true&api-version=7.1' \\\n  --header 'Authorization: Basic **t7**X0='\n\n\nResult\nAnd there it is the “content” key of the json response!\n{\n  \"objectId\": \"529a003b36fb109bb7ef0ead557d4aab7a2b232b\",\n  \"gitObjectType\": \"blob\",\n  \"commitId\": \"e8a6d8791919ea1da0fc956540ae176af38c9264\",\n  \"path\": \"/readme.json\",\n  \"content\": \"{\\\"comments\\\":[{\\\"id\\\":1,\\\"body\\\":\\\"This is some awesome thinking!\\\",\\\"postId\\\":242,\\\"likes\\\":3,\\\"user\\\":{\\\"id\\\":105,\\\"username\\\":\\\"emmac\\\",\\\"fullName\\\":\\\"Emma Wilson\\\"}},{\\\"id\\\":2,\\\"body\\\":\\\"What terrific math skills you're showing!\\\",\\\"postId\\\":46,\\\"likes\\\":4,\\\"user\\\":{\\\"id\\\":183,\\\"username\\\":\\\"cameronp\\\",\\\"fullName\\\":\\\"Cameron Perez\\\"}},{\\\"id\\\":3,\\\"body\\\":\\\"You are an amazing writer!\\\",\\\"postId\\\":235,\\\"likes\\\":2,\\\"user\\\":{\\\"id\\\":1,\\\"username\\\":\\\"emilys\\\",\\\"fullName\\\":\\\"Emily Johnson\\\"}},{\\\"id\\\":4,\\\"body\\\":\\\"Wow! You have improved so much!\\\",\\\"postId\\\":31,\\\"likes\\\":1,\\\"user\\\":{\\\"id\\\":89,\\\"username\\\":\\\"braydenf\\\",\\\"fullName\\\":\\\"Brayden Fleming\\\"}},{\\\"id\\\":5,\\\"body\\\":\\\"Nice idea!\\\",\\\"postId\\\":212,\\\"likes\\\":1,\\\"user\\\":{\\\"id\\\":149,\\\"username\\\":\\\"wyattp\\\",\\\"fullName\\\":\\\"Wyatt Perry\\\"}},{\\\"id\\\":6,\\\"body\\\":\\\"You are showing excellent understanding!\\\",\\\"postId\\\":184,\\\"likes\\\":5,\\\"user\\\":{\\\"id\\\":110,\\\"username\\\":\\\"danielt\\\",\\\"fullName\\\":\\\"Daniel Taylor\\\"}},{\\\"id\\\":7,\\\"body\\\":\\\"This is clear, concise, and complete!\\\",\\\"postId\\\":172,\\\"likes\\\":1,\\\"user\\\":{\\\"id\\\":4,\\\"username\\\":\\\"jamesd\\\",\\\"fullName\\\":\\\"James Davis\\\"}},{\\\"id\\\":8,\\\"body\\\":\\\"What a powerful argument!\\\",\\\"postId\\\":233,\\\"likes\\\":0,\\\"user\\\":{\\\"id\\\":145,\\\"username\\\":\\\"lukec\\\",\\\"fullName\\\":\\\"Luke Cooper\\\"}},{\\\"id\\\":9,\\\"body\\\":\\\"I knew you could do it!\\\",\\\"postId\\\":207,\\\"likes\\\":3,\\\"user\\\":{\\\"id\\\":207,\\\"username\\\":\\\"jaces\\\",\\\"fullName\\\":\\\"Jace Smith\\\"}},{\\\"id\\\":10,\\\"body\\\":\\\"Wonderful ideas!\\\",\\\"postId\\\":87,\\\"likes\\\":0,\\\"user\\\":{\\\"id\\\":86,\\\"username\\\":\\\"noram\\\",\\\"fullName\\\":\\\"Nora Mills\\\"}},{\\\"id\\\":11,\\\"body\\\":\\\"It was a pleasure to grade this!\\\",\\\"postId\\\":156,\\\"likes\\\":8,\\\"user\\\":{\\\"id\\\":162,\\\"username\\\":\\\"mateob\\\",\\\"fullName\\\":\\\"Mateo Bennett\\\"}},{\\\"id\\\":12,\\\"body\\\":\\\"Keep up the incredible work!\\\",\\\"postId\\\":119,\\\"likes\\\":10,\\\"user\\\":{\\\"id\\\":90,\\\"username\\\":\\\"scarlettb\\\",\\\"fullName\\\":\\\"Scarlett Bowman\\\"}},{\\\"id\\\":13,\\\"body\\\":\\\"My goodness, how impressive!\\\",\\\"postId\\\":108,\\\"likes\\\":10,\\\"user\\\":{\\\"id\\\":87,\\\"username\\\":\\\"hunterg\\\",\\\"fullName\\\":\\\"Hunter Gordon\\\"}},{\\\"id\\\":14,\\\"body\\\":\\\"You're showing inventive ideas!\\\",\\\"postId\\\":20,\\\"likes\\\":8,\\\"user\\\":{\\\"id\\\":77,\\\"username\\\":\\\"jonathanp\\\",\\\"fullName\\\":\\\"Jonathan Pierce\\\"}},{\\\"id\\\":15,\\\"body\\\":\\\"You've shown so much growth!\\\",\\\"postId\\\":6,\\\"likes\\\":2,\\\"user\\\":{\\\"id\\\":17,\\\"username\\\":\\\"evelyns\\\",\\\"fullName\\\":\\\"Evelyn Sanchez\\\"}},{\\\"id\\\":16,\\\"body\\\":\\\"Interesting thoughts!\\\",\\\"postId\\\":14,\\\"likes\\\":5,\\\"user\\\":{\\\"id\\\":199,\\\"username\\\":\\\"viviang\\\",\\\"fullName\\\":\\\"Vivian Carter\\\"}},{\\\"id\\\":17,\\\"body\\\":\\\"I love your neat work!\\\",\\\"postId\\\":240,\\\"likes\\\":7,\\\"user\\\":{\\\"id\\\":155,\\\"username\\\":\\\"nicholase\\\",\\\"fullName\\\":\\\"Nicholas Edwards\\\"}},{\\\"id\\\":18,\\\"body\\\":\\\"Doesn't it feel good to do such great work?\\\",\\\"postId\\\":227,\\\"likes\\\":6,\\\"user\\\":{\\\"id\\\":134,\\\"username\\\":\\\"noramx\\\",\\\"fullName\\\":\\\"Nora Russell\\\"}},{\\\"id\\\":19,\\\"body\\\":\\\"First-rate work!\\\",\\\"postId\\\":245,\\\"likes\\\":1,\\\"user\\\":{\\\"id\\\":203,\\\"username\\\":\\\"novab\\\",\\\"fullName\\\":\\\"Nova Cooper\\\"}},{\\\"id\\\":20,\\\"body\\\":\\\"This is fascinating information!\\\",\\\"postId\\\":176,\\\"likes\\\":4,\\\"user\\\":{\\\"id\\\":39,\\\"username\\\":\\\"lucasg\\\",\\\"fullName\\\":\\\"Lucas Gray\\\"}},{\\\"id\\\":21,\\\"body\\\":\\\"You inspire me!\\\",\\\"postId\\\":229,\\\"likes\\\":0,\\\"user\\\":{\\\"id\\\":113,\\\"username\\\":\\\"miam\\\",\\\"fullName\\\":\\\"Mia Miller\\\"}},{\\\"id\\\":22,\\\"body\\\":\\\"This is right on target!\\\",\\\"postId\\\":9,\\\"likes\\\":5,\\\"user\\\":{\\\"id\\\":113,\\\"username\\\":\\\"miam\\\",\\\"fullName\\\":\\\"Mia Miller\\\"}},{\\\"id\\\":23,\\\"body\\\":\\\"What an astounding observation!\\\",\\\"postId\\\":33,\\\"likes\\\":1,\\\"user\\\":{\\\"id\\\":155,\\\"username\\\":\\\"nicholase\\\",\\\"fullName\\\":\\\"Nicholas Edwards\\\"}},{\\\"id\\\":24,\\\"body\\\":\\\"This is very well thought out!\\\",\\\"postId\\\":121,\\\"likes\\\":1,\\\"user\\\":{\\\"id\\\":169,\\\"username\\\":\\\"jaxonb\\\",\\\"fullName\\\":\\\"Jaxon Barnes\\\"}},{\\\"id\\\":25,\\\"body\\\":\\\"I can tell you've been practicing!\\\",\\\"postId\\\":247,\\\"likes\\\":9,\\\"user\\\":{\\\"id\\\":160,\\\"username\\\":\\\"claires\\\",\\\"fullName\\\":\\\"Claire Foster\\\"}},{\\\"id\\\":26,\\\"body\\\":\\\"You've come a long way!\\\",\\\"postId\\\":79,\\\"likes\\\":7,\\\"user\\\":{\\\"id\\\":59,\\\"username\\\":\\\"ethanf\\\",\\\"fullName\\\":\\\"Ethan Fletcher\\\"}},{\\\"id\\\":27,\\\"body\\\":\\\"I can tell you've been paying attention!\\\",\\\"postId\\\":55,\\\"likes\\\":9,\\\"user\\\":{\\\"id\\\":57,\\\"username\\\":\\\"nathand\\\",\\\"fullName\\\":\\\"Nathan Dixon\\\"}},{\\\"id\\\":28,\\\"body\\\":\\\"Reading this made my day!\\\",\\\"postId\\\":209,\\\"likes\\\":8,\\\"user\\\":{\\\"id\\\":177,\\\"username\\\":\\\"xavierw\\\",\\\"fullName\\\":\\\"Xavier Wright\\\"}},{\\\"id\\\":29,\\\"body\\\":\\\"This is very perceptive!\\\",\\\"postId\\\":31,\\\"likes\\\":2,\\\"user\\\":{\\\"id\\\":168,\\\"username\\\":\\\"lunah\\\",\\\"fullName\\\":\\\"Luna Perez\\\"}},{\\\"id\\\":30,\\\"body\\\":\\\"What an accomplishment!\\\",\\\"postId\\\":126,\\\"likes\\\":8,\\\"user\\\":{\\\"id\\\":89,\\\"username\\\":\\\"braydenf\\\",\\\"fullName\\\":\\\"Brayden Fleming\\\"}}],\\\"total\\\":340,\\\"skip\\\":0,\\\"limit\\\":30}\",\n  \"url\": \"https://dev.azure.com/myORG/77fc686c-02b2-422a-8a90-c2cad564c113/_apis/git/repositories/df59246f-22f1-4c49-b9dc-f55f80419b15/items?path=%2Freadme.json&versionType=Branch&versionOptions=None\",\n  \"_links\": {\n    \"self\": {\n      \"href\": \"https://dev.azure.com/myORG/77fc686c-02b2-422a-8a90-c2cad564c113/_apis/git/repositories/df59246f-22f1-4c49-b9dc-f55f80419b15/items?path=%2Freadme.json&versionType=Branch&versionOptions=None\"\n    },\n    \"repository\": {\n      \"href\": \"https://dev.azure.com/myORG/77fc686c-02b2-422a-8a90-c2cad564c113/_apis/git/repositories/df59246f-22f1-4c49-b9dc-f55f80419b15\"\n    },\n    \"blob\": {\n      \"href\": \"https://dev.azure.com/myORG/77fc686c-02b2-422a-8a90-c2cad564c113/_apis/git/repositories/df59246f-22f1-4c49-b9dc-f55f80419b15/blobs/529a003b36fb109bb7ef0ead557d4aab7a2b232b\"\n    }\n  }\n}"
  },
  {
    "objectID": "posts/creatingacustomllamafile/index.html",
    "href": "posts/creatingacustomllamafile/index.html",
    "title": "Creating a custom llamafile 🦙",
    "section": "",
    "text": "I recently watched the keynote/demo on llamfile which showed how local LLMs in a single executable. Ollama is great but the additional installation/maintenance overhead it brings can be seen as one of the negatives. That said, most of these tools currently are still in their infancy and with time, they will only get easier to install, use, scale and maintain.\n\nI soon figured that llama index already has an integration for working with llamafiles and a concise blog post on how to use a llamafile to build a rudimentary RAG system.\n\nllamafile-llamaindex\nllamafile-RAG\n\nAccording to me, the major advantage of a llamafile is that it exposes an API service in addition to a web user interface. This means that we could use the API endpoints from a llamafile and use it in Retrieval Augmented Generation (RAG) projects or any other LLM use case. The available API endpoints are described in this link."
  },
  {
    "objectID": "posts/creatingacustomllamafile/index.html#creating-your-own-llamafiles",
    "href": "posts/creatingacustomllamafile/index.html#creating-your-own-llamafiles",
    "title": "Creating a custom llamafile 🦙",
    "section": "Creating your own llamafiles",
    "text": "Creating your own llamafiles\nAll the above is great to know, but how do we put it to use by creating our own llamafile? This sections covers some of the missing steps in the readme.md of the llamafile project. I recommend you to also read the entire readme as it covers some known workarounds to get things going.\nLets begin!\n\nStep 1 - Install llamafile\nClone the llamafile repo from Mozilla. Change directories and run the make install command with sudo permissions.\ngit clone https://github.com/Mozilla-Ocho/llamafile\ncd llamafile\nsudo make install\nThe above command will install all the necessary binaries to this folder /usr/local/bin/llamafile with the following terminal output.\nmkdir -p /usr/local/bin\ninstall o//llamafile/zipalign /usr/local/bin/zipalign\ninstall o//llamafile/tokenize /usr/local/bin/llamafile-tokenize\ninstall o//llama.cpp/main/main /usr/local/bin/llamafile\ninstall o//llama.cpp/imatrix/imatrix /usr/local/bin/llamafile-imatrix\ninstall o//llama.cpp/quantize/quantize /usr/local/bin/llamafile-quantize\ninstall o//llama.cpp/llama-bench/llama-bench /usr/local/bin/llamafile-bench\ninstall build/llamafile-convert /usr/local/bin/llamafile-convert\ninstall build/llamafile-upgrade-engine /usr/local/bin/llamafile-upgrade-engine\ninstall o//llama.cpp/perplexity/perplexity /usr/local/bin/llamafile-perplexity\ninstall o//llama.cpp/llava/llava-quantize /usr/local/bin/llava-quantize\nmkdir -p /usr/local/share/man/man1\ninstall -m 0644 llamafile/zipalign.1 /usr/local/share/man/man1/zipalign.1\ninstall -m 0644 llama.cpp/main/main.1 /usr/local/share/man/man1/llamafile.1\ninstall -m 0644 llama.cpp/imatrix/imatrix.1 /usr/local/share/man/man1/llamafile-imatrix.1\ninstall -m 0644 llama.cpp/quantize/quantize.1 /usr/local/share/man/man1/llamafile-quantize.1\ninstall -m 0644 llama.cpp/perplexity/perplexity.1 /usr/local/share/man/man1/llamafile-perplexity.1\ninstall -m 0644 llama.cpp/llava/llava-quantize.1 /usr/local/share/man/man1/llava-quantize.1\n\n\nStep 2 - Check version\nOpen a new terminal window and check the version of llamafile you have.\n$ llamafile --version  \nllamafile v0.8.9\n\n\nStep 3 - Creating the args file\nThe .args file allows you to customize the llamafile you want to generate. The following is an example content of the .args file\n-m\nMeta-Llama-3-8B-Instruct-IQ4_NL.gguf\n--mmproj\nLLaMA3-8B_mmproj-Q4_1.gguf\n--host\n0.0.0.0\n-ngl\n9999\n...\n\n\n\n\n\n\nNote\n\n\n\nTo create another llamafile just find the appropriate model you want to use and download the .gguf file from HuggingFace and follow the same steps mentioned above.\nThe --mmproj is optional but the m (model) option is mandatory in the .args file\n\n\n\n\nStep 4 - Building on the llamafile binary\nWe first copy the llamafile and give it a new name.\ncp /usr/local/bin/llamafile llama3.llamafile \nThe working folder should contain the following files.\nLlamafileExperiments  \n│\n└───.args\n└───Meta-Llama-3-8B-Instruct-IQ4_NL.gguf\n└───LLaMA3-8B_mmproj-Q4_1.gguf\n└───llama3.llamafile \nNow use the zipalign binary which is an alternative to zip. This library is designed to concatenate gigabytes of LLM weights to an executable. If you observe the output from Step 1, you see that the zipalign binary was also saved to /usr/local/bin/zipalign. Therefore, your terminal should recognize the zipalign command. Read more about zipalign by using the command man zipalign.\nzipalign -j0 \\\n  llama3.llamafile \\\n  Meta-Llama-3-8B-Instruct-IQ4_NL.gguf \\\n  LLaMA3-8B_mmproj-Q4_1.gguf \\\n  .args\nThats all! A llama3.llamafile will be generated in that current folder. This can then be run by using any of the three ways mentioned above\n./llama3.llamafile\n\n\n\nLlamafileOutput\n\n\n\nPorting the output llamafile to windows\nIf you want to use the output file in Windows then add the .exe extension to the generated llamafile. For example, llama3.llamafile to llama3.llamafile.exe.\n\n\n\nCredits\nAll this magic with llamafile is possible because of the two previous open-source projects, namely llama.cpp and cosmopolitan_Libc. Kudos to the developers and maintainers! Not to forget all the credit to realize llamafile goes to the Mozilla builders project.\n\n\n\n\n\nflowchart LR\n\nsubgraph Mozilla Builders Project\nc\nend\n\nLlama.cpp --&gt; c[fa:fa-heart llamafile]\nCosmopolitan_Libc --&gt; c[fa:fa-heart llamafile]"
  },
  {
    "objectID": "posts/berylglmt1300travelrouter/index.html",
    "href": "posts/berylglmt1300travelrouter/index.html",
    "title": "Beryl GL MT-1300 travel router",
    "section": "",
    "text": "After weeks of researching for the ideal travel router in March of this year, I decided to buy the Beryl GL MT1300 travel router from Gl-inet.\n\nRequirements for a travel router\nMy requirements were quite basic:\n\nWi-fi 6 or higher\nFunction as a repeater/range extender so that I could also use it also at home when not traveling\nFunction as a VPN client (Tailscale preferably)\nA functional UI for administration\nOpen source firmware which can ensure long-term updates for the device\nToggle switch to activate the VPN client\nCan be connected to internet via a RJ45 cable or via Wi-fi\nA positive if it also came with compatible power plugs\nPositive reviews and security reviews\n\n\n\nTravel router networking\n\n\n\n\n\n---\ntitle: Travel router to home router\n---\ngraph LR\n    subgraph Remote network - cable or wi-fi\n        Beryl-GL-MT1300\n        RemoteDevice1 \n        RemoteDevice2 \n        RemoteDeviceN\n\n    end\n\n    subgraph Home network\n        HomeRouter\n    end\n\nBeryl-GL-MT1300&lt;--&gt;| Tailscale VPN| HomeRouter[\"Asus 86U\"]\nRemoteDevice1 &lt;-.-&gt; Beryl-GL-MT1300 \nRemoteDevice2 &lt;-.-&gt; Beryl-GL-MT1300 \nRemoteDeviceN &lt;-.-&gt; Beryl-GL-MT1300 \n\n\n\n\n\n\n\nThe plan was to use my existing VPN on my ASUS router and connect to the it via the travel router’s VPN client functions.\n\n\n\n\n\n\nThis meant that all devices connected to the travel router in the remote location can browse the internet via the VPN (Tailscale).\nI could also avoid creating separate Tailscale configurations for all devices/users who need access.\nAdditionally, since the VPN client runs on the travel router, the mobile device would avoid draining the battery as it usually would when a VPN is connected. Fantastic!\n\n\n\nThe only preparation required from my end was to connect all the devices to the travel routers wi-fi SSID while connected to the home network (before the trip).\n\n\nThe first experience\nUnder a work trip in May, I could finally use this router. Everything went as planned, the remote wifi (hotel wi-fi) was connected to the travel router. Via the toggle switch the Tailscale client was started and all my devices connected to the travel router at the remote location without any hiccups.\nOverall, I am quite satisfied with the performance of this tiny travel router and this certainly will be a part of my future travels. I would happily recommend this to my contacts."
  },
  {
    "objectID": "posts/ventoybootabelusb/index.html",
    "href": "posts/ventoybootabelusb/index.html",
    "title": "Ventoy bootable USB",
    "section": "",
    "text": "During the past 10 years, I probably have written OS images to pendrives over 300 times. This to either fresh install or reinstall OS on windows, linux, mac and on raspberrypi computers. However, in 2024 I came to know of a tool called Ventoy! This could have saved me a lot of time and effort in the past.\nVentoy is a open source tool to create bootable usb drives which supports images in ISO/WIM/IMG/VHD(x)/EFI formats.\nUnlike traditional way of creating an USB bootable drive for each OS image, a Ventoy USB drive can contain all different kind of OS images in one pendrive. It can even organize them as you wish. On boot, you are greeted with a simple GUI where you get the option to choose the image you want to boot into.\nI went from using over 8 pendrives to just 1. The simplicity it has brought in my “distro-hopping” workflow is just bliss. I have organized the images files as shown below.\nVentoyDrive  \n│\n└───Windows\n│   │   Windows 10\n│   │   Windows 11\n│   \n└───Linux\n    │   Ubuntu 22.4\n    │   Ubuntu 24.4\n    │   Kubuntu.24.4\n    │   PopOS 22.4\n    │   LMDE 6\n    │   LinuxMint 21.3\n    │   Proxmox 8.2\n    │   Gparted\n    |   MxLinux\n\n\n\n\n\n\n\nNote\n\n\n\nAnother great feature of ventoy is that in addition to the OS image files, you are free to use the Ventoy USB drive as a normal USB drive.\nWith a large enough pendrive, ventoy can act as both a bootable usb drive and a regular usb storage device. Fantastic!"
  },
  {
    "objectID": "posts/notesontimeseries/index.html",
    "href": "posts/notesontimeseries/index.html",
    "title": "Notes on time series analysis",
    "section": "",
    "text": "mindmap\n  root((Time Series Analysis))\n    Databases\n        TimeScaleDB\n        InfluxDB\n    Python tools\n        Altair\n        Pandas\n        DuckDB\n        Polars\n        Scipy\n        Gekko\n        Matplotlib\n    Core concepts\n        Trend\n        Cycle\n        Variation\n        Statistical movements\n        Stationary and non-stationary\n        Seasonality\n        Auto-Correlation\n    Forcasting\n        Auto Regressive Integrated Moving Average - ARIMA\n        Exponential smoothing"
  },
  {
    "objectID": "posts/notesontimeseries/index.html#a-mindmap-of-time-series-analysis",
    "href": "posts/notesontimeseries/index.html#a-mindmap-of-time-series-analysis",
    "title": "Notes on time series analysis",
    "section": "",
    "text": "mindmap\n  root((Time Series Analysis))\n    Databases\n        TimeScaleDB\n        InfluxDB\n    Python tools\n        Altair\n        Pandas\n        DuckDB\n        Polars\n        Scipy\n        Gekko\n        Matplotlib\n    Core concepts\n        Trend\n        Cycle\n        Variation\n        Statistical movements\n        Stationary and non-stationary\n        Seasonality\n        Auto-Correlation\n    Forcasting\n        Auto Regressive Integrated Moving Average - ARIMA\n        Exponential smoothing"
  },
  {
    "objectID": "posts/notesontimeseries/index.html#core-concepts-explained-visually",
    "href": "posts/notesontimeseries/index.html#core-concepts-explained-visually",
    "title": "Notes on time series analysis",
    "section": "Core concepts explained visually",
    "text": "Core concepts explained visually\n\nTime series\nObserved values of the same variable collected at regular time intervals.\nIn the below example, revenue is the variable observed with a monthly time intervals given a year.\n\n\n\n\n\n---\nconfig:\n    xyChart:\n        width: 700\n        height: 350\n    themeVariables:\n        xyChart:\n            titleColor: \"#ff0000\"\n---\nxychart-beta\n    title \"Sales revenue\"\n    x-axis [jan, feb, mar, apr, may, jun, jul, aug, sep, oct, nov, dec]\n    y-axis \"Revenue (in $)\" 4000 --&gt; 13000\n    line [5000, 6200, 7900, 8800, 10300, 11500, 12200, 11600, 10800, 10300, 9000, 8200]\n\n\n\n\n\n\n\n\nTime series analysis\nThe method of analyzing a timestamped dataset to observe/forecast/predict past and future values for the observed variable. Hence this analysis can be used as a decision support system.\n\nTrend\nPositive secular trend\n\n\n\n\n\n---\nconfig:\n    xyChart:\n        width: 700\n        height: 350\n    themeVariables:\n        xyChart:\n            titleColor: \"#ff0000\"\n---\nxychart-beta\n    title \"Sales revenue (positive secular trend)\"\n    x-axis [jan, feb, mar, apr, may, jun, jul, aug, sep, oct, nov, dec]\n    y-axis \"Revenue (in $)\" 4000 --&gt; 13000\n    line [5000, 5200, 6100, 5900, 6800, 7500, 8200, 7900, 8600, 8400, 9100, 9500]\n\n\n\n\n\n\nNegative secular trend\n\n\n\n\n\n---\nconfig:\n    xyChart:\n        width: 700\n        height: 350\n    themeVariables:\n        xyChart:\n            titleColor: \"#ff0000\"\n---\nxychart-beta\n    title \"Sales revenue (negative secular trend)\"\n    x-axis [jan, feb, mar, apr, may, jun, jul, aug, sep, oct, nov, dec]\n    y-axis \"Revenue (in $)\" 4000 --&gt; 20000\n    line [14000, 13600, 13900, 13200, 13500, 12800, 13100, 12500, 12700, 12100, 11800, 11500]\n\n\n\n\n\n\n\n\nSeasonality\nTime series data can change depending on the seasons/seasonal pattern.\nSeasonal positive secular trend\n\n\n\n\n\n---\nconfig:\n    xyChart:\n        width: 700\n        height: 350\n    themeVariables:\n        xyChart:\n            titleColor: \"#ff0000\"\n---\nxychart-beta\n    title \"Sales revenue (seasonal postive secular trend)\"\n    x-axis [jan, feb, mar, apr, may, jun, jul, aug, sep, oct, nov, dec]\n    y-axis \"Revenue (in $)\" 4000 --&gt; 20000\n    line [14000, 13600, 14800, 14200, 15100, 15600, 15300, 14900, 15800, 15400, 15900, 16200]\n\n\n\n\n\n\nSeasonal negative secular trend\n\n\n\n\n\n---\nconfig:\n    xyChart:\n        width: 700\n        height: 350\n    themeVariables:\n        xyChart:\n            titleColor: \"#ff0000\"\n---\nxychart-beta\n    title \"Sales revenue (seasonal negative secular trend)\"\n    x-axis [jan, feb, mar, apr, may, jun, jul, aug, sep, oct, nov, dec]\n    y-axis \"Revenue (in $)\" 4000 --&gt; 20000\n   line [14000, 13200, 13900, 12800, 13500, 14100, 12600, 13200, 13800, 12400, 12900, 13300]\n\n\n\n\n\n\n\n\nCyclic\n\n\n\n\n\n---\nconfig:\n    xyChart:\n        width: 700\n        height: 350\n    themeVariables:\n        xyChart:\n            titleColor: \"#ff0000\"\n---\nxychart-beta\n    title \"Sales revenue (Cyclic trend)\"\n    x-axis [jan, feb, mar, apr, may, jun, jul, aug, sep, oct, nov, dec]\n    y-axis \"Revenue (in $)\" 4000 --&gt; 20000\n    line [10000, 10800, 11900, 13200, 14300, 15100, 15500, 15400, 14800, 13900, 12800, 11700]"
  },
  {
    "objectID": "posts/notesontimeseries/index.html#resources",
    "href": "posts/notesontimeseries/index.html#resources",
    "title": "Notes on time series analysis",
    "section": "Resources",
    "text": "Resources\nHere are some of the resources which helped me learn the basic concepts of time series analysis with no specific order.\nIntroduction to time series analysis \nLecture on time series analysis \nPatterns and trends in time series plots \nWhen I learn an interesting aspect of time series analysis, I will update this post. For now, time series analysis was fun learning. I have barely scratched the surface here.\nMore fun to be had!"
  },
  {
    "objectID": "posts/searchandvalidationofanorwegianidentitynumber/index.html",
    "href": "posts/searchandvalidationofanorwegianidentitynumber/index.html",
    "title": "Is that NiN valid?",
    "section": "",
    "text": "Norwegian Identity Number (NiN) is a interesting unique identifier. In this post, I would like to show how to parse an input text and validate a NiN using Python."
  },
  {
    "objectID": "posts/searchandvalidationofanorwegianidentitynumber/index.html#background",
    "href": "posts/searchandvalidationofanorwegianidentitynumber/index.html#background",
    "title": "Is that NiN valid?",
    "section": "Background",
    "text": "Background\nAn NiN is a 11 digit number allocated to Norwegian citizens and residents with the following properties * The first 6 digits signify a date * The 7th, 8th and the 9th digit signify a individual number specific to each person * The 9th digit is special as it also encodes the two biological gender of the person (male or female) * The fascinating part is the last two digits which are known as control digits, which is mathematically determined\nLet’s continue with the same example NiN used in the reference article from matematikk.no and build a visual representation of a NiN.\n\n\n\n\n\ngraph TD\nsubgraph NiN[02016126007]\nend\n\nNiN --&gt;|Birthday| BirthDay(020161)\nNiN --&gt;|Personal number| Personnummer(26007)\n\nBirthDay --&gt;|BirthDate-d1d2| Date(02)\nBirthDay --&gt;|BirthMonth-m1m2| Month(01)\nBirthDay --&gt;|BirthYear-y1y2| Year(61)\n\nPersonnummer --&gt;|Individual number&lt;br&gt;Decade specific| IndividualNumber(260)\nIndividualNumber --&gt;|RandomlyGenerated&lt;br&gt;DependentOnNumberOfBirthsonTheDay|RandomlyGenerated(26)\nIndividualNumber --&gt;|GenderEncoding&lt;br&gt;Even=Women=0,2,4,6,8&lt;br&gt;Odd=Men=1,3,5,7,9|Gender(0)\nPersonnummer --&gt;|Control digits| ControlDigits(07)\n\nControlDigits --&gt;|\"sum(dotproduct)MOD11&lt;br&gt;if result==0:&lt;br&gt;c1=0&lt;br&gt;else:&lt;br&gt;c1=11-result\"| digit10[0]\nControlDigits --&gt;|\"sum(dotproduct)MOD11&lt;br&gt;if result==0:&lt;br&gt;c2=0&lt;br&gt;else:&lt;br&gt;c2=11-result\"| digit11[7]\n\n\n\n\n\n\n\n\nSpecimen calculation\nThe two multiplier list used are\nc1_multipliers = [3, 7, 6, 1, 8, 9, 4, 5, 2]\nc2_multipliers = [5, 4, 3, 2, 7, 6, 5, 4, 3, 2]\nLets represent the input string “02016126007” as an array/list of int\nNiN = [0,2,0,1,6,1,2,6,0,0,7]\n\nCalculating c1\nremainder = DotProduct(NiN[0:9],c1_multipliers)%11\nc1 = 0 if remainder == 0 else 11 - remainder\nc1 = 0\n\n\nCalculating c2\nremainder = DotProduct(NiN[0:10],c2_multipliers)%11\nc2 = 0 if remainder == 0 else 11 - remainder\nc2 = 7\nIn the case of 02016126007, we see that the last 2 digits match with c1c2(07). Hence this is a valid NiN!"
  },
  {
    "objectID": "posts/searchandvalidationofanorwegianidentitynumber/index.html#building-a-parser",
    "href": "posts/searchandvalidationofanorwegianidentitynumber/index.html#building-a-parser",
    "title": "Is that NiN valid?",
    "section": "Building a parser",
    "text": "Building a parser\n\nTo make it easier to read sometimes the NiN can also be written with a space in between the date and individual numbers like so 020161 26007. Therefore the parsing logic we build needs to consider this variation as well.\n\nTo check if a text string contains one or more valid NiN’s we can generate three simple methods. ### 1. Check the first 6 digits are valid\n\ndef is_valid_date(date_str) -&gt; bool:\n    \"\"\"\n    Validates whether a given string follows the specified date format.\n\n    This function checks if `date_str` conforms to the 'DDMMYY' format and ensures the date is valid.\n    It attempts to parse the string using Python's datetime library and returns True if successful; otherwise,\n    it catches a ValueError exception and returns False.\n\n    Args:\n        date_str (str): The date string to be validated.\n\n    Returns:\n        bool: True if `date_str` matches the 'DDMMYY' format and is a valid date, False otherwise.\n\n    Example:\n        is_valid = is_valid_date(\"310323\")\n        # This will return True since \"31-03-2023\" is a valid date in 'DDMMYY' format.\n\n    Note:\n        - The function assumes that `date_str` should be exactly 6 characters long to match the 'DDMMYY' format.\n        - Checks are performed for invalid dates like \"310429\" (April 30th does not exist).\n    \"\"\"\n    from datetime import datetime \n\n    if len(date_str) != 6:\n        return False\n\n    try:\n        # Parse the date string\n        date_obj = datetime.strptime(date_str, \"%d%m%y\")\n\n        # Check if the day is valid for the given month and year\n        if date_obj.day != int(date_str[:2]) or date_obj.month != int(date_str[2:4]):\n            return False\n\n        return True\n    except ValueError:\n        return False\n\n\n2. Check if the control numbers are valid in the input string\n\ndef check_nin_control_numbers(text: str) -&gt; bool:\n    \"\"\"\n    Validates a Norwegian National Identification Number (NIN) based on control digits.\n\n    This function checks the validity of a Norwegian National Identification Number (NIN) by calculating\n    two control digits using predefined multipliers and verifying them against the provided NIN.\n\n    Args:\n        text (str): A string representing the NIN to be validated. The NIN should be 11 digits long.\n\n    Returns:\n        bool: True if the NIN is valid, False otherwise.\n\n    Example:\n        &gt;&gt;&gt; check_nin_control_numbers(\"01010112345\")\n        True\n    \"\"\"\n    if len(text) != 11:\n        return False\n\n    def calculate_control_digit(digits, multipliers):\n        total = sum(int(d) * m for d, m in zip(digits, multipliers))\n        remainder = total % 11\n        return 0 if remainder == 0 else 11 - remainder\n\n    k1_multipliers = [3, 7, 6, 1, 8, 9, 4, 5, 2]\n    k2_multipliers = [5, 4, 3, 2, 7, 6, 5, 4, 3, 2]\n\n    k1 = calculate_control_digit(text[:9], k1_multipliers)\n    k2 = calculate_control_digit(text[:9] + str(k1), k2_multipliers)\n\n    return text[9:] == f\"{k1}{k2}\"\n\n\n\n3. Using regex to extract potential 11 digit numbers (with or without spaces after 6th digit) and we invoke the above to methods\n\ndef find_norwegian_ss_number(text: str) -&gt; list[str]:\n    \"\"\"\n    Finds and extracts Norwegian Social Security Numbers (SSNs) from a given text.\n\n    This method searches through the input text to identify patterns that match\n    possible formats of Norwegian SSNs. The recognized formats include:\n    - Continuous 11-digit numbers (`ddddddmmmyyy`),\n    - Split format with space between day/month and year (`dddd dd mmmyyy`), and\n    - An alternative split format used in some historical contexts (`dddd mm yyyy`).\n\n    It returns a list of extracted SSNs, where all spaces are removed for consistency.\n\n    Parameters:\n    text (str): The input string from which Norwegian SSNs will be extracted.\n                This can include any textual content that might contain the target patterns.\n\n    Returns:\n    list: A list of strings, each representing an identified and formatted SSN with\n        no internal spaces. If no valid SSNs are found, returns an empty list.\n\n    Example:\n    &gt;&gt;&gt; text = \"Some example numbers 01010112345, 0101 01 012345, and 0101 02 1987.\"\n    &gt;&gt;&gt; find_norwegian_ss_number(text)\n    ['01010112345', '01010123456', '0101021987']\n    \"\"\"\n    pattern = re.compile(r\"\\b(\\d{6} \\d{5}|\\d{11}|\\d{4} \\d{2} \\d{5})\\b\")\n    matches = pattern.findall(text)\n\n    results = []\n    for match in matches:\n        if len(match) &gt; 0:\n            # Join the match parts (in case of split formats) into a continuous string\n            formatted_match = \"\".join(filter(None, match))\n            first_six_digits = formatted_match[:6]\n            if is_valid_date(\n                first_six_digits\n            ) and check_ss_control_numbers(formatted_match):\n                results.append(formatted_match)\n    return results\n\n\n\n\n\n\n\nCaution\n\n\n\nAlthough the above approach works currently, by the year 2040 the current number series can no longer generate unique numbers. To counter this, a set of alterations have been proposed by the tax authorities and will be implemented by the year 2032.\n\nThe new number series will still be 11 digits long\nThe gender encoding will be removed\nThe information regarding the person’s birthday in the NiN will also be removed\n\n\n\n\n\n\n\n\n\nAll previously valid NiN’s will remain valid\nAll organizations using NiN’s in their systems will need to adopt newer validation methods which support both the older and the newer series of NiN’s as mentioned by skatteetaten"
  },
  {
    "objectID": "posts/searchandvalidationofanorwegianidentitynumber/index.html#resources",
    "href": "posts/searchandvalidationofanorwegianidentitynumber/index.html#resources",
    "title": "Is that NiN valid?",
    "section": "Resources",
    "text": "Resources\n\nStore norske leksikon\nEr fødselsnummeret gyldig?\nDagens system med individ- og kontrollnummer\nNytt fødselsnummer fra 2032"
  },
  {
    "objectID": "posts/notesonpracticalstatisticsfordatascientist/index.html",
    "href": "posts/notesonpracticalstatisticsfordatascientist/index.html",
    "title": "Notes on Practical Statistics for Data Scientists 📗",
    "section": "",
    "text": "This is my personal notes of the book Practical Statistics for Data Scientists - 50+ Essential Concepts Using R and Python by Peter Bruce, Andrew Bruce, Peter Gedeck.\nI will update this post, as I study and digest the contents of this book."
  },
  {
    "objectID": "posts/notesonpracticalstatisticsfordatascientist/index.html#chapter-1---exploratory-data-analysis",
    "href": "posts/notesonpracticalstatisticsfordatascientist/index.html#chapter-1---exploratory-data-analysis",
    "title": "Notes on Practical Statistics for Data Scientists 📗",
    "section": "Chapter 1 - Exploratory Data Analysis",
    "text": "Chapter 1 - Exploratory Data Analysis\nJohn W. Tukey established the field of exploratory data analysis through his 1977 publication exploratory data analysis, in which he introduced methods to explore a dataset by using plots and summary statistics (mean, median etc.). Later in 2015, one of Tukey’s former undergraduate student David Donoho published a summary article 50 years of Data Science showing the genesis and developments of data science as a field.\n\nBefore analyzing the data it is important to identify the type of data to be studied.\nType of data can influence the kind of data analysis methods which can be used to explore the data.\n\nThe figure below provides the taxonomy of data types.\n\n\n\n\n\nmindmap\n  root((Data types))\n    Numeric \n        Continuous (Continuous - Data that can contain any value in an interval)\n        Discrete (Discrete - Data that can contain only integer values)\n    Categorical\n        Binary (Binary - Data that contain just two categories )\n        Ordinal (Ordinal - Data that is explicitly ordered)\n\n\n\n\n\n\n\n\nOrdinal data\nThe interesting data type above is the ordinal data type where the order of the data is important. Here is an example of ordinal data type using sklearn.\n\nfrom sklearn.preprocessing import OrdinalEncoder\nenc = OrdinalEncoder()\nX = [['Male', 1], ['Female', 3], ['Female', 2]]\nenc.fit(X)\nenc.categories_\n\nOrdinalEncoder()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.OrdinalEncoder?Documentation for OrdinalEncoderiFitted\n        \n            \n                Parameters\n                \n\n\n\n\ncategories \n'auto'\n\n\n\ndtype \n&lt;class 'numpy.float64'&gt;\n\n\n\nhandle_unknown \n'error'\n\n\n\nunknown_value \nNone\n\n\n\nencoded_missing_value \nnan\n\n\n\nmin_frequency \nNone\n\n\n\nmax_categories \nNone\n\n\n\n\n            \n        \n    \n\n\n[array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n\n\n\nenc.transform([['Female', 3], ['Male', 1]])\n\narray([[0., 2.],\n       [1., 0.]])\n\n\nExplanation of enc.transform First sample: ['Female', 3]\nGender column: 'Female' is the first category in enc.categories_ → encoded as 0\nNumber column: 3 is the third category in enc.categories_[1] → encoded as 2\nResult: [0., 2.]\nSecond sample: ['Male', 1]\nGender column: 'Male' is the second category → encoded as 1\nNumber column: 1 is the first category → encoded as 0\nResult: [1., 0.]\n\nenc.inverse_transform([[1, 1], [0, 2]])\n\narray([['Male', 2],\n       ['Female', 3]], dtype=object)\n\n\n\n\nRectangular data\nTypically analysis in data science focuses on rectangular data. It is a two-dimensional matrix containing records in form of rows and features/variables in form of columns. Rectangular data is usually the result of some preprocessing of unstructured data. Following are the key terms in rectangular data.\n\n\n\n\n\nmindmap\n  root((Rectangular data))\n    Dataframe \n        A basic data structure used in statistical and machine learning models\n    Feature\n        Each column within a table is referred to as a feature\n    Outcome\n        The dependent variable. Output variable which is dependent on one or many features. Also called target, response, output. \n    Records\n        Each row in a table. Can be defined as singular case,  scenario, observation, pattern or sample\n\n\n\n\n\n\n\n\nNon rectangular data structures\n\nTime Series data records sequential measurements of a same variable with time. This kind of data is used to create statistical forecasting models. An example is a IOT sensor capturing temperature data every 2 minutes perpetually. Such data structures always need to include a time at which the record was captured.\nSpatial data can be used to create location based analytics. The object under observation can be for example a house or a point of interest in a map and its spatial coordinates.\nGraph/ Network are used to represent abstract relationships between the object under observation. An example can be a social network of a person showing how many contacts or friends that person has and how often he/she interacts with them. These types of data is useful in recommender systems and optimization problems.\n\nAll these three can also be combined in a single use case. For example, Google maps can store spatial data in a time series manner for a person and include a graph/ network data on how the user interacts with other spatial objects (shops, landmarks) when they travel 60 kms away from their home.\n\nDifference in terminologies\nStatisticians use predicator variables to predict a response or dependent variable and data scientist use features to predit a target\nThe term sample to a computer scientist signifies a single row while a sample to a statistician means a collection of rows.\nGraph in statistics can mean plots or visualization and not just connections of entities as it is in computer science or information technology.\n\n\n\nEstimates of location\n\n\n\n\n\nmindmap\n  root((Estimates of location))\n    Mean \n        Sum of all observations divided by total number of observations. Also known as **average**\n    Trimmed mean\n        By removing the first n and the last n observations from a sorted set of observations, trimmed mean can then be calculated on the remaining observations divided by the total number of remaining observations. This helps us eliminate very high or very low values in the dataset. This can also result in a trimmed mean, which is closer to the median value.\n    Weighter mean\n        Sum of all observations multiplied by corresponding weights and divided by sum of the weights. Also known as **weighted average**\n    Median\n        The middle most observation in the ascendingly sorted dataset. Also known as 50th percentile and a robust estimate of location.\n    Weighted median\n        The value such that one-half of the sum of the weights lies below or above the sorted data\n    Percentile\n        The value which signifies the percentage of data which lies below or equal to a given data value. Can also be called as quantile.\n    Robust\n        Not sensitive to extreme observations\n    Outlier\n        Observation/s which are located in extreme ranges when compared with most of the observations\n\n\n\n\n\n\n\\[\n\\begin{equation}\n  Mean = \\bar{x}= \\frac{\\sum_{i=1}^n x_i}{n}\n  \\label{eq:mean}\n\\end{equation}\n\\]\n\\[\n\\begin{equation}\n  TrimmedMean = \\bar{x}= \\frac{\\sum_{i=p+1}^{n-p} x_i}{n - 2p}\n  \\label{eq:trimmedmean}\n\\end{equation}\n\\]\n\\[\n\\begin{equation}\n  WeightedMean = \\bar{x}_w= \\frac{\\sum_{i=1}^{n} w_i*x_i}{\\sum_{i=1}^{n} w_i}\n  \\label{eq:weightedmean}\n\\end{equation}\n\\]\n\nDifference in terminologies\nDatascientist measure and statisticians estimates. Statisticians account for uncertainities in the analysis whereas concrete business objectives are the focus for datascientists.\n\n\nimport pandas as pd\nfrom scipy.stats import trim_mean\nimport numpy as np\nimport wquantiles\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/gedeck/practical-statistics-for-data-scientists/refs/heads/master/data/state.csv\")\n\ndf_mean = df[\"Population\"].mean()\ndf_median = df[\"Population\"].median()\ntrimmed_mean = trim_mean(df[\"Population\"], 0.1)\nweighted_mean = np.average(df[\"Murder.Rate\"],weights=df[\"Population\"])\nweighted_median = wquantiles.median(df[\"Murder.Rate\"],weights=df[\"Population\"])\n\n\n\nEstimates of variability\nVariability (aka dispersion) is the second dimension of summarizing a feature. The first one is Location. Dispersion measures if the data values are tightly clustered or spread out.\n\n\n\n\n\nmindmap\n  root((Estimates of variability))\n    Deviations \n        The difference between the observed values and the estimates of locations. Also known as residuals or errors. \n    Variance\n        The sum of squared deviations from the mean divided by n-1 where n is the number of observations. Also known as mean-squared-error\n    Standard deviation\n        The square root of variance\n    Mean absolute deviation\n        The absolute values of deviations from the mean. Also known as L1 norm or Manhattan norm\n    Median absolute deviation from the median\n        The median absolute values of the deviations from the median\n    Range\n        The difference between the largest and the smallest observations in the data set\n    Order statistics\n        Metrics based on sorted data from smallest ti biggest. Also known as ranks. \n    Interquartile range\n        The difference between the 75th percentile and 25th percentile"
  },
  {
    "objectID": "posts/notesonpracticalstatisticsfordatascientist/index.html#related-literature",
    "href": "posts/notesonpracticalstatisticsfordatascientist/index.html#related-literature",
    "title": "Notes on Practical Statistics for Data Scientists 📗",
    "section": "Related literature",
    "text": "Related literature\n\nJon Tukey : Exploratory Data Analysis\nDavid Donoho : 50 years of Data Science\nBruce, P., Bruce, A., & Gedeck, P. (2020). Practical statistics for data scientists: 50+ essential concepts using R and Python. O’Reilly Media."
  },
  {
    "objectID": "posts/zshwithpowerlevel10k/index.html",
    "href": "posts/zshwithpowerlevel10k/index.html",
    "title": "ZSH with Powerlevel10k",
    "section": "",
    "text": "My terminal now looks something like this across all of my devices. Thanks to zsh, ohmyzsh and p10k projects!\n\nStandard directory view \nStandard git directory and virtualenv \n\n\nSome years ago I had tested the zsh terminal and the Powerlevel10K theme to go with it, but for reasons unknown I did not use it actively. In recent times, my time within the terminal has increased, hence it was time to make the terminal look a little prettier than the standard bash terminal.\n\nInstallation script\nThe script below installs the zsh terminal and installs the powerlevel10K theme. We also need to install the ohmyzsh project. Finally, the zsh terminal is set as the default terminal. This last part requires sudo permissions/authentication and cannot be completed unattended.\n#!/bin/bash\n\nsudo apt install zsh\ncd\ngit clone --depth=1 https://github.com/romkatv/powerlevel10k.git ~/powerlevel10k\necho 'source ~/powerlevel10k/powerlevel10k.zsh-theme' &gt;&gt;~/.zshrc\ngit clone --depth=1 https://github.com/romkatv/powerlevel10k.git ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/themes/powerlevel10k\nchsh -s /bin/zsh\np10k configure can then be used to configure the terminal as one pleases. This runs the configuration wizard.\n\n\n\np10kconfigure\n\n\nPlugins used in .zshrc are independent of the options chosen in the p10k configure. For example, I needed the auto suggestions in ohmyzsh, python environment and pip zsh plugins. Installing plugins is a simple as setting the following line in the .zshrc file. Here is a list of available plugins from ohmyzsh project.\n.zshrc\n....\nplugins=(git zsh-autosuggestions zsh-syntax-highlighting pipenv pip)\n...\n\n\nCustomized p10K configure\nMy p10k options are: awesome-fontconfig, large icons, rainbow, unicode, 24h time, blurred heads, blurred tails, 2 lines, solid, full frame, light-ornaments, sparse, many icons, fluent, instant_prompt=verbose.\n\n\n\n\n\n\nNote\n\n\n\nIn the last step of the configuration wizard, the .zshrc file is updated with the selected options.\nRemember that the .zshrc and .p10k.zsh files are overwritten by default. So backup them before running p10K configure.\n\n\n\n\nContents of my .zshrc and .p10k.zsh\n\n\n.zshrc gist\n\n\n\n.p10k.zsh gist"
  },
  {
    "objectID": "posts/diagramascode/index.html",
    "href": "posts/diagramascode/index.html",
    "title": "Diagram as code",
    "section": "",
    "text": "I have been searching for a tool which can create diagrams based on declarative texts (diagram as code). A tool much like graphviz but easier to get started with and use. Two tools which fits my requirements are:"
  },
  {
    "objectID": "posts/diagramascode/index.html#mermaid",
    "href": "posts/diagramascode/index.html#mermaid",
    "title": "Diagram as code",
    "section": "Mermaid",
    "text": "Mermaid\nMermaid offers plenty of charts to choose from and has a user friendly syntax. The documentation provides many examples for different diagrams and the syntax seem to be easy to learn.\nThere are many ways to convert the diagram definitions as described below.\n\nUsing Docker\n\nUsing docker container can read a .mmd file and generate a output in the given path\n\ndocker run --rm -u `id -u`:`id -g` -v ~/Path/ToDiagramCode:/data minlag/mermaid-cli -i diagram.mmd\nOutput:  ### Using a HTML file In the below example from mermaid’s documentation, we define the diagram within a &lt;pre&gt;&lt;/pre&gt; tag and of class=mermaid.\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n  &lt;body&gt;\n    &lt;pre class=\"mermaid\"&gt;\n  graph LR\n      A --- B\n      B--&gt;C[fa:fa-ban forbidden]\n      B--&gt;D(fa:fa-spinner);\n    &lt;/pre&gt;\n    &lt;script type=\"module\"&gt;\n      import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';\n    &lt;/script&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\nThis html can then be served and the DOM of the webpage is updated like so.\n\n\n\n\n\ngraph LR\n      A --- B\n      B--&gt;C[fa:fa-ban forbidden]\n      B--&gt;D(fa:fa-spinner)\n\n\n\n\n\n\n\n\nUsing mermaid-live-editor\n\nOnline\nThe maintainers of mermaid have also open-sourced the live editor https://mermaid.live/ so that users can opt to run them locally. This self-hosting option is a handy option to have for users who do not want to use the hosted live-editor (although everything runs in the users browser even in the cloud version). #### Self-hosted To spin up your own instance of the live-editor use the docker/podman command.\ndocker run --platform linux/amd64 --publish 8000:8080 ghcr.io/mermaid-js/mermaid-live-editor\nTo change the port which is mapped to the container port 8080 edit --publish 8081:8080 This will then run mermaid server at https://localhost:8080\nYou have other options which can be customized, such as MERMAID_RENDERER_URL and MERMAID_KROKI_RENDERER_URL. Details can be found at https://github.com/mermaid-js/mermaid-live-editor\n\n\n\nUsing obsidian\nAn example diagram definition followed by the output (rendered within obsidian with mermaid tag). No extra community packages need to be installed within obsidian for this to work.\n\n\n\n\n\n---\ntitle: UiPath Reframework for Perfomer\n---\nstateDiagram-v2\n\nclassDef badBadEvent fill:#f00,color:white,font-weight:bold,stroke-width:2px,stroke:yellow\nclassDef goodEvent fill:green\n\n    [*] --&gt; Startup\n    Startup --&gt; Initialization\n    Initialization --&gt; [*]: Exit on error\n    Initialization --&gt; [*]: Exit if queue items &lt; 1\n    Initialization --&gt; GetTransactions\n    GetTransactions --&gt; ProcessTransactions\n    ProcessTransactions --&gt;  Application_Exception: App Exception Count &lt; n\n    ProcessTransactions --&gt;  [*]: App Exception Count &gt; n\n    Application_Exception:::badBadEvent  --&gt;  Initialization: App Exception Count &lt; n\n    ProcessTransactions --&gt;  Business_Exception\n    Business_Exception:::badBadEvent  --&gt;  GetTransactions: Get next queue item\n    ProcessTransactions --&gt;  Successful\n    Successful:::goodEvent --&gt;  GetTransactions: Get next queue item\n    GetTransactions --&gt; [*]:No queue items\n\n\n\n\n\n\n\n\n\n\n\n---\ntitle: UiPath Reframework for Perfomer\n---\n\nstateDiagram-v2\n\nclassDef badBadEvent fill:#f00,color:white,font-weight:bold,stroke-width:2px,stroke:yellow\nclassDef goodEvent fill:green\n\n    [*] --&gt; Startup\n    Startup --&gt; Initialization\n    Initialization --&gt; [*]: Exit on error\n    Initialization --&gt; [*]: Exit if queue items &lt; 1\n    Initialization --&gt; GetTransactions\n    GetTransactions --&gt; ProcessTransactions\n    ProcessTransactions --&gt;  Application_Exception: App Exception Count &lt; n\n    ProcessTransactions --&gt;  [*]: App Exception Count &gt; n\n    Application_Exception:::badBadEvent  --&gt;  Initialization: App Exception Count &lt; n\n    ProcessTransactions --&gt;  Business_Exception\n    Business_Exception:::badBadEvent  --&gt;  GetTransactions: Get next queue item\n    ProcessTransactions --&gt;  Successful\n    Successful:::goodEvent --&gt;  GetTransactions: Get next queue item\n    GetTransactions --&gt; [*]:No queue items\n\n\n\n\n\n\n\n\nUsing VSCode extension\nMarkdown Preview Mermaid Support from Matt Bierner can render any mermaid diagram definition and the UI interface allows the user to export the diagram in different image formats. \n\n\nUsing Mermaid CLI\nIf one prefers the command line, then mermaid-cli is the official CLI by the maintainers of mermaid js. This option will require installation of additional javascript libraries, which some might want to avoid.\nInstalling mermaid-cli\nnpm install -g @mermaid-js/mermaid-cli\nThe below command takes in an input diagram definition and outputs the diagram file in the requured format.\nmmdc -i input.mmd -o output.svg"
  },
  {
    "objectID": "posts/diagramascode/index.html#kroki",
    "href": "posts/diagramascode/index.html#kroki",
    "title": "Diagram as code",
    "section": "Kroki",
    "text": "Kroki\nKroki allows the user to use any of the supported drawings tools and creates a REST API endpoints which the user can integrate in their applications. The overhead of interfacing with variety of “Diagram as code” tools is avoided by using Kroki. \n\nUsing docker\nThis is quite straight forward the user defines the port to be mapped. Kroki is\ndocker run -d -p 8000:8000 yuzutech/kroki\nWhat is great with kroki is provides a one-stop-shop to integrate with many tools via a REST-API. That said, the documentation of usage is somewhat lacking in my opinion.\nThe documentation provides ways to interact with the API for different tools: https://docs.kroki.io/kroki/setup/http-clients/"
  },
  {
    "objectID": "posts/bruno/index.html",
    "href": "posts/bruno/index.html",
    "title": "Bruno API client",
    "section": "",
    "text": "I recently opened up Postman to access an old collection. To my surprise, Postman has now fully gone the cloud way and syncs all collections to the cloud.\nThis means that if I somehow forgot to remove my tokens or credentials, they can potential be synced to Postman servers!\nThe scratchpad within Postman is also limited now and is next to useless. I did also try insomnia, and it seems they too have fallen for the same honeypot!\nThere has to be a better API client, and my search today ended with bruno."
  },
  {
    "objectID": "posts/bruno/index.html#bruno",
    "href": "posts/bruno/index.html#bruno",
    "title": "Bruno API client",
    "section": "bruno",
    "text": "bruno\nA great alternative to postman or insomnia.\nbruno is written in electron the same framework used by Visual Studio Code and the code is open-source with a MIT license.\nThe founders manifesto is a treat to read for any open-source enthusiast.\nBruno is offered both as a free version and a paid version.\nThe paid version cost $19 for 2 machines for a span of 2 years. After which the renewal is set to 60% of the $19 to cover development of the project. The feature set is great for my use-case and I admire Anoop’s honesty. I bought the Golden Edition (Individual License) today (2.6.2024)\n\n\n\n\n\n\nI wish bruno and Anoop great success. It is so refreshing to see when a great product meets a transparent and simple founder.\n\n\n\nHere are some YouTube videos to get started with bruno."
  },
  {
    "objectID": "posts/solvinglinearprogrammingproblems/index.html",
    "href": "posts/solvinglinearprogrammingproblems/index.html",
    "title": "Solving linear programming problems",
    "section": "",
    "text": "At work a presentation on an optimization problem was made by researches working in the local hospital. They were looking to minimizing staffing resources given certain variables and constraints.\nThe field which studies such problems is called Linear Programming (LP). I remember studying this vaguely during my bachelors in industrial engineering, but since then I have forgotten the basics of it.\nConsider a case of staffing a hospital. Nurses and doctors are expected to working in shifts, but as the rest of us they too may have personal commitments which have to be taken into consideration when planning for resource planning. Some of them may be specialists in one form of care so they cannot not easily be replaced by others from the resource pool.\nNow such problems are where linear programming can be useful.\nIn the above case, we are trying to maximize resource utilization while considering the dynamic nature of resource availability and skills. We also need to adhere to the constraint of minimum required resources for smooth operations.\nA visual representation of components involved in linear programming.\n---\ntitle: Components within linear programming\n---\nstateDiagram-v2 \ndirection LR\n[*]--&gt; Decision_Variables\nnote left of Decision_Variables\n    Output influencing variables\nend note\n[*]--&gt; Constraints\nnote left of Constraints\n    Defined limitations\nend note\nDecision_Variables --&gt; Objective_Function \nnote right of Objective_Function\n    Quantitatively calculable\nend note\nConstraints --&gt; Objective_Function\nObjective_Function --&gt;Optimal_Solution:Linear Solver\nnote right of Optimal_Solution\n    Solution given the other three components\nend note\nOptimal_Solution --&gt;[*]"
  },
  {
    "objectID": "posts/solvinglinearprogrammingproblems/index.html#modules-in-python",
    "href": "posts/solvinglinearprogrammingproblems/index.html#modules-in-python",
    "title": "Solving linear programming problems",
    "section": "Modules in python",
    "text": "Modules in python\nPluP, Docplex, Pyomo and Gekko are some of the well known python modules used to solve linear programming problems.\nLP solvers are independent of the python modules, as many LP solvers are openly available and the python modules have built an api over these solvers to make it easier to integrate in python."
  },
  {
    "objectID": "posts/solvinglinearprogrammingproblems/index.html#example-problems",
    "href": "posts/solvinglinearprogrammingproblems/index.html#example-problems",
    "title": "Solving linear programming problems",
    "section": "Example problems",
    "text": "Example problems\nSolving some problems without external help. I tested my understanding after watching videos to learn the concept. Problem 1 and Problem 2 are credited to Byjus. Although, I am not totally certain about the origin of these questions (may not be a primary source).\n\nProblem 1 - Vitamin blending\nA doctor wishes to mix two types of Dishes in such a way that the vitamin contents of the mixture contain at least 8 units of vitamin A and 10 units of vitamin C. Dish 1 contains 2 units/kg of vitamin A and 1 unit/kg of vitamin C. Dish 2 contains 1 unit/kg of vitamin A and 2 units/kg of vitamin C. It costs Kr 50 per kg to purchase Dish 1 and Kr 70 per kg to purchase Dish 2. Formulate this problem as a linear programming problem to minimize the cost of such a mixture\n\nIntuition\n\nFormulating the problem. Before we do that we create a table to make this easier. This video was of good to understand and build an intuition. {: .prompt-tip}\n\n\n\n\n\nVitamin A (units)\nVitamin C (units)\nCost (Kr)\n\n\n\n\nDish 1\n2\n1\n50\n\n\nDish 2\n1\n2\n70\n\n\nDaily need\n8\n10\n\n\n\n\nFor simplicity Vitamin A = x Vitamin C = y\nVariables x, y daily feed in units\nObjective function: f(x, y) = 50*x + 70*y –&gt; Minimzation problem\nSubject to constraints:\n2*x + 1*y &gt;= 8\n1*x + 2*y &gt;= 10\nx &gt;=0\ny &gt;= 0\n\n\nPython implementation\n\n%%time\n\nimport pulp   \n\n# Creating LP problem\n\"\"\"\nHere we are looking to minimize the cost while getting the enough of vitamin a and c in the diet. This is therefore a minimization LP problem.\n\"\"\"\nvitamin_mix_lp = pulp.LpProblem(\"vitamin_mix_lp\", pulp.LpMinimize)\n\n# Creating LPvariables\nx = pulp.LpVariable(\"Dish 1\", lowBound=0, cat='Integer')\ny = pulp.LpVariable(\"Dish 2\", lowBound=0, cat='Integer')\n\n# Creating Constraints: Mixture contain at least 8 units of vitamin A and 10 units of vitamin C\nvitamin_mix_lp += 2*x + 1*y &gt;=8\nvitamin_mix_lp += 1*x + 2*y &gt;= 10\nvitamin_mix_lp += x &gt;= 0\nvitamin_mix_lp += y &gt;=0\n\n# Creating the objective function: f(x, y) = 50*x + 70*y\nvitamin_mix_lp += 50*x + 70*y\n\n# Solving the objective function\nsolution = vitamin_mix_lp.solve()\n\n# Printing the results and optimal variable values\nprint(\"Status:\", solution) # 1: Optimal solution exists\nprint(\"Optimal Solution Value:\", vitamin_mix_lp.objective.value(), \"Kr.\")\nfor var in vitamin_mix_lp.variables():\n    print(var.name, \"=\", var.varValue, \"units\")\n\nWelcome to the CBC MILP Solver \nVersion: 2.10.3 \nBuild Date: Dec 15 2019 \n\ncommand line - /home/wslap/Documents/jeev20.github.io/.venv/lib/python3.10/site-packages/pulp/apis/../solverdir/cbc/linux/i64/cbc /tmp/0dba693602c244eb9fb5a6a5b2f69975-pulp.mps -timeMode elapsed -branch -printingOptions all -solution /tmp/0dba693602c244eb9fb5a6a5b2f69975-pulp.sol (default strategy 1)\nAt line 2 NAME          MODEL\nAt line 3 ROWS\nAt line 9 COLUMNS\nAt line 22 RHS\nAt line 27 BOUNDS\nAt line 30 ENDATA\nProblem MODEL has 4 rows, 2 columns and 6 elements\nCoin0008I MODEL read with 0 errors\nOption for timeMode changed from cpu to elapsed\nContinuous objective value is 380 - 0.00 seconds\nCgl0003I 0 fixed, 2 tightened bounds, 0 strengthened rows, 0 substitutions\nCgl0004I processed model has 2 rows, 2 columns (2 integer (0 of which binary)) and 4 elements\nCutoff increment increased from 1e-05 to 9.9999\nCbc0012I Integer solution of 380 found by greedy cover after 0 iterations and 0 nodes (0.00 seconds)\nCbc0001I Search completed - best objective 380, took 0 iterations and 0 nodes (0.00 seconds)\nCbc0035I Maximum depth 0, 0 variables fixed on reduced cost\nCuts at root node changed objective from 380 to 380\nProbing was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nGomory was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nKnapsack was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nClique was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nMixedIntegerRounding2 was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nFlowCover was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nTwoMirCuts was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nZeroHalf was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n\nResult - Optimal solution found\n\nObjective value:                380.00000000\nEnumerated nodes:               0\nTotal iterations:               0\nTime (CPU seconds):             0.00\nTime (Wallclock seconds):       0.00\n\nOption for printingOptions changed from normal to all\nTotal time (CPU seconds):       0.00   (Wallclock seconds):       0.00\n\nStatus: 1\nOptimal Solution Value: 380.0 Kr.\nDish_1 = 2.0 units\nDish_2 = 4.0 units\nCPU times: user 74.6 ms, sys: 4.93 ms, total: 79.5 ms\nWall time: 93.2 ms\n\n\nOutput from the above problem results in 2 units of dish 1 and 4 units of dish 2, which results in the minimum cost of 380 kroner while adhering to the set constraints.\nStatus: 1 (optimal solution exists)\nOptimal Solution Value: 380.0 Kr.\nDish_1 = 2.0 units\nDish_2 = 4.0 units\n\n\n\n\nProblem 2 - Bakery case\nOne kind of cake requires 200g of flour and 25g of fat, and another kind of cake requires 100g of flour and 50g of fat. Formulate this problem as a linear programming problem to find the maximum number of cakes that can be made from 5kg of flour and 1 kg of fat assuming that there is no shortage of the other ingredients used in making the cakes.\n\nIntuition\n\nFormulating the problem. We here are trying maximize the amount of cakes baked given the limited supplies. {: .prompt-tip}\n\n\n\n\n\nFlour (grams)\nFat (grams)\n\n\n\n\nCake A\n200\n25\n\n\nCake B\n100\n50\n\n\nAvailability\n5000\n1000\n\n\n\nFor simplicity Flour = x & Fat = y\nVariables x, y in grams\nObjective function: f(x, y) = SUM((200*x + 100*y),(25*x + 50*y)) –&gt; Maximization problem\nSubject to constraints:\n200*x + 100*y &lt;= 5000\n25*x + 50*y &lt;= 1000\nx &gt;=0\ny &gt;= 0\n\n\nPython implementation\n\n%%time\nimport pulp   \n\n# Creating LP problem: Here we are looking to maximize the objective function.\nbakery_lp = pulp.LpProblem(\"bakery_lp\", pulp.LpMaximize)\n\n# Creating LPvariables\nx = pulp.LpVariable(\"Cake A\", lowBound=0, cat='Integer')\ny = pulp.LpVariable(\"Cake B\", lowBound=0, cat='Integer')\n\n# Creating Constraints:number of cakes that can be made from 5kg of flour and 1 kg of fat\nbakery_lp += 200*x + 100*y &lt;= 5000\nbakery_lp += 25*x + 50*y &lt;= 1000\nbakery_lp += x &gt;= 0\nbakery_lp += y &gt;=0\n\n# Creating the objective function: f(x, y) = (200*x + 100*y)+(25*x + 50*y)\nbakery_lp += (200*x + 100*y)+(25*x + 50*y)\n\n# Solving the objective function\nsolution = bakery_lp.solve()\n\n# Printing the results and optimal variable values\nprint(\"Status:\", solution) # 1: Optimal solution exists\nprint(\"Optimal Solution Value:\", bakery_lp.objective.value())\nfor var in bakery_lp.variables():\n    print(var.name, \"=\", var.varValue, \"units\")\n\nWelcome to the CBC MILP Solver \nVersion: 2.10.3 \nBuild Date: Dec 15 2019 \n\ncommand line - /home/wslap/Documents/jeev20.github.io/.venv/lib/python3.10/site-packages/pulp/apis/../solverdir/cbc/linux/i64/cbc /tmp/1d5a0e8ed12f443487a3047dacf98672-pulp.mps -max -timeMode elapsed -branch -printingOptions all -solution /tmp/1d5a0e8ed12f443487a3047dacf98672-pulp.sol (default strategy 1)\nAt line 2 NAME          MODEL\nAt line 3 ROWS\nAt line 9 COLUMNS\nAt line 22 RHS\nAt line 27 BOUNDS\nAt line 30 ENDATA\nProblem MODEL has 4 rows, 2 columns and 6 elements\nCoin0008I MODEL read with 0 errors\nOption for timeMode changed from cpu to elapsed\nContinuous objective value is 6000 - 0.00 seconds\nCgl0004I processed model has 2 rows, 2 columns (2 integer (0 of which binary)) and 4 elements\nCutoff increment increased from 1e-05 to 74.9999\nCbc0012I Integer solution of -6000 found by DiveCoefficient after 0 iterations and 0 nodes (0.00 seconds)\nCbc0001I Search completed - best objective -6000, took 0 iterations and 0 nodes (0.00 seconds)\nCbc0035I Maximum depth 0, 0 variables fixed on reduced cost\nCuts at root node changed objective from -6000 to -6000\nProbing was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nGomory was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nKnapsack was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nClique was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nMixedIntegerRounding2 was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nFlowCover was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nTwoMirCuts was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nZeroHalf was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n\nResult - Optimal solution found\n\nObjective value:                6000.00000000\nEnumerated nodes:               0\nTotal iterations:               0\nTime (CPU seconds):             0.00\nTime (Wallclock seconds):       0.00\n\nOption for printingOptions changed from normal to all\nTotal time (CPU seconds):       0.00   (Wallclock seconds):       0.00\n\nStatus: 1\nOptimal Solution Value: 6000.0\nCake_A = 20.0 units\nCake_B = 10.0 units\nCPU times: user 1.55 ms, sys: 0 ns, total: 1.55 ms\nWall time: 19.9 ms\n\n\nOutput from the above problem results in 20 units of cake 1 and 10 units of cake 2, which results in the maximum revenue of 6000 kroner while adhering to the set constraints. Totally 30 cakes need to be baked by the baker.\nStatus: 1\nOptimal Solution Value: 6000 kr.\nCake_A = 20.0 units\nCake_B = 10.0 units\n\n\n\n\nProblem 3 - Protein intake\nThis problem is sourced from Brilliant.org\nAn amateur bodybuilder is looking for supplement protein bars to build his muscle fast, and there are 2 available products: protein bar A and protein bar B. Each protein bar A contains 15 g of protein and 30 g of carbohydrates and has total 200 calories. On the other hand, each protein bar B contains 30 g of protein and 20 g of carbohydrates and has total 240 calories. According to his nutritional plan, this bodybuilder needs at least 20,000 calories from these supplements over the month, which must comprise of at least 1,800 g of protein and at least 2,200 g of carbohydrates. If each protein bar A costs $3 and each protein bar B costs $4, what is the least possible amount of money (in $) he can spend to meet all his one-month requirements?\n\nIntuition\n\nFormulating the problem. We here are trying maximize the amount of cakes baked given the limited supplies. {: .prompt-tip}\n\n\n\n\n\n\n\n\n\n\n\n\nProtein (grams)\nCarbohydrates (grams)\nCalories\nCost ($)\n\n\n\n\nProtein A\n15\n30\n200\n3\n\n\nProtein B\n30\n20\n240\n4\n\n\nNeed\n1800\n2200\n20000\n\n\n\n\nFor simplicity Protein A = x & Protein B = y\nVariables x, y\nObjective function: f(x, y) = (3*x + 4*y) –&gt; Minimization problem\nSubject to constraints:\n15*x + 30*y &gt;= 1800\n30*x + 20*y &gt;= 2200\n200*x + 240*y &gt;= 20000\nx &gt;= 0\ny &gt;= 0\n\n\nPython implementation\n\n%%time\nimport pulp   \n\n# Creating LP problem: Here we are looking to minimize the objective function.\nbodybuilderdiet_lp = pulp.LpProblem(\"bodybuilderdiet_lp\", pulp.LpMinimize)\n\n# Creating LPvariables\nx = pulp.LpVariable(\"Protein\", lowBound=0, cat='Integer')\ny = pulp.LpVariable(\"Carbohydrates\", lowBound=0, cat='Integer')\n\n# Creating Constraints\nbodybuilderdiet_lp += 15*x + 30*y &gt;= 1800\nbodybuilderdiet_lp += 30*x + 20*y &gt;= 2200\nbodybuilderdiet_lp += 200*x + 240*y &gt;= 20000\nbodybuilderdiet_lp += x &gt;= 0\nbodybuilderdiet_lp += y &gt;=0\n\n# Creating the objective function: f(x, y) = (3*x + 4*y)\nbodybuilderdiet_lp += 3*x + 4*y\n\n# Solving the objective function\nsolution = bodybuilderdiet_lp.solve()\n\n# Printing the results and optimal variable values\nprint(\"Status:\", solution) # 1: Optimal solution exists\nprint(\"Optimal Solution Value:\", bodybuilderdiet_lp.objective.value())\nfor var in bodybuilderdiet_lp.variables():\n    print(var.name, \"=\", var.varValue, \"units\")\n\nWelcome to the CBC MILP Solver \nVersion: 2.10.3 \nBuild Date: Dec 15 2019 \n\ncommand line - /home/wslap/Documents/jeev20.github.io/.venv/lib/python3.10/site-packages/pulp/apis/../solverdir/cbc/linux/i64/cbc /tmp/45898add32254e48883beeb499460506-pulp.mps -timeMode elapsed -branch -printingOptions all -solution /tmp/45898add32254e48883beeb499460506-pulp.sol (default strategy 1)\nAt line 2 NAME          MODEL\nAt line 3 ROWS\nAt line 10 COLUMNS\nAt line 25 RHS\nAt line 31 BOUNDS\nAt line 34 ENDATA\nProblem MODEL has 5 rows, 2 columns and 8 elements\nCoin0008I MODEL read with 0 errors\nOption for timeMode changed from cpu to elapsed\nContinuous objective value is 310 - 0.00 seconds\nCgl0003I 0 fixed, 2 tightened bounds, 0 strengthened rows, 0 substitutions\nCgl0004I processed model has 3 rows, 2 columns (2 integer (0 of which binary)) and 6 elements\nCutoff increment increased from 1e-05 to 0.9999\nCbc0012I Integer solution of 310 found by greedy cover after 0 iterations and 0 nodes (0.00 seconds)\nCbc0001I Search completed - best objective 310, took 0 iterations and 0 nodes (0.00 seconds)\nCbc0035I Maximum depth 0, 0 variables fixed on reduced cost\nCuts at root node changed objective from 310 to 310\nProbing was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nGomory was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nKnapsack was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nClique was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nMixedIntegerRounding2 was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nFlowCover was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nTwoMirCuts was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nZeroHalf was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n\nResult - Optimal solution found\n\nObjective value:                310.00000000\nEnumerated nodes:               0\nTotal iterations:               0\nTime (CPU seconds):             0.00\nTime (Wallclock seconds):       0.00\n\nOption for printingOptions changed from normal to all\nTotal time (CPU seconds):       0.00   (Wallclock seconds):       0.00\n\nStatus: 1\nOptimal Solution Value: 310.0\nCarbohydrates = 25.0 units\nProtein = 70.0 units\nCPU times: user 599 μs, sys: 988 μs, total: 1.59 ms\nWall time: 4.84 ms\n\n\nOutput from the above problem results in 25 units of Carbohydrates and 70 units of Protein, which results in the minimum expense of 310 dollars while adhering to the set constraints.\nStatus: 1\nOptimal Solution Value: 310.0 $\nCarbohydrates = 25.0 units\nProtein = 70.0 units"
  },
  {
    "objectID": "posts/solvinglinearprogrammingproblems/index.html#resources",
    "href": "posts/solvinglinearprogrammingproblems/index.html#resources",
    "title": "Solving linear programming problems",
    "section": "Resources",
    "text": "Resources\nHere are some of the resources which helped me learn the concepts of linear programming with no specific order.\nI enjoyed reading this to dig deeper into the workings of LP\nVideo tutorials on linear programming \nA deep dive into concepts behind linear programming \n\nUsing PulP in python \nGoing forward, when I come across an interesting LP problem, I will update this post. For now, LP was fun learning. I have barely scratched the surface here.\nMore fun to be had!"
  },
  {
    "objectID": "posts/usefullinuxcommands/index.html",
    "href": "posts/usefullinuxcommands/index.html",
    "title": "Useful linux commands",
    "section": "",
    "text": "The aim with this post is to centralize all my most used linux commands and their different use case descriptions using the tldr module.\nSomeday when I wonder what the syntax was for a command, I have a place to refer, hopefully!\n\nscp\nscp\n\n  Secure copy.\n  Copy files between hosts using Secure Copy Protocol over SSH.\n  More information: https://man.openbsd.org/scp.\n\n  - Copy a local file to a remote host:\n    scp path/to/local_file remote_host:path/to/remote_file\n\n  - Use a specific port when connecting to the remote host:\n    scp -P port path/to/local_file remote_host:path/to/remote_file\n\n  - Copy a file from a remote host to a local directory:\n    scp remote_host:path/to/remote_file path/to/local_directory\n\n  - Recursively copy the contents of a directory from a remote host to a local directory:\n    scp -r remote_host:path/to/remote_directory path/to/local_directory\n\n  - Copy a file between two remote hosts transferring through the local host:\n    scp -3 host1:path/to/remote_file host2:path/to/remote_directory\n\n  - Use a specific username when connecting to the remote host:\n    scp path/to/local_file remote_username@remote_host:path/to/remote_directory\n\n  - Use a specific SSH private key for authentication with the remote host:\n    scp -i ~/.ssh/private_key path/to/local_file remote_host:path/to/remote_file\n\n  - Use a specific proxy when connecting to the remote host:\n    scp -J proxy_username@proxy_host path/to/local_file remote_host:path/to/remote_file\n\n\nwhere\nwhere\n\n  Reports all known instances of a command.\n  It could be an executable in the PATH environment variable, an alias, or a shell builtin.\n  More information: https://zsh.sourceforge.io/Doc/Release/Shell-Builtin-Commands.html.\n\n  - Find all instances of a command:\n    where command\n\n\nmake\nmake\n\n  Task runner for targets described in Makefile.\n  Mostly used to control the compilation of an executable from source code.\n  More information: https://www.gnu.org/software/make/manual/make.html.\n\n  - Call the first target specified in the Makefile (usually named \"all\"):\n    make\n\n  - Call a specific target:\n    make target\n\n  - Call a specific target, executing 4 jobs at a time in parallel:\n    make -j4 target\n\n  - Use a specific Makefile:\n    make --file path/to/file\n\n  - Execute make from another directory:\n    make --directory path/to/directory\n\n  - Force making of a target, even if source files are unchanged:\n    make --always-make target\n\n  - Override a variable defined in the Makefile:\n    make target variable=new_value\n\n  - Override variables defined in the Makefile by the environment:\n    make --environment-overrides target\n\n\nssh\nssh\n\n  Secure Shell is a protocol used to securely log onto remote systems.\n  It can be used for logging or executing commands on a remote server.\n  More information: https://man.openbsd.org/ssh.\n\n  - Connect to a remote server:\n    ssh username@remote_host\n\n  - Connect to a remote server with a specific identity (private key):\n    ssh -i path/to/key_file username@remote_host\n\n  - Connect to a remote server using a specific [p]ort:\n    ssh username@remote_host -p 2222\n\n  - Run a command on a remote server with a [t]ty allocation allowing interaction with the remote command:\n    ssh username@remote_host -t command command_arguments\n\n  - SSH tunneling: [D]ynamic port forwarding (SOCKS proxy on localhost:1080):\n    ssh -D 1080 username@remote_host\n\n  - SSH tunneling: Forward a specific port (localhost:9999 to example.org:80) along with disabling pseudo-[T]ty allocation and executio[N] of remote commands:\n    ssh -L 9999:example.org:80 -N -T username@remote_host\n\n  - SSH [J]umping: Connect through a jumphost to a remote server (Multiple jump hops may be specified separated by comma characters):\n    ssh -J username@jump_host username@remote_host\n\n  - Agent forwarding: Forward the authentication information to the remote machine (see man ssh_config for available options):\n    ssh -A username@remote_host\n\n\nshutdown\nshutdown\n\n  Shutdown and reboot the system.\n  More information: https://manned.org/shutdown.8.\n\n  - Power off ([h]alt) immediately:\n    shutdown -h now\n\n  - [r]eboot immediately:\n    shutdown -r now\n\n  - [r]eboot in 5 minutes:\n    shutdown -r +5 &\n\n  - Shutdown at 1:00 pm (Uses 24[h] clock):\n    shutdown -h 13:00\n\n  - [c]ancel a pending shutdown/reboot operation:\n    shutdown -c\n\n\ndocker\ndocker\n\n  Manage Docker containers and images.\n  Some subcommands such as docker run have their own usage documentation.\n  More information: https://docs.docker.com/engine/reference/commandline/cli/.\n\n  - List all Docker containers (running and stopped):\n    docker ps --all\n\n  - Start a container from an image, with a custom name:\n    docker run --name container_name image\n\n  - Start or stop an existing container:\n    docker start|stop container_name\n\n  - Pull an image from a Docker registry:\n    docker pull image\n\n  - Display the list of already downloaded images:\n    docker images\n\n  - Open an [i]nteractive [t]ty with Bourne shell (sh) inside a running container:\n    docker exec -it container_name sh\n\n  - Remove a stopped container:\n    docker rm container_name\n\n  - Fetch and follow the logs of a container:\n    docker logs -f container_name"
  },
  {
    "objectID": "posts/notesdatascience/EDA/index.html",
    "href": "posts/notesdatascience/EDA/index.html",
    "title": "Exploratory data analysis",
    "section": "",
    "text": "According to Campusx-YouTube these are some of the suggested questions to answer when a new dataset is obtained.\n\n\n\n\n\nmindmap\n  root((Quick Data Exploration))\n    1 - How big is the data? \n    2 - How does the data look like?\n    3 - What type of datatypes are found in the data?\n    4 - Are there missing data in this dataset?\n    5 - How does the data look mathematically?\n    6 - Are there duplicate rows in this data?\n    7 - What is the correlation between the columns in this data?\n            \n\n\n\n\n\n\n\n\n\nimport duckdb\nimport polars as pl\nimport seaborn as sn\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\n\n\n# ts is in epoch time format so converting it to timestamp\n# rounding values for temperature and humidity\n# converting temperature from farhaneit to celsius\n\ninput_data_path = f\"../data/iot/iot_telemetry_data.parquet\"\n\n\ndf_raw = duckdb.sql(\n    f\"SELECT ts, to_timestamp(ts) AS timestamp, device, temp,ROUND((temp - 32) * 5.0 / 9, 4) AS temp_c, ROUND(humidity, 4) AS humidity, lpg, smoke, light FROM '{input_data_path}'\"\n)\n\n\n\n\nThe seven questions to get insight into the data\n\n\n\n# Converting to polars to easy statistics and exploration\ndf_pl = df_raw.pl()  \ndf_pl.shape\n\n(405184, 9)\n\n\n\n\n\n\ndf_pl.head()\n\n\nshape: (5, 9)\n\n\n\nts\ntimestamp\ndevice\ntemp\ntemp_c\nhumidity\nlpg\nsmoke\nlight\n\n\nf64\ndatetime[μs, Europe/Oslo]\nstr\nf64\nf64\nf64\nf64\nf64\nbool\n\n\n\n\n1.5945e9\n2020-07-12 02:01:34.385975 CEST\n\"b8:27:eb:bf:9d:51\"\n22.7\n-5.1667\n51.0\n0.007651\n0.020411\nfalse\n\n\n1.5945e9\n2020-07-12 02:01:34.735568 CEST\n\"00:0f:00:70:91:0a\"\n19.700001\n-6.8333\n76.0\n0.005114\n0.013275\nfalse\n\n\n1.5945e9\n2020-07-12 02:01:38.073573 CEST\n\"b8:27:eb:bf:9d:51\"\n22.6\n-5.2222\n50.9\n0.007673\n0.020475\nfalse\n\n\n1.5945e9\n2020-07-12 02:01:39.589146 CEST\n\"1c:bf:ce:15:ec:4d\"\n27.0\n-2.7778\n76.8\n0.007023\n0.018628\ntrue\n\n\n1.5945e9\n2020-07-12 02:01:41.761235 CEST\n\"b8:27:eb:bf:9d:51\"\n22.6\n-5.2222\n50.9\n0.007664\n0.020448\nfalse\n\n\n\n\n\n\n\n\n\n\nduckdb.sql(\"SUMMARIZE df_raw;\")\n\n┌─────────────┬──────────────────────────┬───────────────────────────────┬───────────────────────────────┬───────────────┬───────────────────────────────┬───────────────────────┬───────────────────────────────┬───────────────────────────────┬───────────────────────────────┬────────┬─────────────────┐\n│ column_name │       column_type        │              min              │              max              │ approx_unique │              avg              │          std          │              q25              │              q50              │              q75              │ count  │ null_percentage │\n│   varchar   │         varchar          │            varchar            │            varchar            │     int64     │            varchar            │        varchar        │            varchar            │            varchar            │            varchar            │ int64  │  decimal(9,2)   │\n├─────────────┼──────────────────────────┼───────────────────────────────┼───────────────────────────────┼───────────────┼───────────────────────────────┼───────────────────────┼───────────────────────────────┼───────────────────────────────┼───────────────────────────────┼────────┼─────────────────┤\n│ ts          │ DOUBLE                   │ 1594512094.3859746            │ 1595203417.2643125            │        491304 │ 1594858017.2968097            │ 199498.39927628823    │ 1594686010.292624             │ 1594857972.1364734            │ 1595030547.7465725            │ 405184 │            0.00 │\n│ timestamp   │ TIMESTAMP WITH TIME ZONE │ 2020-07-12 02:01:34.385975+02 │ 2020-07-20 02:03:37.264312+02 │        326811 │ 2020-07-16 02:06:57.296824+02 │ NULL                  │ 2020-07-14 02:20:10.292624+02 │ 2020-07-16 02:06:12.136474+02 │ 2020-07-18 02:02:27.746572+02 │ 405184 │            0.00 │\n│ device      │ VARCHAR                  │ 00:0f:00:70:91:0a             │ b8:27:eb:bf:9d:51             │             3 │ NULL                          │ NULL                  │ NULL                          │ NULL                          │ NULL                          │ 405184 │            0.00 │\n│ temp        │ DOUBLE                   │ 0.0                           │ 30.600000381469727            │           287 │ 22.453987345644748            │ 2.698346951263289     │ 19.888040295855575            │ 22.243208182088022            │ 23.516330668892394            │ 405184 │            0.00 │\n│ temp_c      │ DOUBLE                   │ -17.7778                      │ -0.7778                       │           235 │ -5.303340377703994            │ 1.4990816816561081    │ -6.728858088985643            │ -5.417193168388082            │ -4.713166284527654            │ 405184 │            0.00 │\n│ humidity    │ DOUBLE                   │ 1.1                           │ 99.9                          │           626 │ 60.51169394645508             │ 11.366489374741377    │ 51.025250291852586            │ 54.94414996066462             │ 74.2764440390179              │ 405184 │            0.00 │\n│ lpg         │ DOUBLE                   │ 0.002693478622661808          │ 0.016567377162503137          │          8188 │ 0.007237125655057899          │ 0.001444115678768443  │ 0.006458547423805879          │ 0.007470350246685729          │ 0.00814041627569619           │ 405184 │            0.00 │\n│ smoke       │ DOUBLE                   │ 0.006692096317386558          │ 0.04659011562630793           │          6997 │ 0.019263611784848263          │ 0.0040861300601221845 │ 0.017032583919261386          │ 0.019897415643395536          │ 0.021809493412627078          │ 405184 │            0.00 │\n│ light       │ BOOLEAN                  │ false                         │ true                          │             2 │ NULL                          │ NULL                  │ NULL                          │ NULL                          │ NULL                          │ 405184 │            0.00 │\n└─────────────┴──────────────────────────┴───────────────────────────────┴───────────────────────────────┴───────────────┴───────────────────────────────┴───────────────────────┴───────────────────────────────┴───────────────────────────────┴───────────────────────────────┴────────┴─────────────────┘\n\n\n\n\n\nThis dataset is quiet clean, there are no missing data in the input data in any feature. Docs Reference\n\ndf_pl.null_count()\n\n\nshape: (1, 9)\n\n\n\nts\ntimestamp\ndevice\ntemp\ntemp_c\nhumidity\nlpg\nsmoke\nlight\n\n\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n\n\ndf_pl.describe()\n\n\nshape: (9, 10)\n\n\n\nstatistic\nts\ntimestamp\ndevice\ntemp\ntemp_c\nhumidity\nlpg\nsmoke\nlight\n\n\nstr\nf64\nstr\nstr\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"count\"\n405184.0\n\"405184\"\n\"405184\"\n405184.0\n405184.0\n405184.0\n405184.0\n405184.0\n405184.0\n\n\n\"null_count\"\n0.0\n\"0\"\n\"0\"\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\"mean\"\n1.5949e9\n\"2020-07-16 02:06:57.296824+02:…\nnull\n22.453987\n-5.30334\n60.511694\n0.007237\n0.019264\n0.277718\n\n\n\"std\"\n199498.399262\nnull\nnull\n2.698347\n1.499082\n11.366489\n0.001444\n0.004086\nnull\n\n\n\"min\"\n1.5945e9\n\"2020-07-12 02:01:34.385975+02:…\n\"00:0f:00:70:91:0a\"\n0.0\n-17.7778\n1.1\n0.002693\n0.006692\n0.0\n\n\n\"25%\"\n1.5947e9\n\"2020-07-14 02:20:00.478589+02:…\nnull\n19.9\n-6.7222\n51.0\n0.006456\n0.017024\nnull\n\n\n\"50%\"\n1.5949e9\n\"2020-07-16 02:06:28.940644+02:…\nnull\n22.2\n-5.4444\n54.9\n0.007489\n0.01995\nnull\n\n\n\"75%\"\n1.5950e9\n\"2020-07-18 02:02:56.634962+02:…\nnull\n23.6\n-4.6667\n74.3\n0.00815\n0.021838\nnull\n\n\n\"max\"\n1.5952e9\n\"2020-07-20 02:03:37.264312+02:…\n\"b8:27:eb:bf:9d:51\"\n30.6\n-0.7778\n99.9\n0.016567\n0.04659\n1.0\n\n\n\n\n\n\n\n\n\nAlthough the below script shows there are duplicates, it looks like polars and pandas work differently here so it is important to check both. While polars returns all occurances of the duplicate, pandas only gets the duplicated values. Hence polars shows the shape to be (26,8) while pandas returns 13 duplicated rows in this dataset\n\ndup_mask = df_pl.is_duplicated()\nduplicates = df_pl.filter(dup_mask)  # Filter to show duplicated rows only\n\ndf_pd = df_pl.to_pandas().duplicated().sum() # from pandas\ndf_pd\nduplicates  # from polars\n\nnp.int64(13)\n\n\n\nshape: (26, 9)\n\n\n\nts\ntimestamp\ndevice\ntemp\ntemp_c\nhumidity\nlpg\nsmoke\nlight\n\n\nf64\ndatetime[μs, Europe/Oslo]\nstr\nf64\nf64\nf64\nf64\nf64\nbool\n\n\n\n\n1.5945e9\n2020-07-12 10:32:27.272352 CEST\n\"1c:bf:ce:15:ec:4d\"\n24.700001\n-4.0556\n74.2\n0.006644\n0.017556\ntrue\n\n\n1.5945e9\n2020-07-12 10:32:27.272352 CEST\n\"1c:bf:ce:15:ec:4d\"\n24.700001\n-4.0556\n74.2\n0.006644\n0.017556\ntrue\n\n\n1.5947e9\n2020-07-13 17:02:04.678796 CEST\n\"1c:bf:ce:15:ec:4d\"\n23.700001\n-4.6111\n61.7\n0.006916\n0.018325\ntrue\n\n\n1.5947e9\n2020-07-13 17:02:04.678796 CEST\n\"1c:bf:ce:15:ec:4d\"\n23.700001\n-4.6111\n61.7\n0.006916\n0.018325\ntrue\n\n\n1.5948e9\n2020-07-15 00:14:33.272537 CEST\n\"b8:27:eb:bf:9d:51\"\n22.6\n-5.2222\n48.6\n0.008071\n0.02161\nfalse\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n1.5951e9\n2020-07-18 16:50:31.409988 CEST\n\"b8:27:eb:bf:9d:51\"\n21.5\n-5.8333\n51.9\n0.008912\n0.024024\nfalse\n\n\n1.5951e9\n2020-07-19 02:01:15.810328 CEST\n\"b8:27:eb:bf:9d:51\"\n22.6\n-5.2222\n52.1\n0.008822\n0.023765\nfalse\n\n\n1.5951e9\n2020-07-19 02:01:15.810328 CEST\n\"b8:27:eb:bf:9d:51\"\n22.6\n-5.2222\n52.1\n0.008822\n0.023765\nfalse\n\n\n1.5951e9\n2020-07-19 05:54:33.955044 CEST\n\"b8:27:eb:bf:9d:51\"\n22.4\n-5.3333\n50.8\n0.008671\n0.02333\nfalse\n\n\n1.5951e9\n2020-07-19 05:54:33.955044 CEST\n\"b8:27:eb:bf:9d:51\"\n22.4\n-5.3333\n50.8\n0.008671\n0.02333\nfalse\n\n\n\n\n\n\n\n\n\nWe need to use pandas here as the .corr() function in pandas provides a more readable table for inspecting correlation\n\n# Select only numeric columns for the correlation matrix\nnumeric_df = df_pl.select(pl.col(pl.Float64 or pl.Int64)) \nnumeric_df.to_pandas().corr(method='pearson')\nnumeric_df.to_pandas().corr(method='spearman')\nnumeric_df.to_pandas().corr(method='kendall')\n\n\n\n\n\n\n\n\nts\ntemp\ntemp_c\nhumidity\nlpg\nsmoke\n\n\n\n\nts\n1.000000\n0.074443\n0.074442\n0.017752\n0.014178\n0.016349\n\n\ntemp\n0.074443\n1.000000\n1.000000\n-0.410427\n0.136396\n0.131891\n\n\ntemp_c\n0.074442\n1.000000\n1.000000\n-0.410427\n0.136397\n0.131891\n\n\nhumidity\n0.017752\n-0.410427\n-0.410427\n1.000000\n-0.672113\n-0.669863\n\n\nlpg\n0.014178\n0.136396\n0.136397\n-0.672113\n1.000000\n0.999916\n\n\nsmoke\n0.016349\n0.131891\n0.131891\n-0.669863\n0.999916\n1.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nts\ntemp\ntemp_c\nhumidity\nlpg\nsmoke\n\n\n\n\nts\n1.000000\n0.055377\n0.055408\n0.051560\n0.077576\n0.077576\n\n\ntemp\n0.055377\n1.000000\n0.999994\n-0.334051\n0.121469\n0.121469\n\n\ntemp_c\n0.055408\n0.999994\n1.000000\n-0.334268\n0.121609\n0.121609\n\n\nhumidity\n0.051560\n-0.334051\n-0.334268\n1.000000\n-0.764612\n-0.764612\n\n\nlpg\n0.077576\n0.121469\n0.121609\n-0.764612\n1.000000\n1.000000\n\n\nsmoke\n0.077576\n0.121469\n0.121609\n-0.764612\n1.000000\n1.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nts\ntemp\ntemp_c\nhumidity\nlpg\nsmoke\n\n\n\n\nts\n1.000000\n0.035450\n0.035481\n0.030211\n0.046115\n0.046115\n\n\ntemp\n0.035450\n1.000000\n0.999815\n-0.196422\n0.037934\n0.037934\n\n\ntemp_c\n0.035481\n0.999815\n1.000000\n-0.196596\n0.038080\n0.038080\n\n\nhumidity\n0.030211\n-0.196422\n-0.196596\n1.000000\n-0.559969\n-0.559969\n\n\nlpg\n0.046115\n0.037934\n0.038080\n-0.559969\n1.000000\n1.000000\n\n\nsmoke\n0.046115\n0.037934\n0.038080\n-0.559969\n1.000000\n1.000000"
  },
  {
    "objectID": "posts/notesdatascience/EDA/index.html#importing-modules",
    "href": "posts/notesdatascience/EDA/index.html#importing-modules",
    "title": "Exploratory data analysis",
    "section": "",
    "text": "import duckdb\nimport polars as pl\nimport seaborn as sn\nimport pandas as pd\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/notesdatascience/EDA/index.html#reading-data",
    "href": "posts/notesdatascience/EDA/index.html#reading-data",
    "title": "Exploratory data analysis",
    "section": "",
    "text": "# ts is in epoch time format so converting it to timestamp\n# rounding values for temperature and humidity\n# converting temperature from farhaneit to celsius\n\ninput_data_path = f\"../data/iot/iot_telemetry_data.parquet\"\n\n\ndf_raw = duckdb.sql(\n    f\"SELECT ts, to_timestamp(ts) AS timestamp, device, temp,ROUND((temp - 32) * 5.0 / 9, 4) AS temp_c, ROUND(humidity, 4) AS humidity, lpg, smoke, light FROM '{input_data_path}'\"\n)"
  },
  {
    "objectID": "posts/notesdatascience/EDA/index.html#exploring-the-data",
    "href": "posts/notesdatascience/EDA/index.html#exploring-the-data",
    "title": "Exploratory data analysis",
    "section": "",
    "text": "The seven questions to get insight into the data\n\n\n\n# Converting to polars to easy statistics and exploration\ndf_pl = df_raw.pl()  \ndf_pl.shape\n\n(405184, 9)\n\n\n\n\n\n\ndf_pl.head()\n\n\nshape: (5, 9)\n\n\n\nts\ntimestamp\ndevice\ntemp\ntemp_c\nhumidity\nlpg\nsmoke\nlight\n\n\nf64\ndatetime[μs, Europe/Oslo]\nstr\nf64\nf64\nf64\nf64\nf64\nbool\n\n\n\n\n1.5945e9\n2020-07-12 02:01:34.385975 CEST\n\"b8:27:eb:bf:9d:51\"\n22.7\n-5.1667\n51.0\n0.007651\n0.020411\nfalse\n\n\n1.5945e9\n2020-07-12 02:01:34.735568 CEST\n\"00:0f:00:70:91:0a\"\n19.700001\n-6.8333\n76.0\n0.005114\n0.013275\nfalse\n\n\n1.5945e9\n2020-07-12 02:01:38.073573 CEST\n\"b8:27:eb:bf:9d:51\"\n22.6\n-5.2222\n50.9\n0.007673\n0.020475\nfalse\n\n\n1.5945e9\n2020-07-12 02:01:39.589146 CEST\n\"1c:bf:ce:15:ec:4d\"\n27.0\n-2.7778\n76.8\n0.007023\n0.018628\ntrue\n\n\n1.5945e9\n2020-07-12 02:01:41.761235 CEST\n\"b8:27:eb:bf:9d:51\"\n22.6\n-5.2222\n50.9\n0.007664\n0.020448\nfalse\n\n\n\n\n\n\n\n\n\n\nduckdb.sql(\"SUMMARIZE df_raw;\")\n\n┌─────────────┬──────────────────────────┬───────────────────────────────┬───────────────────────────────┬───────────────┬───────────────────────────────┬───────────────────────┬───────────────────────────────┬───────────────────────────────┬───────────────────────────────┬────────┬─────────────────┐\n│ column_name │       column_type        │              min              │              max              │ approx_unique │              avg              │          std          │              q25              │              q50              │              q75              │ count  │ null_percentage │\n│   varchar   │         varchar          │            varchar            │            varchar            │     int64     │            varchar            │        varchar        │            varchar            │            varchar            │            varchar            │ int64  │  decimal(9,2)   │\n├─────────────┼──────────────────────────┼───────────────────────────────┼───────────────────────────────┼───────────────┼───────────────────────────────┼───────────────────────┼───────────────────────────────┼───────────────────────────────┼───────────────────────────────┼────────┼─────────────────┤\n│ ts          │ DOUBLE                   │ 1594512094.3859746            │ 1595203417.2643125            │        491304 │ 1594858017.2968097            │ 199498.39927628823    │ 1594686010.292624             │ 1594857972.1364734            │ 1595030547.7465725            │ 405184 │            0.00 │\n│ timestamp   │ TIMESTAMP WITH TIME ZONE │ 2020-07-12 02:01:34.385975+02 │ 2020-07-20 02:03:37.264312+02 │        326811 │ 2020-07-16 02:06:57.296824+02 │ NULL                  │ 2020-07-14 02:20:10.292624+02 │ 2020-07-16 02:06:12.136474+02 │ 2020-07-18 02:02:27.746572+02 │ 405184 │            0.00 │\n│ device      │ VARCHAR                  │ 00:0f:00:70:91:0a             │ b8:27:eb:bf:9d:51             │             3 │ NULL                          │ NULL                  │ NULL                          │ NULL                          │ NULL                          │ 405184 │            0.00 │\n│ temp        │ DOUBLE                   │ 0.0                           │ 30.600000381469727            │           287 │ 22.453987345644748            │ 2.698346951263289     │ 19.888040295855575            │ 22.243208182088022            │ 23.516330668892394            │ 405184 │            0.00 │\n│ temp_c      │ DOUBLE                   │ -17.7778                      │ -0.7778                       │           235 │ -5.303340377703994            │ 1.4990816816561081    │ -6.728858088985643            │ -5.417193168388082            │ -4.713166284527654            │ 405184 │            0.00 │\n│ humidity    │ DOUBLE                   │ 1.1                           │ 99.9                          │           626 │ 60.51169394645508             │ 11.366489374741377    │ 51.025250291852586            │ 54.94414996066462             │ 74.2764440390179              │ 405184 │            0.00 │\n│ lpg         │ DOUBLE                   │ 0.002693478622661808          │ 0.016567377162503137          │          8188 │ 0.007237125655057899          │ 0.001444115678768443  │ 0.006458547423805879          │ 0.007470350246685729          │ 0.00814041627569619           │ 405184 │            0.00 │\n│ smoke       │ DOUBLE                   │ 0.006692096317386558          │ 0.04659011562630793           │          6997 │ 0.019263611784848263          │ 0.0040861300601221845 │ 0.017032583919261386          │ 0.019897415643395536          │ 0.021809493412627078          │ 405184 │            0.00 │\n│ light       │ BOOLEAN                  │ false                         │ true                          │             2 │ NULL                          │ NULL                  │ NULL                          │ NULL                          │ NULL                          │ 405184 │            0.00 │\n└─────────────┴──────────────────────────┴───────────────────────────────┴───────────────────────────────┴───────────────┴───────────────────────────────┴───────────────────────┴───────────────────────────────┴───────────────────────────────┴───────────────────────────────┴────────┴─────────────────┘\n\n\n\n\n\nThis dataset is quiet clean, there are no missing data in the input data in any feature. Docs Reference\n\ndf_pl.null_count()\n\n\nshape: (1, 9)\n\n\n\nts\ntimestamp\ndevice\ntemp\ntemp_c\nhumidity\nlpg\nsmoke\nlight\n\n\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n\n\ndf_pl.describe()\n\n\nshape: (9, 10)\n\n\n\nstatistic\nts\ntimestamp\ndevice\ntemp\ntemp_c\nhumidity\nlpg\nsmoke\nlight\n\n\nstr\nf64\nstr\nstr\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"count\"\n405184.0\n\"405184\"\n\"405184\"\n405184.0\n405184.0\n405184.0\n405184.0\n405184.0\n405184.0\n\n\n\"null_count\"\n0.0\n\"0\"\n\"0\"\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\"mean\"\n1.5949e9\n\"2020-07-16 02:06:57.296824+02:…\nnull\n22.453987\n-5.30334\n60.511694\n0.007237\n0.019264\n0.277718\n\n\n\"std\"\n199498.399262\nnull\nnull\n2.698347\n1.499082\n11.366489\n0.001444\n0.004086\nnull\n\n\n\"min\"\n1.5945e9\n\"2020-07-12 02:01:34.385975+02:…\n\"00:0f:00:70:91:0a\"\n0.0\n-17.7778\n1.1\n0.002693\n0.006692\n0.0\n\n\n\"25%\"\n1.5947e9\n\"2020-07-14 02:20:00.478589+02:…\nnull\n19.9\n-6.7222\n51.0\n0.006456\n0.017024\nnull\n\n\n\"50%\"\n1.5949e9\n\"2020-07-16 02:06:28.940644+02:…\nnull\n22.2\n-5.4444\n54.9\n0.007489\n0.01995\nnull\n\n\n\"75%\"\n1.5950e9\n\"2020-07-18 02:02:56.634962+02:…\nnull\n23.6\n-4.6667\n74.3\n0.00815\n0.021838\nnull\n\n\n\"max\"\n1.5952e9\n\"2020-07-20 02:03:37.264312+02:…\n\"b8:27:eb:bf:9d:51\"\n30.6\n-0.7778\n99.9\n0.016567\n0.04659\n1.0\n\n\n\n\n\n\n\n\n\nAlthough the below script shows there are duplicates, it looks like polars and pandas work differently here so it is important to check both. While polars returns all occurances of the duplicate, pandas only gets the duplicated values. Hence polars shows the shape to be (26,8) while pandas returns 13 duplicated rows in this dataset\n\ndup_mask = df_pl.is_duplicated()\nduplicates = df_pl.filter(dup_mask)  # Filter to show duplicated rows only\n\ndf_pd = df_pl.to_pandas().duplicated().sum() # from pandas\ndf_pd\nduplicates  # from polars\n\nnp.int64(13)\n\n\n\nshape: (26, 9)\n\n\n\nts\ntimestamp\ndevice\ntemp\ntemp_c\nhumidity\nlpg\nsmoke\nlight\n\n\nf64\ndatetime[μs, Europe/Oslo]\nstr\nf64\nf64\nf64\nf64\nf64\nbool\n\n\n\n\n1.5945e9\n2020-07-12 10:32:27.272352 CEST\n\"1c:bf:ce:15:ec:4d\"\n24.700001\n-4.0556\n74.2\n0.006644\n0.017556\ntrue\n\n\n1.5945e9\n2020-07-12 10:32:27.272352 CEST\n\"1c:bf:ce:15:ec:4d\"\n24.700001\n-4.0556\n74.2\n0.006644\n0.017556\ntrue\n\n\n1.5947e9\n2020-07-13 17:02:04.678796 CEST\n\"1c:bf:ce:15:ec:4d\"\n23.700001\n-4.6111\n61.7\n0.006916\n0.018325\ntrue\n\n\n1.5947e9\n2020-07-13 17:02:04.678796 CEST\n\"1c:bf:ce:15:ec:4d\"\n23.700001\n-4.6111\n61.7\n0.006916\n0.018325\ntrue\n\n\n1.5948e9\n2020-07-15 00:14:33.272537 CEST\n\"b8:27:eb:bf:9d:51\"\n22.6\n-5.2222\n48.6\n0.008071\n0.02161\nfalse\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n1.5951e9\n2020-07-18 16:50:31.409988 CEST\n\"b8:27:eb:bf:9d:51\"\n21.5\n-5.8333\n51.9\n0.008912\n0.024024\nfalse\n\n\n1.5951e9\n2020-07-19 02:01:15.810328 CEST\n\"b8:27:eb:bf:9d:51\"\n22.6\n-5.2222\n52.1\n0.008822\n0.023765\nfalse\n\n\n1.5951e9\n2020-07-19 02:01:15.810328 CEST\n\"b8:27:eb:bf:9d:51\"\n22.6\n-5.2222\n52.1\n0.008822\n0.023765\nfalse\n\n\n1.5951e9\n2020-07-19 05:54:33.955044 CEST\n\"b8:27:eb:bf:9d:51\"\n22.4\n-5.3333\n50.8\n0.008671\n0.02333\nfalse\n\n\n1.5951e9\n2020-07-19 05:54:33.955044 CEST\n\"b8:27:eb:bf:9d:51\"\n22.4\n-5.3333\n50.8\n0.008671\n0.02333\nfalse\n\n\n\n\n\n\n\n\n\nWe need to use pandas here as the .corr() function in pandas provides a more readable table for inspecting correlation\n\n# Select only numeric columns for the correlation matrix\nnumeric_df = df_pl.select(pl.col(pl.Float64 or pl.Int64)) \nnumeric_df.to_pandas().corr(method='pearson')\nnumeric_df.to_pandas().corr(method='spearman')\nnumeric_df.to_pandas().corr(method='kendall')\n\n\n\n\n\n\n\n\nts\ntemp\ntemp_c\nhumidity\nlpg\nsmoke\n\n\n\n\nts\n1.000000\n0.074443\n0.074442\n0.017752\n0.014178\n0.016349\n\n\ntemp\n0.074443\n1.000000\n1.000000\n-0.410427\n0.136396\n0.131891\n\n\ntemp_c\n0.074442\n1.000000\n1.000000\n-0.410427\n0.136397\n0.131891\n\n\nhumidity\n0.017752\n-0.410427\n-0.410427\n1.000000\n-0.672113\n-0.669863\n\n\nlpg\n0.014178\n0.136396\n0.136397\n-0.672113\n1.000000\n0.999916\n\n\nsmoke\n0.016349\n0.131891\n0.131891\n-0.669863\n0.999916\n1.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nts\ntemp\ntemp_c\nhumidity\nlpg\nsmoke\n\n\n\n\nts\n1.000000\n0.055377\n0.055408\n0.051560\n0.077576\n0.077576\n\n\ntemp\n0.055377\n1.000000\n0.999994\n-0.334051\n0.121469\n0.121469\n\n\ntemp_c\n0.055408\n0.999994\n1.000000\n-0.334268\n0.121609\n0.121609\n\n\nhumidity\n0.051560\n-0.334051\n-0.334268\n1.000000\n-0.764612\n-0.764612\n\n\nlpg\n0.077576\n0.121469\n0.121609\n-0.764612\n1.000000\n1.000000\n\n\nsmoke\n0.077576\n0.121469\n0.121609\n-0.764612\n1.000000\n1.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nts\ntemp\ntemp_c\nhumidity\nlpg\nsmoke\n\n\n\n\nts\n1.000000\n0.035450\n0.035481\n0.030211\n0.046115\n0.046115\n\n\ntemp\n0.035450\n1.000000\n0.999815\n-0.196422\n0.037934\n0.037934\n\n\ntemp_c\n0.035481\n0.999815\n1.000000\n-0.196596\n0.038080\n0.038080\n\n\nhumidity\n0.030211\n-0.196422\n-0.196596\n1.000000\n-0.559969\n-0.559969\n\n\nlpg\n0.046115\n0.037934\n0.038080\n-0.559969\n1.000000\n1.000000\n\n\nsmoke\n0.046115\n0.037934\n0.038080\n-0.559969\n1.000000\n1.000000"
  },
  {
    "objectID": "posts/notesdatascience/EDA/index.html#univariate-data-analysis",
    "href": "posts/notesdatascience/EDA/index.html#univariate-data-analysis",
    "title": "Exploratory data analysis",
    "section": "Univariate data analysis",
    "text": "Univariate data analysis\n\nHandling categorial data\n\nCountplot\nCount plots provide the unique values of the column and their frequency of occurance. This provides a understanding on how the data is spread across the classes / categories of the\n\nsn.countplot(df_pl.select(pl.col('device')), x='device')\n\n\n\n\n\n\n\n\n\n\nPieChart\nA PieChart can be customized to show both the count of categories and their percentages in the dataset.\n\nvc = df_pl.to_pandas()['device'].value_counts()\nvc.plot(kind='pie', autopct=lambda pct: f'{int(round(pct * vc.sum() / 100))}\\n({pct:.1f}%)')\n\n\n\n\n\n\n\n\n\n\n\nHandling numerical data\n\nHistogram\nHistogram is a way to visualize a frequency table. The datapoints can be binned to n number of bins.\n\nsn.histplot(df_pl, x='humidity', bins=60)\n\n\n\n\n\n\n\n\n\nsn.histplot(df_pl, x='temp_c', bins=60)\n\n\n\n\n\n\n\n\n\n\nDistplot\nDensity plots show the distribution of the data values as a continuous line. It is a smoothed version of a histogram and is calculated via a kernel density estimate (page 24 - Practical Statistics for Data Scientists). The main difference between an histogram and a distplot is the y-axis scale. Displot uses probability scale and histogram uses frequency / count of observations.\n\nsn.displot(df_pl, x='humidity', kind=\"kde\")\n\n\n\n\n\n\n\n\n\nsn.displot(df_pl, x='temp_c', kind=\"kde\")\n\n\n\n\n\n\n\n\nBut remember our data has three different sensors. Would be great to ensure we can see the probablity density function of each one seperately. But this will fall in Bivariate data analysis, as we are using another column to make sense of the humidity or temp_c column.\n\nsn.kdeplot(df_pl, x='humidity', hue='device')\n\n\n\n\n\n\n\n\n\n\nBoxplot\nBoxplot give a quick way to visualize the distribution of data and provide boundaries to the observation outside which we find the outliers. It also shows the 25th percentile, median, 75th percentile.\nThe visual also helps quickly notice the outliers in the observations on both lower and higher boundries.\nThis is also called as a 5 point summary visual.\n\n\n\nBoxplotVisualized\n\n\n\nsn.boxplot(df_pl, x='humidity')\n\n\n\n\n\n\n\n\n\nsn.boxplot(df_pl, x='temp_c')"
  },
  {
    "objectID": "posts/notesdatascience/EDA/index.html#bivariate-data-analysis",
    "href": "posts/notesdatascience/EDA/index.html#bivariate-data-analysis",
    "title": "Exploratory data analysis",
    "section": "Bivariate data analysis",
    "text": "Bivariate data analysis\n\nScatterplot (Numerical to Numerical)\n\nsn.scatterplot(df_pl, x='temp_c', y='humidity' )\n\n\n\n\n\n\n\n\n\n\nBarplot (Numerical to categorical)\n\nsn.barplot(df_pl, x='temp_c', y='device')\n\n\n\n\n\n\n\n\n\n\nBoxplot (Numerical to categorical)\n\nsn.boxplot(df_pl, x='temp_c', y='device')"
  },
  {
    "objectID": "posts/notesdatascience/EDA/index.html#multivariate-data-analysis",
    "href": "posts/notesdatascience/EDA/index.html#multivariate-data-analysis",
    "title": "Exploratory data analysis",
    "section": "Multivariate data analysis",
    "text": "Multivariate data analysis\n\nsn.scatterplot(df_pl, x='temp_c', y='humidity', hue='device' )\n\n\n\n\n\n\n\n\n\n# Convert Polars DataFrame to pandas for seaborn plotting\ndf = df_pl.to_pandas()\n\n# Get unique devices (you want 3 scatterplots, one for each device)\ndevices = df['device'].unique()\n\n# Create a figure with 1 row and 3 columns for side-by-side plots\nfig, axes = plt.subplots(1, 3, figsize=(18, 5), sharex=True, sharey=True)\n\n# Loop through devices and axes to plot each device's scatterplot\nfor ax, device in zip(axes, devices):\n    subset = df[df['device'] == device]\n    sn.scatterplot(data=subset, x='temp_c', y='humidity', ax=ax)\n    ax.set_title(f'Device: {device}')\n    ax.set_xlabel('Temperature (°C)')\n    ax.set_ylabel('Humidity')\n\nplt.tight_layout()\nplt.show()\n\nText(0.5, 1.0, 'Device: b8:27:eb:bf:9d:51')\n\n\nText(0.5, 0, 'Temperature (°C)')\n\n\nText(0, 0.5, 'Humidity')\n\n\nText(0.5, 1.0, 'Device: 00:0f:00:70:91:0a')\n\n\nText(0.5, 0, 'Temperature (°C)')\n\n\nText(0, 0.5, 'Humidity')\n\n\nText(0.5, 1.0, 'Device: 1c:bf:ce:15:ec:4d')\n\n\nText(0.5, 0, 'Temperature (°C)')\n\n\nText(0, 0.5, 'Humidity')"
  },
  {
    "objectID": "posts/notesdatascience/DataIngestion/index.html",
    "href": "posts/notesdatascience/DataIngestion/index.html",
    "title": "Data ingestion",
    "section": "",
    "text": "The location of the data depends on ones use-case. Some have it locally, others on the cloud in a storage bucket or database. There is always a way to get your data to your development environment. The way we get it will differ.\nIn this notes, I use the kagglehub module to get a time series dataset.\nimport kagglehub\n# Download latest version\npath = kagglehub.dataset_download(\"garystafford/environmental-sensor-data-132k\")\nprint(\"Path to dataset files:\", path)"
  },
  {
    "objectID": "posts/notesdatascience/DataIngestion/index.html#getting-the-data",
    "href": "posts/notesdatascience/DataIngestion/index.html#getting-the-data",
    "title": "Data ingestion",
    "section": "",
    "text": "The location of the data depends on ones use-case. Some have it locally, others on the cloud in a storage bucket or database. There is always a way to get your data to your development environment. The way we get it will differ.\nIn this notes, I use the kagglehub module to get a time series dataset.\nimport kagglehub\n# Download latest version\npath = kagglehub.dataset_download(\"garystafford/environmental-sensor-data-132k\")\nprint(\"Path to dataset files:\", path)"
  },
  {
    "objectID": "posts/notesdatascience/DataIngestion/index.html#decrease-data-size",
    "href": "posts/notesdatascience/DataIngestion/index.html#decrease-data-size",
    "title": "Data ingestion",
    "section": "Decrease data size",
    "text": "Decrease data size\nOne needs to evaluate the size of the dataset and the resources available to process the data. One way og limiting the size of the data is to use effecient file formats.\nData formatted as Comma Seperated Value (CSV) is everywhere, but it is not the most lighweight or fast format when it comes read/write from disks. So it is wise to convert large CSV files to formats which are faster and take lesser space on disk/memory. One such format is parquet.\nWe can always decrease the data size to make ingestion easier. So we use polars to convert it to paraquet format.\n\n%%time\nimport polars as pl\n\ninput_data_path = f\"../data/iot/iot_telemetry_data.parquet\"\ndf = pl.scan_csv(\"../data/iot/iot_telemetry_data.csv\")\ndf.sink_parquet(input_data_path)  # Saves the lazyframe as a parquet file\n\nCPU times: user 720 ms, sys: 83.5 ms, total: 803 ms\nWall time: 387 ms\n\n\nFor small size datasets pandas and polars will do fine. As the dataset size increases, we need to look for effecient ways to read and prosess our data. In python, DuckDb and Pyspark are the best performing ETL libraries for large datasets.\nThat said, output from both DuckDb and Pyspark are not directly compatible with visualization libraries or other third party modules, for example, pandas-profiling.\nSo a hybrid approach is required, where the transformations are made using DuckDb or Pyspark, but the output is later converted to either polars or pandas dataframes. This allows us to efficiently perform ETL operations, but still be compatible with visualization libraries via polars or pandas formats."
  },
  {
    "objectID": "posts/notesdatascience/DataIngestion/index.html#using-pandas",
    "href": "posts/notesdatascience/DataIngestion/index.html#using-pandas",
    "title": "Data ingestion",
    "section": "Using pandas",
    "text": "Using pandas\n\n%%time\nimport pandas as pd\n\npd_df =  pd.read_parquet(input_data_path)\npd_df.head(2)\n\nCPU times: user 139 ms, sys: 35.5 ms, total: 175 ms\nWall time: 78.4 ms\n\n\n\n\n\n\n\n\n\nts\ndevice\nco\nhumidity\nlight\nlpg\nmotion\nsmoke\ntemp\n\n\n\n\n0\n1.594512e+09\nb8:27:eb:bf:9d:51\n0.004956\n51.0\nFalse\n0.007651\nFalse\n0.020411\n22.700000\n\n\n1\n1.594512e+09\n00:0f:00:70:91:0a\n0.002840\n76.0\nFalse\n0.005114\nFalse\n0.013275\n19.700001"
  },
  {
    "objectID": "posts/notesdatascience/DataIngestion/index.html#using-polars",
    "href": "posts/notesdatascience/DataIngestion/index.html#using-polars",
    "title": "Data ingestion",
    "section": "Using Polars",
    "text": "Using Polars\n\n%%time\nimport polars as pl\n\npl_df =  pl.scan_parquet(input_data_path)\npl_df.head(2).collect()\n\nCPU times: user 9.14 ms, sys: 1.99 ms, total: 11.1 ms\nWall time: 21.5 ms\n\n\n\nshape: (2, 9)\n\n\n\nts\ndevice\nco\nhumidity\nlight\nlpg\nmotion\nsmoke\ntemp\n\n\nf64\nstr\nf64\nf64\nbool\nf64\nbool\nf64\nf64\n\n\n\n\n1.5945e9\n\"b8:27:eb:bf:9d:51\"\n0.004956\n51.0\nfalse\n0.007651\nfalse\n0.020411\n22.7\n\n\n1.5945e9\n\"00:0f:00:70:91:0a\"\n0.00284\n76.0\nfalse\n0.005114\nfalse\n0.013275\n19.700001"
  },
  {
    "objectID": "posts/notesdatascience/DataIngestion/index.html#using-duckdb",
    "href": "posts/notesdatascience/DataIngestion/index.html#using-duckdb",
    "title": "Data ingestion",
    "section": "Using Duckdb",
    "text": "Using Duckdb\n\n%%time\nimport duckdb \n\nresult = duckdb.sql(f\"SELECT * FROM '{input_data_path}'\")\nresult.show()\n\n┌────────────────────┬───────────────────┬───────────────────────┬───────────────────┬─────────┬───────────────────────┬─────────┬──────────────────────┬────────────────────┐\n│         ts         │      device       │          co           │     humidity      │  light  │          lpg          │ motion  │        smoke         │        temp        │\n│       double       │      varchar      │        double         │      double       │ boolean │        double         │ boolean │        double        │       double       │\n├────────────────────┼───────────────────┼───────────────────────┼───────────────────┼─────────┼───────────────────────┼─────────┼──────────────────────┼────────────────────┤\n│ 1594512094.3859746 │ b8:27:eb:bf:9d:51 │  0.004955938648391245 │              51.0 │ false   │   0.00765082227055719 │ false   │  0.02041127012241292 │               22.7 │\n│ 1594512094.7355676 │ 00:0f:00:70:91:0a │ 0.0028400886071015706 │              76.0 │ false   │  0.005114383400977071 │ false   │ 0.013274836704851536 │ 19.700000762939453 │\n│ 1594512098.0735729 │ b8:27:eb:bf:9d:51 │  0.004976012340421658 │              50.9 │ false   │  0.007673227406398091 │ false   │  0.02047512557617824 │               22.6 │\n│  1594512099.589146 │ 1c:bf:ce:15:ec:4d │  0.004403026829699689 │ 76.80000305175781 │ true    │  0.007023337145877314 │ false   │ 0.018628225377018803 │               27.0 │\n│  1594512101.761235 │ b8:27:eb:bf:9d:51 │  0.004967363641908952 │              50.9 │ false   │  0.007663577282372411 │ false   │ 0.020447620810233658 │               22.6 │\n│ 1594512104.4684107 │ 1c:bf:ce:15:ec:4d │  0.004391003954583357 │  77.9000015258789 │ true    │  0.007009458543138704 │ false   │  0.01858890754005078 │               27.0 │\n│ 1594512105.4488637 │ b8:27:eb:bf:9d:51 │  0.004976025118224167 │              50.9 │ false   │  0.007673241660297752 │ false   │ 0.020475166204362245 │               22.6 │\n│  1594512106.869076 │ 00:0f:00:70:91:0a │ 0.0029381156266604295 │              76.0 │ false   │  0.005241481841731117 │ false   │ 0.013627521132019194 │ 19.700000762939453 │\n│ 1594512108.2753816 │ 1c:bf:ce:15:ec:4d │  0.004345471359573249 │  77.9000015258789 │ true    │  0.006956802377235561 │ false   │  0.01843978190211682 │               27.0 │\n│ 1594512109.1366868 │ b8:27:eb:bf:9d:51 │ 0.0049702557644185795 │              50.9 │ false   │ 0.0076668047981169295 │ false   │ 0.020456819607064126 │               22.6 │\n│          ·         │         ·         │            ·          │                ·  │  ·      │           ·           │   ·     │           ·          │                 ·  │\n│          ·         │         ·         │            ·          │                ·  │  ·      │           ·           │   ·     │           ·          │                 ·  │\n│          ·         │         ·         │            ·          │                ·  │  ·      │           ·           │   ·     │           ·          │                 ·  │\n│  1594528617.316101 │ 1c:bf:ce:15:ec:4d │  0.004046021057852555 │ 76.80000305175781 │ true    │ 0.0066065967976340415 │ false   │ 0.017449961409724143 │ 25.700000762939453 │\n│ 1594528620.1956518 │ 00:0f:00:70:91:0a │  0.002656364232179544 │ 75.80000305175781 │ false   │  0.004872844318332629 │ false   │  0.01260624929176923 │ 19.399999618530273 │\n│ 1594528620.5726128 │ b8:27:eb:bf:9d:51 │  0.004838697447060515 │              52.7 │ false   │  0.007519458225661741 │ false   │ 0.020037135448620128 │               22.1 │\n│  1594528623.834102 │ 1c:bf:ce:15:ec:4d │   0.00405307240177605 │ 76.80000305175781 │ true    │  0.006614923978741507 │ false   │ 0.017473456575501277 │ 25.700000762939453 │\n│ 1594528624.2579694 │ b8:27:eb:bf:9d:51 │  0.004859394667370261 │              52.6 │ false   │  0.007542712063759859 │ false   │ 0.020103331890812447 │               22.0 │\n│   1594528626.16313 │ 00:0f:00:70:91:0a │  0.002612589347788125 │ 75.69999694824219 │ false   │  0.004814621044662395 │ false   │ 0.012445419108693902 │ 19.399999618530273 │\n│ 1594528627.9447663 │ b8:27:eb:bf:9d:51 │ 0.0048428875642098305 │              52.6 │ false   │  0.007524168143299383 │ false   │  0.02005054199296001 │               22.0 │\n│ 1594528631.8750868 │ b8:27:eb:bf:9d:51 │  0.004855276661403902 │              52.6 │ false   │  0.007538087568901512 │ false   │ 0.020090166282918154 │               22.0 │\n│ 1594528631.9181976 │ 1c:bf:ce:15:ec:4d │   0.00405307240177605 │ 76.80000305175781 │ true    │  0.006614923978741507 │ false   │ 0.017473456575501277 │ 25.700000762939453 │\n│  1594528635.561384 │ b8:27:eb:bf:9d:51 │     0.004833237960391 │              52.6 │ false   │  0.007513319775311182 │ false   │ 0.020019663516957425 │               22.0 │\n├────────────────────┴───────────────────┴───────────────────────┴───────────────────┴─────────┴───────────────────────┴─────────┴──────────────────────┴────────────────────┤\n│ ? rows (&gt;9999 rows, 20 shown)                                                                                                                                    9 columns │\n└────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n\nCPU times: user 160 ms, sys: 19.1 ms, total: 179 ms\nWall time: 197 ms"
  },
  {
    "objectID": "posts/notesdatascience/DataIngestion/index.html#using-pyspark",
    "href": "posts/notesdatascience/DataIngestion/index.html#using-pyspark",
    "title": "Data ingestion",
    "section": "Using pyspark",
    "text": "Using pyspark\n\n%%time\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"readParquet\").getOrCreate()\ndf = spark.read.parquet(input_data_path)\ndf.show()\n\n+--------------------+-----------------+--------------------+-----------------+-----+--------------------+------+--------------------+------------------+\n|                  ts|           device|                  co|         humidity|light|                 lpg|motion|               smoke|              temp|\n+--------------------+-----------------+--------------------+-----------------+-----+--------------------+------+--------------------+------------------+\n|1.5945120943859746E9|b8:27:eb:bf:9d:51|0.004955938648391245|             51.0|false| 0.00765082227055719| false| 0.02041127012241292|              22.7|\n|1.5945120947355676E9|00:0f:00:70:91:0a|0.002840088607101...|             76.0|false|0.005114383400977071| false|0.013274836704851536|19.700000762939453|\n|1.5945120980735729E9|b8:27:eb:bf:9d:51|0.004976012340421658|             50.9|false|0.007673227406398091| false| 0.02047512557617824|              22.6|\n| 1.594512099589146E9|1c:bf:ce:15:ec:4d|0.004403026829699689|76.80000305175781| true|0.007023337145877314| false|0.018628225377018803|              27.0|\n| 1.594512101761235E9|b8:27:eb:bf:9d:51|0.004967363641908952|             50.9|false|0.007663577282372411| false|0.020447620810233658|              22.6|\n|1.5945121044684107E9|1c:bf:ce:15:ec:4d|0.004391003954583357| 77.9000015258789| true|0.007009458543138704| false| 0.01858890754005078|              27.0|\n|1.5945121054488637E9|b8:27:eb:bf:9d:51|0.004976025118224167|             50.9|false|0.007673241660297752| false|0.020475166204362245|              22.6|\n| 1.594512106869076E9|00:0f:00:70:91:0a|0.002938115626660...|             76.0|false|0.005241481841731117| false|0.013627521132019194|19.700000762939453|\n|1.5945121082753816E9|1c:bf:ce:15:ec:4d|0.004345471359573249| 77.9000015258789| true|0.006956802377235561| false| 0.01843978190211682|              27.0|\n|1.5945121091366868E9|b8:27:eb:bf:9d:51|0.004970255764418...|             50.9|false|0.007666804798116...| false|0.020456819607064126|              22.6|\n| 1.594512112798518E9|b8:27:eb:bf:9d:51|0.004960208655965963|             50.9|false|0.007655590313556344| false| 0.02042485815208522|              22.6|\n|1.5945121152885423E9|1c:bf:ce:15:ec:4d| 0.00438304383734993|             78.0| true|0.007000264000767255| false|0.018562862485791535|              27.0|\n|1.5945121164982603E9|b8:27:eb:bf:9d:51|0.004971644949355083|             50.9|false|0.007668354899155367| false|0.020461237669931027|              22.6|\n|1.5945121190980136E9|1c:bf:ce:15:ec:4d|0.004451497630812575|             78.0| true|0.007079183500131396| false|0.018786490564423525|              27.0|\n| 1.594512120184931E9|b8:27:eb:bf:9d:51|0.004964564518477901|             50.9|false|0.007660453055613286| false|0.020438716650667384|              22.6|\n|1.5945121227857318E9|00:0f:00:70:91:0a|0.002905014756555...|75.80000305175781|false|0.005198697479294309| false|0.013508733329556249|19.700000762939453|\n|1.5945121238726196E9|b8:27:eb:bf:9d:51|0.004975983419764024|             50.9|false|0.007673195144776...| false| 0.02047503362023219|              22.6|\n|1.5945121275601885E9|b8:27:eb:bf:9d:51|0.004960208655965963|             50.9|false|0.007655590313556344| false| 0.02042485815208522|              22.6|\n| 1.594512129374153E9|1c:bf:ce:15:ec:4d|0.004439322766059633| 77.9000015258789| true|0.007065171934738014| false| 0.01874677460984377|              27.0|\n|1.5945121312478008E9|b8:27:eb:bf:9d:51|0.004956119201656337|             50.9|false|0.007651023905784...| false|0.020411844733363067|              22.6|\n+--------------------+-----------------+--------------------+-----------------+-----+--------------------+------+--------------------+------------------+\nonly showing top 20 rows\nCPU times: user 279 ms, sys: 36.5 ms, total: 315 ms\nWall time: 11.5 s"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello, I’m Jeevith. I love automation and currently working my way into the Data Science field. My expierence spans across the field of technical safety, underwater robotics, process/test automation and data science.\nThis website is an effort to document my journey of learning new technologies.\nAfter many years of working as a developer, I missed having a repository where I could document and access my learnings. I also hope that by sharing my know-how, I can help others along the way.\nThe plan is to write about topics covered in this mindmap, which will expand with new learnings.\nmindmap\n  root((Knowledge base))\n    Project execution\n        Business analysis\n        Strategy\n    Research\n      Risk analysis\n      Autonomous systems\n        Underwater Remotely Operated Vehicles\n        TCAS & COLREGs\n      Applied ML in safety science\n    Data science\n        Exploratory data analysis\n        Time Series Analysis\n    Applied Knowledge\n      Cloud\n        Azure\n      Automation\n        UiPath\n        PowerAutomate\n        BluePrism\n        Robocorp\n      Misc\n        Mermaid\n        SharePoint\n        PowerApps\n        Godot\n        Bruno\n      Programming\n        Python\n         Pytorch\n        SQL\n        PowerShell\n        Git\n        GDScript\n        REST API\n        SOAP\n      Visualizations\n        Splunk\n        PowerBI\n        Grafana\n      Large Language Model\n        Llamafile\n        Ollama\n        Llamaindex\n        Retrieval Augmented Generation-RAG"
  },
  {
    "objectID": "about.html#use-of-llms",
    "href": "about.html#use-of-llms",
    "title": "About",
    "section": "Use of LLMs",
    "text": "Use of LLMs\nIf the content in the published posts are produced by using a online/local large-language-model, a clear attribution text with the following information will be shared in the post.\n\n\n\n\n\n\nNote\n\n\n\nThis part was generated using Large Language Model (LLMMODEL) with the following prompt text: PROMTTEXT."
  },
  {
    "objectID": "about.html#credits",
    "href": "about.html#credits",
    "title": "About",
    "section": "Credits",
    "text": "Credits\nLogo was generated using Dummy-logo-maker. Thanks to moiseshp\nAll opinions written in this blog are my own."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Recent posts",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\nFeature engineering\n\n\nUnderstanding how to perform feature engineering\n\n\n\nAug 9, 2025\n\n\n\n\n\n\n\n\n\n\nExploratory data analysis\n\n\nExploring data using univariate, bivariate and multivariate analysis\n\n\n\nAug 9, 2025\n\n\n\n\n\n\n\n\n\n\nData ingestion\n\n\nLoading data using different python modules\n\n\n\nAug 9, 2025\n\n\n\n\n\n\n\n\n\n\nReviving an old apple mac 💻 🖥️\n\n\nOld macs new operating systems\n\n\n\nApr 19, 2025\n\n\n\n\n\n\n\n\n\n\nNotes on Practical Statistics for Data Scientists 📗\n\n\nNotes of the book Practical Statistics for Data Scientists - 50+ Essential Concepts Using R and Python by Peter Bruce, Andrew Bruce, Peter Gedeck\n\n\n\nApr 15, 2025\n\n\n\n\n\n\n\n\n\n\nIs that NiN valid?\n\n\nSearching and validating Norwegian Identity Numbers\n\n\n\nApr 11, 2025\n\n\n\n\n\n\n\n\n\n\nUsing all GPUs when running Ollama and OpenWebUI in docker\n\n\nBundled install of ollama and Open-Webui using docker\n\n\n\nMar 16, 2025\n\n\n\n\n\n\n\n\n\n\nUpdating Ollama in an airgapped linux server\n\n\nA walkthrough\n\n\n\nFeb 17, 2025\n\n\n\n\n\n\n\n\n\n\nSwitching from PIP to UV\n\n\nUpgrading my python workflow\n\n\n\nFeb 10, 2025\n\n\n\n\n\n\n\n\n\n\nContent of file from Azure DevOps repository\n\n\nExploring Azure Devops API to get contents of a file in a repository\n\n\n\nNov 1, 2024\n\n\n\n\n\n\n\n\n\n\nNotes on time series analysis\n\n\nExploring components of time series\n\n\n\nSep 17, 2024\n\n\n\n\n\n\n\n\n\n\nCreating a custom llamafile 🦙\n\n\nA single executable to serve LLMs\n\n\n\nJul 17, 2024\n\n\n\n\n\n\n\n\n\n\nZSH with Powerlevel10k\n\n\nMy personal zsh configuration\n\n\n\nJul 11, 2024\n\n\n\n\n\n\n\n\n\n\nBeryl GL MT-1300 travel router\n\n\nA travel router makes using untrusted wi-fi networks safe and easy\n\n\n\nJul 6, 2024\n\n\n\n\n\n\n\n\n\n\nVentoy bootable USB\n\n\nOS images in one USB!\n\n\n\nJun 14, 2024\n\n\n\n\n\n\n\n\n\n\nNerd miner setup\n\n\nBitcoin mining on edge / IOT devices (ESP32)\n\n\n\nJun 12, 2024\n\n\n\n\n\n\n\n\n\n\nVideo download script\n\n\nAutomating downloading YouTube videos/audio for personal use\n\n\n\nJun 10, 2024\n\n\n\n\n\n\n\n\n\n\nBruno API client\n\n\nA local and git friendly API client\n\n\n\nJun 2, 2024\n\n\n\n\n\n\n\n\n\n\nUseful linux commands\n\n\nSome of my frequently used linux commands\n\n\n\nMay 23, 2024\n\n\n\n\n\n\n\n\n\n\nSolving linear programming problems\n\n\nExploring LP in python\n\n\n\nMay 19, 2024\n\n\n\n\n\n\n\n\n\n\nEssentials installer for debian distribution\n\n\nA script to install my frequently used applications in linux (debain)\n\n\n\nMay 19, 2024\n\n\n\n\n\n\n\n\n\n\nDiagram as code\n\n\nAn introduction to diagram as code tools\n\n\n\nMay 17, 2024\n\n\n\n\n\n\nNo matching items"
  }
]