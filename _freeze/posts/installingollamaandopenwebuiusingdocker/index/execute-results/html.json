{
  "hash": "469644a9a942e7dc1f82e3b75781484d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Using all GPUs when running Ollama and OpenWebUI in docker \ndescription: \"Bundled install of ollama and Open-Webui using docker\"\nformat: html\ntoc: true\nlang: en\njupyter: python3\nipynb-shell-interactivity: all\nexecute:\n  echo: false\ndate: 2025-03-16 12:01 +0200\ncategories: [\"llm\"]\ntags: [\"ollama\", \"openwebui\"]\ncomments:\n  giscus:\n    repo: jeev20/jeev20.github.io\n---\n\n\n> Case: Run both Ollama and OpenWebUI in a single docker container and use all available GPUs in the host machine\n\n\n**TL;DR**\n```{.bash}\n# Installing nvidia-container-toolkit\ncurl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\\n&& curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\\n  sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\\n  sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\nsudo apt-get update\nsudo apt-get install -y nvidia-container-toolkit\n\n\n# Installing ollama and open-webui in docker\nsudo docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama\nsudo docker run --rm --volume /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once open-webui\n \n# Pulling some models from Ollama repository\nsudo docker exec open-webui ollama pull phi4\n```\n\n\n\nI have been using Ollama as a standalone installation in my homelab which ran as a systemd service. I used OpenWebUI as the user interface to the local Ollama installation. However, a recent version of Ollama failed to auto spawn on my LMDE 6 operating system after a restart. \n\nNaturally, I had to look for an alternative with minimal maintenance need. Thankfully, OpenWebUI offers an image which bundles both Ollama and the OpenWebUI and uses the Nvidia-Container-Toolkit to utilize all installed GPUs. \n\nThis means that I only have to maintain one container for both Ollama and OpenWebUI installation. Further, to automate this update process, I use watchtower to install the latest updates of the image from the OpenWebUI team. \n\n## Docker run can fail\nI observed that running only `sudo docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama` did not work as it required [nvidia-container-toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) to be installed first. \n\nSo I ran \n\n```{.bash}\ncurl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\\n  && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\\n    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\\n    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\n```\n\nFollowed by\n\n```{.bash}\nsudo apt-get update\nsudo apt-get install -y nvidia-container-toolkit\n```\n\nAnd then ran the open-webui docker command\n\n```{.bash} \nsudo docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama\nsudo docker run --rm --volume /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once open-webui\n```\n\nDocker started the container and also setup the watchtower auto-update for the `open-webui` container. \n\nI can now use the `docker exec` command to use Ollama cli within the container. I downloaded some models by using\n\n```{.bash}\nsudo docker exec open-webui ollama pull phi4\nsudo docker exec open-webui ollama pull qwq\n```\n\nSince I reused the docker volume I previously used for open-webui, all my historical prompts were still available after this upgrade. Sweet! \n\n\n## Resources\n* [nvidia-container-toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)\n* [openwebui](https://github.com/open-webui/open-webui)\n* [watchtower](https://github.com/containrrr/watchtower)\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}