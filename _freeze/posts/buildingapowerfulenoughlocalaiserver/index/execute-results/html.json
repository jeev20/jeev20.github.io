{
  "hash": "7292ea7890b433f95c7ed85a563373b3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Building a local AI server\nformat: html\ntoc: true\nlang: en\njupyter: python3\nipynb-shell-interactivity: all\nexecute:\n  echo: false\ndate: 2025-06-03 06:50 +0200\ncategories: [\"ai\"]\ntags: [\"ollama\", \"aihardware\"]\ncomments:\n  giscus:\n    repo: jeev20/jeev20.github.io\n---\n\n\n## Requirements\n\n### Hardware\nMy requirements were quite basic:\n\n* A minimum of 16GB VRAM (preferably Nvidia)\n* A minimum of 16GB RAM\n* A minimum of 6 cores / 12 threads Ryzen CPU\n* A motherboard with 2 PCI GPU slots (does not need to be 16 lanes PCI)\n* A minimum of 600 watt power supply\n* A wi-fi smart-plug with scheduling capabilities\n* A motherboard which support power-on after power restoration\n\n\n### Software\n* Linux OS with a long-term support and used by a lot of users\n* Linux OS with Tailscale native support (systemd)\n* Linux OS with easy installation of Docker, Nvidia Container Toolkit and Cuda Toolkit\n* Linux OS with OpenSSH server to manage server remotely\n* Linux OS with crontab to schedule running of scripts\n  \n\n## Server \nI ended up purchasing a second-hand PC with all the above requirements and then purchased two RTX 3060 GPUs with 12 GB Vram each. The RAM capacity is somewhat low for this use-case and I plan to upgrade it in the future. \n\n\n## Scheduling\n\n## Updates\nAs everything runs on an Ubuntu LTS 24.04. \n\n## Docker\n\n## Ollama and OpenWebUi Bundle\n\n## Docker exec\n\n## Power automation\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}