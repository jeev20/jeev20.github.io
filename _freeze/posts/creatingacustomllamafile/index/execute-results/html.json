{
  "hash": "c78728f0cb46d1e030a5b85907441c8e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Creating a custom llamafile ðŸ¦™\ndescription: \"A single executable to serve LLMs\"\nformat: html\ntoc: true\nlang: en\njupyter: python3\nipynb-shell-interactivity: all\nexecute:\n  echo: false\ndate: 2024-07-17 16:44 +0200\ncategories: [\"llm\"]\ntags: [\"local llm\", \"llamafile\", \"self hosted\"]\ncomments:\n  giscus:\n    repo: jeev20/jeev20.github.io\n---\n\n\n### Background\nI recently watched the keynote/demo on llamfile which showed how local LLMs in a single executable. [Ollama](https://ollama.com/) is great but the additional installation/maintenance overhead it brings can be seen as one of the negatives. That said, most of these tools currently are still in their infancy and with time, they will only get easier to install, use, scale and maintain. \n\n\n{{< video https://www.youtube.com/embed/-mRi-B3t6fA >}}\n\n\nI soon figured that llama index already has an integration for working with llamafiles and a concise blog post on how to use a llamafile to build a rudimentary RAG system. \n\n1. <a href=\"https://docs.llamaindex.ai/en/stable/examples/llm/llamafile/\">llamafile-llamaindex</a>\n\n2. <a href=\"https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant\">llamafile-RAG</a>\n\nAccording to me, the major advantage of a llamafile is that it exposes an API service in addition to a web user interface. This means that we could use the API endpoints from a llamafile and use it in Retrieval Augmented Generation (RAG) projects or any other LLM use case. The available API endpoints are described in this [link](https://github.com/Mozilla-Ocho/llamafile/blob/main/llama.cpp/server/README.md#api-endpoints).\n\n\n\n\n### Usage\nWe can use llamafile's in three ways\n\n#### 1. Using API endpoints (the API format is same as OpenAI API)\n\n```{.bash}\n  curl http://localhost:8080/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer no-key\" \\\n  -d '{\n    \"model\": \"LLaMA_CPP\",\n    \"messages\": [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a assistant well versed with the documentation of fastapi python module. Guide the user to a acceptable solution. DO NOT small talk or provide extra information. Be objective precise and provide references to you responses.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"How to create a private api route in fastapi?\"\n        }\n      ]\n  }'\n  ```\n#### 2. Using Web GUI \n   When one runs the executable llamafile, a web ui interface is accessible at a choosen port\n\n   ```{.bash}\n   ./e5-mistral-7b-instruct-Q5_K_M.llamafile -ngl 9999 --server --embedding \\\n   --host 0.0.0.0 --port 8080 \n   ```\n   ![LlamafileOutput](images/llamafilegui.png)\n\n#### 3. Using CLI\n   The executable can also be used via the terminal/cli with corresponding arguments. \n\n```{.bash}\n./llava-v1.5-7b-q4.llamafile --temp 0.2 --image Transporterposter.jpg \\ \n-e -p '### User: What do you see?\\n### Assistant:'\n```\n\n\n\n> The image is a movie poster for the film \"The Transporter.\" The poster features a man in a suit and tie, holding a gun and pointing it at the camera. The man is the main focus of the poster, and he appears to be the main character in the movie. The poster is displayed in a vertical orientation, showcasing the man's action-packed pose.\n\n  \n\n------------------------------------------------\n## Creating your own llamafiles\nAll the above is great to know, but how do we put it to use by creating our own llamafile? This sections covers some of the missing steps in the [readme.md](https://github.com/Mozilla-Ocho/llamafile/blob/main/README.md) of the llamafile project. I recommend you to also read the entire `readme` as it covers some known workarounds to get things going.\n\nLets begin! \n\n### Step 1 - Install llamafile\nClone the [llamafile](https://github.com/Mozilla-Ocho/llamafile) repo from Mozilla. Change directories and run the make install command with sudo permissions. \n\n```{.bash}\ngit clone https://github.com/Mozilla-Ocho/llamafile\ncd llamafile\nsudo make install\n```\n\n\nThe above command will install all the necessary binaries to this folder `/usr/local/bin/llamafile` with the following terminal output.\n\n```{.bash}\nmkdir -p /usr/local/bin\ninstall o//llamafile/zipalign /usr/local/bin/zipalign\ninstall o//llamafile/tokenize /usr/local/bin/llamafile-tokenize\ninstall o//llama.cpp/main/main /usr/local/bin/llamafile\ninstall o//llama.cpp/imatrix/imatrix /usr/local/bin/llamafile-imatrix\ninstall o//llama.cpp/quantize/quantize /usr/local/bin/llamafile-quantize\ninstall o//llama.cpp/llama-bench/llama-bench /usr/local/bin/llamafile-bench\ninstall build/llamafile-convert /usr/local/bin/llamafile-convert\ninstall build/llamafile-upgrade-engine /usr/local/bin/llamafile-upgrade-engine\ninstall o//llama.cpp/perplexity/perplexity /usr/local/bin/llamafile-perplexity\ninstall o//llama.cpp/llava/llava-quantize /usr/local/bin/llava-quantize\nmkdir -p /usr/local/share/man/man1\ninstall -m 0644 llamafile/zipalign.1 /usr/local/share/man/man1/zipalign.1\ninstall -m 0644 llama.cpp/main/main.1 /usr/local/share/man/man1/llamafile.1\ninstall -m 0644 llama.cpp/imatrix/imatrix.1 /usr/local/share/man/man1/llamafile-imatrix.1\ninstall -m 0644 llama.cpp/quantize/quantize.1 /usr/local/share/man/man1/llamafile-quantize.1\ninstall -m 0644 llama.cpp/perplexity/perplexity.1 /usr/local/share/man/man1/llamafile-perplexity.1\ninstall -m 0644 llama.cpp/llava/llava-quantize.1 /usr/local/share/man/man1/llava-quantize.1\n```\n\n### Step 2 - Check version\nOpen a new terminal window and check the version of llamafile you have.\n```{.bash}\n$ llamafile --version  \nllamafile v0.8.9\n```\n\n\n### Step 3 - Creating the args file\nThe `.args` file allows you to customize the llamafile you want to generate. The following is an example content of the `.args` file \n\n```{.bash}\n-m\nMeta-Llama-3-8B-Instruct-IQ4_NL.gguf\n--mmproj\nLLaMA3-8B_mmproj-Q4_1.gguf\n--host\n0.0.0.0\n-ngl\n9999\n...\n```\n::: {.callout-note}\nTo create another llamafile just find the appropriate model you want to use and download the `.gguf` file from HuggingFace and follow the same steps mentioned above. \n\nThe `--mmproj` is optional but the `m` (model) option is mandatory in the `.args` file\n:::\n\n\n### Step 4 - Building on the llamafile binary\nWe first copy the llamafile and give it a new name. \n```{.bash}\ncp /usr/local/bin/llamafile llama3.llamafile \n```\n\nThe working folder should contain the following files.\n```\nLlamafileExperiments  \nâ”‚\nâ””â”€â”€â”€.args\nâ””â”€â”€â”€Meta-Llama-3-8B-Instruct-IQ4_NL.gguf\nâ””â”€â”€â”€LLaMA3-8B_mmproj-Q4_1.gguf\nâ””â”€â”€â”€llama3.llamafile \n```\n\nNow use the `zipalign` binary which is an alternative to zip. This library is designed to concatenate gigabytes of LLM weights to an executable. If you observe the output from Step 1, you see that the `zipalign` binary was also saved to `/usr/local/bin/zipalign`. Therefore, your terminal should recognize the `zipalign` command. Read more about zipalign by using the command `man zipalign`.\n\n```{.bash}\nzipalign -j0 \\\n  llama3.llamafile \\\n  Meta-Llama-3-8B-Instruct-IQ4_NL.gguf \\\n  LLaMA3-8B_mmproj-Q4_1.gguf \\\n  .args\n```\n\nThats all! A `llama3.llamafile` will be generated in that current folder. This can then be run by using any of the three ways [mentioned above](#usage)\n\n```{.bash}\n./llama3.llamafile\n```\n\n![LlamafileOutput](images/llamafilegui.png)\n\n\n\n#### Porting the output llamafile to windows \nIf you want to use the output file in Windows then add the `.exe` extension to the generated llamafile. For example, `llama3.llamafile` to `llama3.llamafile.exe`. \n\n### Credits\n\nAll this magic with llamafile is possible because of the two previous open-source projects, namely [llama.cpp](https://github.com/ggerganov/llama.cpp) and [cosmopolitan_Libc](https://github.com/jart/cosmopolitan). Kudos to the developers and maintainers! Not to forget all the credit to realize llamafile goes to the [Mozilla builders project](https://future.mozilla.org/).\n\n```{mermaid}\n\nflowchart LR\n\nsubgraph Mozilla Builders Project\nc\nend\n\nLlama.cpp --> c[fa:fa-heart llamafile]\nCosmopolitan_Libc --> c[fa:fa-heart llamafile]\n\n```\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}